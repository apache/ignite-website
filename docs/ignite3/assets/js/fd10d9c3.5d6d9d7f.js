"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[545],{28453:(e,t,i)=>{i.d(t,{R:()=>o,x:()=>a});var n=i(96540);const r={},s=n.createContext(r);function o(e){const t=n.useContext(s);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function a(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),n.createElement(s.Provider,{value:t},e.children)}},51647:(e,t,i)=>{i.r(t),i.d(t,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>n,toc:()=>c});const n=JSON.parse('{"id":"configure-and-operate/operations/disaster-recovery-partitions","title":"Disaster Recovery for Data Partitions","description":"You perform disaster recovery operations to recover from situation when data operations on your Apache Ignite cluster nodes become unfeasible because Apache Ignite cannot guarantee data consistency. In such cases, you need to either return data to a consistent state or declare the current state consistent.","source":"@site/docs/configure-and-operate/operations/disaster-recovery-partitions.md","sourceDirName":"configure-and-operate/operations","slug":"/configure-and-operate/operations/disaster-recovery-partitions","permalink":"/docs/ignite3/3.1.0/configure-and-operate/operations/disaster-recovery-partitions","draft":false,"unlisted":false,"tags":[],"version":"current","frontMatter":{"id":"disaster-recovery-partitions","title":"Disaster Recovery for Data Partitions","sidebar_label":"Data Partitions Recovery"},"sidebar":"tutorialSidebar","previous":{"title":"Overview","permalink":"/docs/ignite3/3.1.0/configure-and-operate/operations/disaster-recovery"},"next":{"title":"System Groups Recovery","permalink":"/docs/ignite3/3.1.0/configure-and-operate/operations/disaster-recovery-system-groups"}}');var r=i(74848),s=i(28453);const o={id:"disaster-recovery-partitions",title:"Disaster Recovery for Data Partitions",sidebar_label:"Data Partitions Recovery"},a=void 0,l={},c=[{value:"Disaster Scenarios and Recovery Instructions",id:"disaster-scenarios-and-recovery-instructions",level:2},{value:"Minority Offline",id:"minority-offline",level:3},{value:"Majority Offline",id:"majority-offline",level:3},{value:"Partition Loss",id:"partition-loss",level:3},{value:"Partition States",id:"partition-states",level:2},{value:"Local Partition States",id:"local-partition-states",level:3},{value:"Global Partition States",id:"global-partition-states",level:3}];function d(e){const t={a:"a",admonition:"admonition",code:"code",em:"em",h2:"h2",h3:"h3",li:"li",ol:"ol",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsxs)(t.p,{children:["You perform ",(0,r.jsx)(t.em,{children:"disaster recovery"})," operations to recover from situation when data operations on your Apache Ignite cluster nodes become unfeasible because Apache Ignite cannot guarantee data consistency. In such cases, you need to either return data to a consistent state or declare the current state consistent."]}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsxs)(t.p,{children:["Disaster recovery for system groups, ",(0,r.jsx)(t.a,{href:"/3.1.0/configure-and-operate/operations/disaster-recovery-system-groups#cluster-management-group",children:"Cluster Management Group"})," and ",(0,r.jsx)(t.a,{href:"/3.1.0/configure-and-operate/operations/disaster-recovery-system-groups#metastorage-group",children:"Metastorage Group"}),", is described in a separate page."]})}),"\n",(0,r.jsx)(t.h2,{id:"disaster-scenarios-and-recovery-instructions",children:"Disaster Scenarios and Recovery Instructions"}),"\n",(0,r.jsx)(t.h3,{id:"minority-offline",children:"Minority Offline"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.em,{children:"Minority"})," refers to less than half of the number of ",(0,r.jsx)(t.em,{children:"replicas"})," configured for a distribution zone (DZ). For example, of DZ1 is configured with 2 replicas and DZ2 with 3 replicas, losing a single Apache Ignite node is a majority loss for DZ1 and a minority loss for DZ2."]}),"\n",(0,r.jsxs)(t.p,{children:["You may discover that one or more of your cluster nodes are offline in a number of ways, including the ",(0,r.jsx)(t.code,{children:"recovery partition states"})," ",(0,r.jsx)(t.a,{href:"/3.1.0/tools/cli-commands",children:"CLI command"})," with the ",(0,r.jsx)(t.code,{children:"--global"})," option, which would show ",(0,r.jsx)(t.code,{children:"Read-only partition"}),", ",(0,r.jsx)(t.code,{children:"Degraded partition"}),", or ",(0,r.jsx)(t.code,{children:"Unavailable partition"})," for offline nodes."]}),"\n",(0,r.jsx)(t.p,{children:"Once a minority offline status has been discovered:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsx)(t.li,{children:"Command the system to bring the offline node(s) online."}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"The system attempts to bring the indicated nodes online. Possible outcomes are:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Nodes return online in time (before scale-down timeout) with valid data. The system replicates the missing data (if any) using either log replication or the full state transfer procedure."}),"\n",(0,r.jsx)(t.li,{children:"Nodes return online in time but without data. The system replicates the data using the full state transfer procedure."}),"\n",(0,r.jsx)(t.li,{children:"A node does not return online before scale-down timeout. The system distributes a replica to a new node and starts the rebalance procedure."}),"\n",(0,r.jsx)(t.li,{children:"Node return online with inconsistent data - see steps 4 and 5."}),"\n"]}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsx)(t.p,{children:"Full state transfer and rebalancing might take a long time (tens of minutes). We suggest that you recover nodes as soon as you discover they are offline."})}),"\n",(0,r.jsxs)(t.ol,{start:"2",children:["\n",(0,r.jsxs)(t.li,{children:["Run the ",(0,r.jsx)(t.code,{children:"recovery partition states"})," command on the relevant zones/nodes/partitions to verify ",(0,r.jsx)(t.a,{href:"#partition-states",children:"Partition States"}),"."]}),"\n",(0,r.jsxs)(t.li,{children:["If the state is ",(0,r.jsx)(t.code,{children:"Healthy"})," or ",(0,r.jsx)(t.code,{children:"Available partition"}),", consider the recovery completed."]}),"\n",(0,r.jsxs)(t.li,{children:["If the state is ",(0,r.jsx)(t.code,{children:"Broken"}),":","\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:["Restart your Apache Ignite node or the relevant partitions using the ",(0,r.jsx)(t.code,{children:"recovery partitions restart"})," command."]}),"\n",(0,r.jsxs)(t.li,{children:["Rerun the ",(0,r.jsx)(t.code,{children:"recovery partition states"})," command."]}),"\n",(0,r.jsxs)(t.li,{children:["If the partition state remains ",(0,r.jsx)(t.code,{children:"Broken"}),", reset the partitions with deletion by using the ",(0,r.jsx)(t.code,{children:"recovery partitions restart --with-cleanup"})," command. Local partition data will be deleted and restored from the cluster."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["If the partition state is ",(0,r.jsx)(t.code,{children:"Read-only partition"}),", ",(0,r.jsx)(t.code,{children:"Degraded partition"}),", or ",(0,r.jsx)(t.code,{children:"Unavailable partition"}),", reset the relevant partitions using the ",(0,r.jsx)(t.code,{children:"recovery reset partitions"})," command."]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"majority-offline",children:"Majority Offline"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.em,{children:"Majority"})," refers to half (or more) of the number of ",(0,r.jsx)(t.em,{children:"replicas"})," configured for a distribution zone (DZ). For example, of DZ1 is configured with 2 replicas and DZ2 with 3 replicas, losing a single Apache Ignite node is a majority loss for DZ1 and a minority loss for DZ2."]}),"\n",(0,r.jsxs)(t.p,{children:["You may discover that one or more of your cluster nodes are offline in a number of ways, including the ",(0,r.jsx)(t.code,{children:"recovery partition states"})," CLI command with the ",(0,r.jsx)(t.code,{children:"--global"})," option, which would show ",(0,r.jsx)(t.code,{children:"Read-only partition"}),", ",(0,r.jsx)(t.code,{children:"Degraded partition"}),", or ",(0,r.jsx)(t.code,{children:"Unavailable partition"})," for offline nodes."]}),"\n",(0,r.jsxs)(t.p,{children:["If the node(s) that remain(s) online include the primary replica, the partition becomes ",(0,r.jsx)(t.code,{children:"Read-only partition"})," (see ",(0,r.jsx)(t.a,{href:"#global-partition-states",children:"Global Partition States"}),"); all the data is available for reading until lease expires. If the node(s) that remain(s) online do ",(0,r.jsx)(t.em,{children:"not"})," include the primary replica, the partition becomes ",(0,r.jsx)(t.code,{children:"Unavailable partition"}),"."]}),"\n",(0,r.jsx)(t.p,{children:"Once a majority offline status has been discovered:"}),"\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsx)(t.li,{children:"Command the system to bring the offline nodes online."}),"\n"]}),"\n",(0,r.jsx)(t.p,{children:"The system attempts to bring the indicated nodes online. Possible outcomes are:"}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsx)(t.li,{children:"Nodes return online in time (before scale-down timeout) with valid data. A leader is elected, the system replicates the missing data (if any) using either log replication or the full state transfer procedure, and a leaseholder is elected"}),"\n",(0,r.jsx)(t.li,{children:"Nodes return online with inconsistent data - see steps 4 and 5."}),"\n"]}),"\n",(0,r.jsx)(t.admonition,{type:"note",children:(0,r.jsx)(t.p,{children:"Full state transfer and rebalancing might take a long time (tens of minutes). We suggest that you recover nodes as soon as you discover they are offline."})}),"\n",(0,r.jsxs)(t.ol,{start:"2",children:["\n",(0,r.jsxs)(t.li,{children:["Run the ",(0,r.jsx)(t.code,{children:"recovery partition states"})," command on the relevant zones/nodes/partitions to verify ",(0,r.jsx)(t.a,{href:"#partition-states",children:"Partition States"}),"."]}),"\n",(0,r.jsxs)(t.li,{children:["If the state is ",(0,r.jsx)(t.code,{children:"Healthy"})," or ",(0,r.jsx)(t.code,{children:"Available partition"}),", consider the recovery completed."]}),"\n",(0,r.jsxs)(t.li,{children:["If the state is ",(0,r.jsx)(t.code,{children:"Broken"}),":","\n",(0,r.jsxs)(t.ol,{children:["\n",(0,r.jsxs)(t.li,{children:["Restart your Apache Ignite nodes or the relevant partitions using the ",(0,r.jsx)(t.code,{children:"recovery partitions restart"})," command."]}),"\n",(0,r.jsxs)(t.li,{children:["Rerun the ",(0,r.jsx)(t.code,{children:"recovery partition states"})," command."]}),"\n",(0,r.jsxs)(t.li,{children:["If the partition state remains ",(0,r.jsx)(t.code,{children:"Broken"}),", reset the partitions with deletion by using the ",(0,r.jsx)(t.code,{children:"recovery partitions restart --with-cleanup"})," command. Local partition data will be deleted and restored from the cluster."]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(t.li,{children:["If the partition state is ",(0,r.jsx)(t.code,{children:"Read-only partition"}),", ",(0,r.jsx)(t.code,{children:"Degraded partition"}),", or ",(0,r.jsx)(t.code,{children:"Unavailable partition"}),", reset the relevant partitions using the ",(0,r.jsx)(t.code,{children:"recovery partitions reset"})," CLI command."]}),"\n"]}),"\n",(0,r.jsxs)(t.p,{children:["In the Majority Offline scenario, you would typically lose part of the data. For example, if you reset partition A while partition B was in the ",(0,r.jsx)(t.code,{children:"Available partition"})," state, you would lose:"]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:["The latest data from A that has been restored using ",(0,r.jsx)(t.code,{children:"recovery partitions reset"})]}),"\n",(0,r.jsx)(t.li,{children:"Some of the latest data from B, which had been inserted into it in a transaction that had also inserted data into A"}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"partition-loss",children:"Partition Loss"}),"\n",(0,r.jsxs)(t.p,{children:["In this scenario, in addition to having ",(0,r.jsx)(t.a,{href:"#majority-offline",children:"Majority Offline"}),", you lose all replicas of a partition, e.g., partition A. This causes a loss of ",(0,r.jsx)(t.em,{children:"all"})," the data from partition A once you run the ",(0,r.jsx)(t.code,{children:"recovery partitions reset"})," CLI command, as well possibly a loss of some of the recent updates in other partitions."]}),"\n",(0,r.jsxs)(t.p,{children:["Try bringing the nodes back online as described in the ",(0,r.jsx)(t.a,{href:"#majority-offline",children:"Majority Offline"})," scenario."]}),"\n",(0,r.jsx)(t.h2,{id:"partition-states",children:"Partition States"}),"\n",(0,r.jsx)(t.p,{children:"This section describes the data partition states that define the partition availability and readiness for utilization."}),"\n",(0,r.jsx)(t.h3,{id:"local-partition-states",children:"Local Partition States"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.em,{children:"Local partition state"})," is a local property of a replica, storage, state machine, etc., associated with the partition."]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"Healthy"})," - a state machine is running with no issues."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"Initializing"})," - a node is online, but the corresponding RAFT group has not completed its initialization yet."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"Snapshot installation"})," - a full state transfer is taking place. Once it has finished, the partition will become ",(0,r.jsx)(t.code,{children:"healthy"})," or ",(0,r.jsx)(t.code,{children:"catching-up"}),". Before that, data cannot be read, and log replication is on pause."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"Catching-up"})," - a node is in the process of replicating data from the leader, and its data is slightly in the past. More specifically, node has not replicated the tail of the log that corresponds to 100 log entries."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"Broken"})," - the state machine experiences issues (likely as a result of an exception). Some data might be unavailable for reading, and the log cannot be replicated. This state will not be changed automatically, it requires intervention."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"Unavailable"})," - state of the partition is currently unknown. It may happen when partition is not yet started or is already stopping."]}),"\n"]}),"\n",(0,r.jsx)(t.h3,{id:"global-partition-states",children:"Global Partition States"}),"\n",(0,r.jsxs)(t.p,{children:[(0,r.jsx)(t.em,{children:"Global partition state"})," is a global property of a partition that specifies its apparent functionality from user's point of view."]}),"\n",(0,r.jsxs)(t.ul,{children:["\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"Available"})," - a healthy partition that can process read and write requests. Implies that all peers are healthy at the moment."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"Read-only"})," - a partition that can process read requests but not the write requests. There is no healthy majority. However, there is at least one alive (healthy/catch-up) peer that can process historical read-only queries."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"Unavailable"})," - a partition that cannot process any requests."]}),"\n",(0,r.jsxs)(t.li,{children:[(0,r.jsx)(t.code,{children:"Degraded"})," - a partition that is available to the user, but is at a higher risk of having issues than other partitions. For example, one of the group's peers is offline. There is still a majority, but the backup factor is low."]}),"\n"]})]})}function h(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,r.jsx)(t,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);