"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[325],{20995:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>o,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"api-reference/native-clients/java/data-streamer-api","title":"Data Streamer API","description":"The Data Streamer API provides high-throughput data ingestion into Ignite tables. Applications stream data using reactive publishers that batch records for efficient network transmission and processing. This approach achieves higher performance than individual put operations.","source":"@site/docs/api-reference/native-clients/java/data-streamer-api.md","sourceDirName":"api-reference/native-clients/java","slug":"/api-reference/native-clients/java/data-streamer-api","permalink":"/docs/ignite3/3.1.0/api-reference/native-clients/java/data-streamer-api","draft":false,"unlisted":false,"tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Data Streamer API","id":"data-streamer-api","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Tables API","permalink":"/docs/ignite3/3.1.0/api-reference/native-clients/java/tables-api"},"next":{"title":"SQL API","permalink":"/docs/ignite3/3.1.0/api-reference/native-clients/java/sql-api"}}');var n=t(74848),i=t(28453);const s={title:"Data Streamer API",id:"data-streamer-api",sidebar_position:4},l="Data Streamer API",o={},c=[{value:"Key Concepts",id:"key-concepts",level:2},{value:"Basic Streaming",id:"basic-streaming",level:2},{value:"Stream Options",id:"stream-options",level:2},{value:"Operation Types",id:"operation-types",level:2},{value:"Custom Publishers",id:"custom-publishers",level:2},{value:"Receiver-Based Streaming",id:"receiver-based-streaming",level:2},{value:"Error Handling",id:"error-handling",level:2},{value:"Auto-Flush Interval",id:"auto-flush-interval",level:2},{value:"Key-Value View Streaming",id:"key-value-view-streaming",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Reference",id:"reference",level:2},{value:"DataStreamerTarget Methods",id:"datastreamertarget-methods",level:3},{value:"DataStreamerOptions Configuration",id:"datastreameroptions-configuration",level:3},{value:"DataStreamerItem Operations",id:"datastreameritem-operations",level:3},{value:"DataStreamerReceiver Interface",id:"datastreamerreceiver-interface",level:3}];function u(e){const r={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(r.header,{children:(0,n.jsx)(r.h1,{id:"data-streamer-api",children:"Data Streamer API"})}),"\n",(0,n.jsx)(r.p,{children:"The Data Streamer API provides high-throughput data ingestion into Ignite tables. Applications stream data using reactive publishers that batch records for efficient network transmission and processing. This approach achieves higher performance than individual put operations."}),"\n",(0,n.jsx)(r.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,n.jsx)(r.p,{children:"Data streaming uses the Java Flow API for backpressure-aware data delivery. Publishers produce items that contain operation types and payloads. The streamer batches items, sends them to appropriate nodes, and executes operations in parallel across partitions."}),"\n",(0,n.jsx)(r.p,{children:"Both RecordView and KeyValueView implement DataStreamerTarget, enabling streaming to either view type. Configure streaming behavior through DataStreamerOptions to control batch sizes, parallelism, and retry behavior."}),"\n",(0,n.jsx)(r.h2,{id:"basic-streaming",children:"Basic Streaming"}),"\n",(0,n.jsx)(r.p,{children:"Stream data using a Flow publisher:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'RecordView<Tuple> view = table.recordView();\r\n\r\n// Create publisher\r\nList<DataStreamerItem<Tuple>> items = Arrays.asList(\r\n    DataStreamerItem.of(Tuple.create().set("id", 1).set("name", "Alice")),\r\n    DataStreamerItem.of(Tuple.create().set("id", 2).set("name", "Bob")),\r\n    DataStreamerItem.of(Tuple.create().set("id", 3).set("name", "Carol"))\r\n);\r\n\r\nSubmissionPublisher<DataStreamerItem<Tuple>> publisher =\r\n    new SubmissionPublisher<>();\r\n\r\n// Stream data\r\nCompletableFuture<Void> future = view.streamData(\r\n    publisher,\r\n    DataStreamerOptions.DEFAULT\r\n);\r\n\r\n// Submit items\r\nitems.forEach(publisher::submit);\r\npublisher.close();\r\n\r\nfuture.join();\n'})}),"\n",(0,n.jsx)(r.p,{children:"The operation completes when all items are processed."}),"\n",(0,n.jsx)(r.h2,{id:"stream-options",children:"Stream Options"}),"\n",(0,n.jsx)(r.p,{children:"Configure streaming behavior with options:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:"DataStreamerOptions options = DataStreamerOptions.builder()\r\n    .pageSize(1000)\r\n    .perPartitionParallelOperations(4)\r\n    .autoFlushInterval(1000)\r\n    .retryLimit(3)\r\n    .build();\r\n\r\nCompletableFuture<Void> future = view.streamData(publisher, options);\n"})}),"\n",(0,n.jsx)(r.p,{children:"The pageSize parameter controls batch size. Higher values increase throughput but consume more memory. The perPartitionParallelOperations setting determines concurrent operations per partition."}),"\n",(0,n.jsx)(r.h2,{id:"operation-types",children:"Operation Types"}),"\n",(0,n.jsx)(r.p,{children:"Specify operation types for each item:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'List<DataStreamerItem<Tuple>> items = Arrays.asList(\r\n    DataStreamerItem.of(\r\n        Tuple.create().set("id", 1).set("name", "Alice"),\r\n        DataStreamerOperationType.PUT\r\n    ),\r\n    DataStreamerItem.of(\r\n        Tuple.create().set("id", 2).set("status", "active"),\r\n        DataStreamerOperationType.PUT\r\n    ),\r\n    DataStreamerItem.removed(\r\n        Tuple.create().set("id", 3)\r\n    ),\r\n    DataStreamerItem.of(\r\n        Tuple.create().set("id", 4).set("name", "David")\r\n    )\r\n);\n'})}),"\n",(0,n.jsx)(r.p,{children:"Available operations:"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:["PUT: Insert or update records (default when using ",(0,n.jsx)(r.code,{children:"of()"})," method)"]}),"\n",(0,n.jsxs)(r.li,{children:["REMOVE: Remove records (use ",(0,n.jsx)(r.code,{children:"removed()"})," method or explicit ",(0,n.jsx)(r.code,{children:"DataStreamerOperationType.REMOVE"}),")"]}),"\n"]}),"\n",(0,n.jsx)(r.h2,{id:"custom-publishers",children:"Custom Publishers"}),"\n",(0,n.jsx)(r.p,{children:"Implement custom publishers for streaming from external sources:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'class FilePublisher implements Flow.Publisher<DataStreamerItem<Tuple>> {\r\n    private final Path file;\r\n\r\n    public FilePublisher(Path file) {\r\n        this.file = file;\r\n    }\r\n\r\n    public void subscribe(Flow.Subscriber<? super DataStreamerItem<Tuple>> subscriber) {\r\n        subscriber.onSubscribe(new Flow.Subscription() {\r\n            private BufferedReader reader;\r\n\r\n            public void request(long n) {\r\n                try {\r\n                    if (reader == null) {\r\n                        reader = Files.newBufferedReader(file);\r\n                    }\r\n\r\n                    for (long i = 0; i < n; i++) {\r\n                        String line = reader.readLine();\r\n                        if (line == null) {\r\n                            reader.close();\r\n                            subscriber.onComplete();\r\n                            return;\r\n                        }\r\n\r\n                        String[] parts = line.split(",");\r\n                        Tuple tuple = Tuple.create()\r\n                            .set("id", Integer.parseInt(parts[0]))\r\n                            .set("name", parts[1]);\r\n\r\n                        subscriber.onNext(DataStreamerItem.of(tuple));\r\n                    }\r\n                } catch (IOException e) {\r\n                    subscriber.onError(e);\r\n                }\r\n            }\r\n\r\n            public void cancel() {\r\n                try {\r\n                    if (reader != null) {\r\n                        reader.close();\r\n                    }\r\n                } catch (IOException e) {\r\n                    // Ignore\r\n                }\r\n            }\r\n        });\r\n    }\r\n}\n'})}),"\n",(0,n.jsx)(r.p,{children:"Publishers must respect backpressure signals to avoid overwhelming the system."}),"\n",(0,n.jsx)(r.h2,{id:"receiver-based-streaming",children:"Receiver-Based Streaming"}),"\n",(0,n.jsx)(r.p,{children:"Execute custom processing logic on server nodes using receivers:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'class AggregationReceiver\r\n    implements DataStreamerReceiver<Tuple, String, Integer> {\r\n\r\n    @Override\r\n    public CompletableFuture<List<Integer>> receive(\r\n        List<Tuple> items,\r\n        DataStreamerReceiverContext context,\r\n        String arg\r\n    ) {\r\n        Table table = context.ignite().tables().table("aggregates");\r\n        RecordView<Tuple> view = table.recordView();\r\n\r\n        Map<String, Integer> counts = new HashMap<>();\r\n        for (Tuple item : items) {\r\n            String category = item.stringValue("category");\r\n            counts.merge(category, 1, Integer::sum);\r\n        }\r\n\r\n        for (Map.Entry<String, Integer> entry : counts.entrySet()) {\r\n            Tuple record = Tuple.create()\r\n                .set("category", entry.getKey())\r\n                .set("count", entry.getValue());\r\n            view.put(null, record);\r\n        }\r\n\r\n        return CompletableFuture.completedFuture(\r\n            Collections.singletonList(counts.size())\r\n        );\r\n    }\r\n}\n'})}),"\n",(0,n.jsx)(r.p,{children:"Register and use the receiver:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'DataStreamerReceiverDescriptor<Tuple, String, Integer> descriptor =\r\n    DataStreamerReceiverDescriptor.<Tuple, String, Integer>builder(\r\n        "com.example.AggregationReceiver"\r\n    ).build();\r\n\r\nSubmissionPublisher<Tuple> publisher = new SubmissionPublisher<>();\r\n\r\nCompletableFuture<Void> future = view.streamData(\r\n    publisher,\r\n    descriptor,\r\n    tuple -> tuple.value("id"),\r\n    tuple -> tuple,\r\n    "aggregation-arg",\r\n    null,\r\n    DataStreamerOptions.DEFAULT\r\n);\r\n\r\n// Submit items\r\nList<Tuple> items = Arrays.asList(\r\n    Tuple.create().set("id", 1).set("category", "A"),\r\n    Tuple.create().set("id", 2).set("category", "B")\r\n);\r\nitems.forEach(publisher::submit);\r\npublisher.close();\r\n\r\nfuture.join();\n'})}),"\n",(0,n.jsx)(r.p,{children:"Receivers process batches on server nodes, enabling custom logic like aggregations or complex transformations."}),"\n",(0,n.jsx)(r.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,n.jsx)(r.p,{children:"Handle streaming errors through the returned future:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'CompletableFuture<Void> future = view.streamData(publisher, options);\r\n\r\nfuture.exceptionally(ex -> {\r\n    if (ex instanceof DataStreamerException) {\r\n        System.err.println("Streaming failed: " + ex.getMessage());\r\n    }\r\n    return null;\r\n});\n'})}),"\n",(0,n.jsx)(r.p,{children:"Configure retry behavior through DataStreamerOptions.retryLimit to automatically retry failed batches."}),"\n",(0,n.jsx)(r.h2,{id:"auto-flush-interval",children:"Auto-Flush Interval"}),"\n",(0,n.jsx)(r.p,{children:"Configure periodic flushing for low-volume streams:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:"DataStreamerOptions options = DataStreamerOptions.builder()\r\n    .autoFlushInterval(500)\r\n    .build();\n"})}),"\n",(0,n.jsx)(r.p,{children:"The streamer flushes incomplete batches after the specified interval in milliseconds. This prevents data from remaining buffered indefinitely in low-throughput scenarios."}),"\n",(0,n.jsx)(r.h2,{id:"key-value-view-streaming",children:"Key-Value View Streaming"}),"\n",(0,n.jsx)(r.p,{children:"Stream to key-value views using Entry payloads:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'KeyValueView<Tuple, Tuple> kvView = table.keyValueView();\r\n\r\nList<DataStreamerItem<Map.Entry<Tuple, Tuple>>> items = Arrays.asList(\r\n    DataStreamerItem.of(Map.entry(\r\n        Tuple.create().set("id", 1),\r\n        Tuple.create().set("name", "Alice")\r\n    )),\r\n    DataStreamerItem.of(Map.entry(\r\n        Tuple.create().set("id", 2),\r\n        Tuple.create().set("name", "Bob")\r\n    ))\r\n);\r\n\r\nSubmissionPublisher<DataStreamerItem<Map.Entry<Tuple, Tuple>>> publisher =\r\n    new SubmissionPublisher<>();\r\n\r\nCompletableFuture<Void> future = kvView.streamData(publisher, DataStreamerOptions.DEFAULT);\r\n\r\nitems.forEach(publisher::submit);\r\npublisher.close();\r\n\r\nfuture.join();\n'})}),"\n",(0,n.jsx)(r.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,n.jsx)(r.p,{children:"Optimize streaming throughput by tuning configuration:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:"DataStreamerOptions options = DataStreamerOptions.builder()\r\n    .pageSize(10000)\r\n    .perPartitionParallelOperations(8)\r\n    .retryLimit(5)\r\n    .build();\n"})}),"\n",(0,n.jsx)(r.p,{children:"Larger page sizes reduce per-batch overhead but increase memory usage. Higher parallelism improves throughput on multi-core systems but may cause resource contention."}),"\n",(0,n.jsx)(r.h2,{id:"reference",children:"Reference"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:["Streaming interface: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerTarget<T>"})]}),"\n",(0,n.jsxs)(r.li,{children:["Configuration: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerOptions"})]}),"\n",(0,n.jsxs)(r.li,{children:["Stream items: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerItem<T>"})]}),"\n",(0,n.jsxs)(r.li,{children:["Custom processing: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerReceiver<T, A, R>"})]}),"\n",(0,n.jsxs)(r.li,{children:["Receiver context: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerReceiverContext"})]}),"\n",(0,n.jsxs)(r.li,{children:["Receiver descriptor: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerReceiverDescriptor<T, A, R>"})]}),"\n"]}),"\n",(0,n.jsx)(r.h3,{id:"datastreamertarget-methods",children:"DataStreamerTarget Methods"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"CompletableFuture<Void> streamData(Publisher<DataStreamerItem<T>>, DataStreamerOptions)"})," - Stream data to table"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"<E, V, A, R> CompletableFuture<Void> streamData(Publisher<E>, DataStreamerReceiverDescriptor<V, A, R>, Function<E, T>, Function<E, V>, A, Subscriber<R>, DataStreamerOptions)"})," - Stream with custom receiver"]}),"\n"]}),"\n",(0,n.jsx)(r.h3,{id:"datastreameroptions-configuration",children:"DataStreamerOptions Configuration"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"pageSize"})," - Number of items per batch (default: 1000)"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"perPartitionParallelOperations"})," - Concurrent operations per partition (default: 1)"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"autoFlushInterval"})," - Auto-flush interval in milliseconds (default: 5000)"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"retryLimit"})," - Number of retry attempts for failed batches (default: 16)"]}),"\n"]}),"\n",(0,n.jsx)(r.h3,{id:"datastreameritem-operations",children:"DataStreamerItem Operations"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"PUT"})," - Insert or update record"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"REMOVE"})," - Remove record"]}),"\n"]}),"\n",(0,n.jsx)(r.h3,{id:"datastreamerreceiver-interface",children:"DataStreamerReceiver Interface"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"CompletableFuture<List<R>> receive(List<T>, DataStreamerReceiverContext, A)"})," - Process batch on server"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"Marshaller<T, byte[]> payloadMarshaller()"})," - Custom payload serialization"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"Marshaller<A, byte[]> argumentMarshaller()"})," - Custom argument serialization"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"Marshaller<R, byte[]> resultMarshaller()"})," - Custom result serialization"]}),"\n"]})]})}function d(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(u,{...e})}):u(e)}},28453:(e,r,t)=>{t.d(r,{R:()=>s,x:()=>l});var a=t(96540);const n={},i=a.createContext(n);function s(e){const r=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function l(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),a.createElement(i.Provider,{value:r},e.children)}}}]);