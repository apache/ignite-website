---
title: "Apache Ignite 2.11: Stabilization First"
author: "Maxim Muzafarov"
date: 2021-09-20
tags:
    - database
    - ignite
    - in-memory
    - open-source
    - release
---

p
  | The new 
  a(href='https://ignite.apache.org/') Apache Ignite
  |  2.11 was released on September 17, 2021. It can be considered to be a greater
  | extent as a stabilization release that closed a number of technical debts of the internal architecture and bugs. Out of more than
  | 200 completed tasks, 120 are bug fixes. However, some valuable improvements still exist, so let&apos;s take a quick look at them together.
h3#thin-clients Thin Clients
p
  | Partition awareness is enabled by default in the 2.11 release and allows thin clients to send query requests directly to the
  | node that owns the queried data. Without partition awareness, an application executes all queries and operations via
  | a single server node that acts as a proxy for the incoming requests.

p
  | The support of 
  a(href='https://ignite.apache.org/docs/latest/thin-clients/java-thin-client#cache-entry-listening') Continuous Queries
  | added to the java thin client. For the other supported features, you can check -
  | the 
  a(href='https://cwiki.apache.org/confluence/display/IGNITE/Thin+clients+features') List of Thin Client Features
  | .
<!-- end -->
h3#cellular-clusters-deployment Cellular-clusters Deployment
p
  | The Apache Ignite internals has the so-called 
  em switch
  |  (a part of Partition Map Exchange) process that is used to perform
  | atomic execution of cluster-wide operations and move a cluster from one consistent state to another, for example, a cache creation/destroy,
  | a node JOIN/LEFT/FAIL operations, snapshot creation, etc. During the switching process, all user transactions are parked for a small
  | period of time which in turn increases the average latency and decreases throughput of the overall cluster.
p
  | Splitting the cluster into virtual cells containing 4-8 nodes may increase the total cluster performance and minimize the
  | influence of one cell on another in case of node fail events. Such a technique also significantly increases the recovery speed of
  | transactions on cells not affected by failing nodes. The time when transactions are parked also decreases on non-affected cells which
  | in turn decreases the worst latency for the cluster operations overall.
p
  | From now on, you can use the 
  em RendezvousAffinityFunction
  |  affinity function with 
  em ClusterNodeAttributeColocatedBackupFilter
  |  to
  | group nodes into virtual cells. Since the node baseline attributes are used as cell markers the corresponding 
  a(href='https://ignite.apache.org/docs/latest/monitoring-metrics/system-views#baseline_node_attributes') BASELINE_NODE_ATTRIBUTES
  |  system
  | view was added.
p
  | See benchmarks below that represent the worst (max) latency, which happens in case of node left/failure/timeout events on broken
  | and alive cells.
a(href='/img/blog/ec8a7800-01e9-4910-aaa9-0e27ea2d4303.png')
  img(src='/img/blog/ec8a7800-01e9-4910-aaa9-0e27ea2d4303.png' alt='723rhosidfgu4787fh9sdhf.png' width='50%')
h3#new-page-replacement-policies New Page Replacement Policies
p
  | When Native Persistence is on and the amount of data, which Ignite stores on the disk, is bigger than the off-heap memory amount
  | allocated for the data region, another page should be evicted from the off-heap to the disk to preload a page from the disk to
  | the completely full off-heap memory. This process is called page replacement. Previously, Apache Ignite used the Random-LRU page
  | replacement algorithm which has a low maintenance cost, but it has many disadvantages and greatly affects the performance when
  | the page replacement is started. On some deployments, administrators even force a cluster restart periodically to avoid page
  | replacement. There are a few new algorithms available from now on:
ul
  li Segmented-LRU Algorithm
  li CLOCK Algorithm
p
  | Page replacement algorithm can be configured by the 
  em PageReplacementMode
  |  property of 
  em DataRegionConfiguration
  | . By default, 
  | the CLOCK algorithm is now used. You can check the 
  a(href='https://ignite.apache.org/docs/latest/memory-configuration/replacement-policies') Replacement Policies
  |  in the documentation
  | for more details.
h3#snapshot-restore-and-check-commands Snapshot Restore And Check Commands
h4#check Check
p
  | All snapshots are fully consistent in terms of concurrent cluster-wide operations as well as ongoing changes with Ignite.
  | However, in some cases and for your own peace of mind, it may be necessary to check the snapshot for completeness and
  | for data consistency. The Apache Ignite is now delivered with a built-in snapshot consistency check commands that enable you to
  | verify internal data consistency, calculate data partitions hashes and pages checksums, and print out the result if a problem
  | is found. The check command also compares hashes calculated by containing keys of primary partitions with corresponding backup
  | partitions and reports any differences.

code 
  pre # This procedure does not require the cluster to be in the idle state.
  pre control.(sh|bat) --snapshot check snapshot_name
  
h4#restore Restore
p
  | Previously, only the manual snapshot restore procedure was available by fully copying persistence data files from the 
  | snapshot directory to the Apache Ignite 
  em work
  |  directory. The automatic restore procedure allows you to restore cache groups from
  | a snapshot on an active cluster by using the Java API or command line script (using CLI is recommended).  Currently, the restore
  | procedure has several limitations, so please check the documentation pages for details.

code 
  pre Start restoring all user-created cache groups from the snapshot "snapshot_09062021".
  pre control.(sh|bat) --snapshot restore snapshot_09062021 --start
  br
  pre # Start restoring only "cache-group1" and "cache-group2" from the snapshot "snapshot_09062021".
  pre control.(sh|bat) --snapshot restore snapshot_09062021 --start cache-group1,cache-group2
  br
  pre # Get the status of the restore operation for "snapshot_09062021".
  pre control.(sh|bat) --snapshot restore snapshot_09062021 --status
  br
  pre # Cancel the restore operation for "snapshot_09062021".
  pre control.(sh|bat) --snapshot restore snapshot_09062021 --cancel
  
