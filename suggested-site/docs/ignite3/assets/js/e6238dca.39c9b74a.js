"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[4443],{11015:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>h,frontMatter:()=>s,metadata:()=>a,toc:()=>d});const a=JSON.parse('{"id":"understand/core-concepts/data-partitioning","title":"Data Partitioning","description":"Data partitioning divides table data into fixed-size chunks called partitions and distributes them across cluster nodes. This distribution enables horizontal scaling: adding nodes increases both storage capacity and query throughput.","source":"@site/docs/understand/core-concepts/data-partitioning.md","sourceDirName":"understand/core-concepts","slug":"/understand/core-concepts/data-partitioning","permalink":"/docs/ignite3/3.1.0/understand/core-concepts/data-partitioning","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/ignite-3/tree/main/docs/docs/understand/core-concepts/data-partitioning.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"id":"data-partitioning","title":"Data Partitioning","sidebar_position":2},"sidebar":"tutorialSidebar","previous":{"title":"Distribution and Colocation","permalink":"/docs/ignite3/3.1.0/understand/core-concepts/distribution-and-colocation"},"next":{"title":"Compute and Events","permalink":"/docs/ignite3/3.1.0/understand/core-concepts/compute-and-events"}}');var t=i(74848),r=i(28453);const s={id:"data-partitioning",title:"Data Partitioning",sidebar_position:2},o="Data Partitioning",l={},d=[{value:"Partition Distribution",id:"partition-distribution",level:2},{value:"Partition Number",id:"partition-number",level:3},{value:"Replica Number",id:"replica-number",level:3},{value:"Primary Replicas and Leases",id:"primary-replicas-and-leases",level:2},{value:"Reading Data From Replicas",id:"reading-data-from-replicas",level:3},{value:"Version Storage",id:"version-storage",level:2},{value:"Distribution Reset",id:"distribution-reset",level:2},{value:"Partition Rebalance",id:"partition-rebalance",level:2}];function c(e){const n={a:"a",admonition:"admonition",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",mermaid:"mermaid",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"data-partitioning",children:"Data Partitioning"})}),"\n",(0,t.jsx)(n.p,{children:"Data partitioning divides table data into fixed-size chunks called partitions and distributes them across cluster nodes. This distribution enables horizontal scaling: adding nodes increases both storage capacity and query throughput."}),"\n",(0,t.jsx)(n.h2,{id:"partition-distribution",children:"Partition Distribution"}),"\n",(0,t.jsxs)(n.p,{children:["Tables belong to distribution zones, which define how data is partitioned and replicated. The zone's ",(0,t.jsx)(n.code,{children:"PARTITIONS"})," parameter sets the number of partitions, and ",(0,t.jsx)(n.code,{children:"REPLICAS"})," sets how many copies of each partition exist."]}),"\n",(0,t.jsx)(n.mermaid,{value:'flowchart TB\n    subgraph "Distribution Zone: financial"\n        direction TB\n        ZC[PARTITIONS: 6<br/>REPLICAS: 3]\n    end\n\n    subgraph "Table: accounts"\n        direction LR\n        P0[Partition 0]\n        P1[Partition 1]\n        P2[Partition 2]\n        P3[Partition 3]\n        P4[Partition 4]\n        P5[Partition 5]\n    end\n\n    subgraph "Cluster Nodes"\n        subgraph "Node 1"\n            N1P0[P0 Primary]\n            N1P3[P3 Backup]\n            N1P4[P4 Backup]\n        end\n        subgraph "Node 2"\n            N2P1[P1 Primary]\n            N2P0[P0 Backup]\n            N2P5[P5 Backup]\n        end\n        subgraph "Node 3"\n            N3P2[P2 Primary]\n            N3P1[P1 Backup]\n            N3P3[P3 Primary]\n        end\n        subgraph "Node 4"\n            N4P4[P4 Primary]\n            N4P2[P2 Backup]\n            N4P5[P5 Primary]\n        end\n    end\n\n    ZC --\x3e P0 & P1 & P2 & P3 & P4 & P5\n    P0 --\x3e N1P0 & N2P0\n    P1 --\x3e N2P1 & N3P1\n    P2 --\x3e N3P2 & N4P2\n    P3 --\x3e N3P3 & N1P3\n    P4 --\x3e N4P4 & N1P4\n    P5 --\x3e N4P5 & N2P5'}),"\n",(0,t.jsx)(n.p,{children:"Key behaviors:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Each partition is identified by a number (0 to PARTITIONS-1)"}),"\n",(0,t.jsx)(n.li,{children:"Replicas of the same partition are placed on different nodes when possible"}),"\n",(0,t.jsx)(n.li,{children:"Tables in the same zone share partition-to-node mappings (enabling colocation)"}),"\n",(0,t.jsx)(n.li,{children:"The Fair distribution algorithm stores placement decisions in metastorage and reuses them for consistent assignment"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:["You can configure the way node stores relevant information in the ",(0,t.jsx)(n.a,{href:"/3.1.0/configure-and-operate/reference/node-configuration",children:"node configuration"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"ignite.system.partitionsBasePath"})," defines the folder partitions are stored in. By default, partitions are stored in the ",(0,t.jsx)(n.code,{children:"work/partitions"})," folder."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"ignite.system.partitionsLogPath"})," defines the folder where partition-specific RAFT logs are stored. These logs contain information on RAFT elections and consensus."]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.code,{children:"ignite.system.metastoragePath"})," defines the folder where cluster metadata is stored. It is recommended to store metadata on a separate device from partitions."]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"partition-number",children:"Partition Number"}),"\n",(0,t.jsxs)(n.p,{children:["When creating a distribution zone, you have an option to manually set the number of partitions with the ",(0,t.jsx)(n.code,{children:"PARTITIONS"})," parameter, for example:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-sql",children:"CREATE ZONE IF NOT EXISTS exampleZone (PARTITIONS 10) STORAGE PROFILES ['default'];\n"})}),"\n",(0,t.jsx)(n.p,{children:"As partitions will be spread across the cluster, we recommend to set the number of partitions depending on its size and the number of available cores."}),"\n",(0,t.jsx)(n.p,{children:"In most cases, we recommend using 2, 3 or 4 times the number of total available cores, divided by the number of replicas as the number of partitions. For example:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"For a cluster with 3 nodes, 8 cores on each node, and 3 data replicas, we recommend using 16, 24 or 32 partitions."}),"\n",(0,t.jsx)(n.li,{children:"For a cluster with 7 nodes, 16 cores on each node, and 3 data replicas, we recommend using 75, 112 or 150 partitions."}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"It is not recommended to set a significantly larger number of partitions or replicas, as maintaining partitions and their distribution can cause a performance drain on the cluster."}),"\n",(0,t.jsx)(n.p,{children:"Otherwise, Apache Ignite will automatically calculate the recommended number of partitions:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"dataNodesCount * coresOnNode * 2 / replicas\n"})}),"\n",(0,t.jsxs)(n.p,{children:["In this case, the ",(0,t.jsx)(n.code,{children:"dataNodesCount"})," is the estimated number of nodes that will be in the distribution zone when it is created, according to its ",(0,t.jsx)(n.a,{href:"/3.1.0/sql/reference/language-definition/distribution-zones",children:"filter"})," and ",(0,t.jsx)(n.a,{href:"/3.1.0/understand/architecture/storage-architecture",children:"storage profiles"}),". At least 1 partition is always created."]}),"\n",(0,t.jsx)(n.h3,{id:"replica-number",children:"Replica Number"}),"\n",(0,t.jsxs)(n.p,{children:["When creating a distribution zone, you can configure the number of ",(0,t.jsx)(n.em,{children:"replicas"})," (individual copies of data on the cluster) by setting the ",(0,t.jsx)(n.code,{children:"REPLICAS"})," parameter. By default, no additional replicas of data are created. As more replicas are added, additional copies of data will be stored on the cluster, and automatically spread to ensure data availability in case of a node leaving the cluster."]}),"\n",(0,t.jsxs)(n.p,{children:["Replicas of each partition form a RAFT group, and a ",(0,t.jsx)(n.a,{href:"/3.1.0/sql/reference/language-definition/distribution-zones",children:"quorum"})," in that group is required to perform updates to the partition. The default quorum size depends on the number of replicas in the distribution zone: 3 replicas are required for quorum if the distribution zone has 5 or more replicas, 2 if there are between 2 and 4 replicas, or 1 if only one data replica exists."]}),"\n",(0,t.jsxs)(n.p,{children:["Some replicas will be selected as part of a consensus group. These nodes will be voting members, confirming all data changes in the replication group, while other replicas will be ",(0,t.jsx)(n.em,{children:"learners"}),", only passively receiving data from the group leader and not participating in elections."]}),"\n",(0,t.jsxs)(n.p,{children:["Losing the majority of the consensus group leads the partition to enter the ",(0,t.jsx)(n.code,{children:"Read-only"})," state. In this state, no data can be written and only explicit read-only transactions can be used to retrieve data. If the distribution zone ",(0,t.jsx)(n.a,{href:"/3.1.0/sql/reference/language-definition/distribution-zones",children:"scales"})," up or down (typically, due to a node entering or leaving the cluster), new replicas will be selected as the consensus group."]}),"\n",(0,t.jsx)(n.p,{children:"The size of the consensus group is automatically calculated based on quorum size:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-text",children:"quorumSize * 2 - 1\n"})}),"\n",(0,t.jsx)(n.p,{children:"For example, with 5 replicas and quorum size of 2, 3 replicas will be part of consensus group, and 2 replicas will be learners. In this scenario, losing 2 nodes will lead to some partitions losing the majority of the consensus group and becoming unavailable. For this reason, it is recommended to have a quorum size of 3 for a 5-node cluster."}),"\n",(0,t.jsxs)(n.p,{children:["It is recommended to always have an odd number of replicas and at least 3 replicas of your data on the cluster. When only 2 data replicas exist, losing one will always lead to losing majority, while having 3 or 5 data replicas will allow the cluster to stay functional in ",(0,t.jsx)(n.a,{href:"/3.1.0/configure-and-operate/operations/lifecycle",children:"network segmentation"})," scenarios."]}),"\n",(0,t.jsxs)(n.p,{children:["You can also store data replicas on every node in cluster by creating a zone with ",(0,t.jsx)(n.code,{children:"REPLICAS ALL"})," parameter to ensure data is always available to the cluster."]}),"\n",(0,t.jsx)(n.h2,{id:"primary-replicas-and-leases",children:"Primary Replicas and Leases"}),"\n",(0,t.jsx)(n.p,{children:"Once partitions are distributed, Apache Ignite forms replication groups for each partition. Each group elects a leader through RAFT consensus, and the placement driver grants a lease to designate one replica as primary."}),"\n",(0,t.jsx)(n.mermaid,{value:'flowchart TB\n    subgraph "Placement Driver"\n        PD[Lease Placement Driver]\n        MS[(Metastorage)]\n    end\n\n    subgraph "Partition 0 Replication Group"\n        subgraph "Node 1"\n            R1[Replica<br/>PRIMARY<br/>Lease Holder]\n        end\n        subgraph "Node 2"\n            R2[Replica<br/>BACKUP<br/>Voter]\n        end\n        subgraph "Node 3"\n            R3[Replica<br/>BACKUP<br/>Learner]\n        end\n    end\n\n    PD --\x3e|"Grant Lease"| R1\n    PD --\x3e|"Store Lease"| MS\n    R1 <--\x3e|"RAFT Consensus"| R2\n    R1 --\x3e|"Replicate"| R3\n\n    Client[Client] --\x3e|"Read-Write TX"| R1\n    Client -.->|"Read-Only TX"| R2\n    Client -.->|"Read-Only TX"| R3'}),"\n",(0,t.jsx)(n.p,{children:"The lease mechanism provides:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Write linearization"}),": Only the primary replica handles read-write transactions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Lease renewal"}),": Leases are extended periodically to maintain continuity"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Automatic failover"}),": When a lease expires, the placement driver negotiates a new primary"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Only the primary replica can handle operations of read-write transactions. Other replicas of the partition can be read from by using read-only transactions."}),"\n",(0,t.jsxs)(n.p,{children:["If a new replica is chosen to receive the lease, it first makes sure it is up-to-date with its replication group based on the stored data. In scenarios where replication group is no longer operable (for example, a node unexpectedly leaves the cluster and the group loses majority), it follows the ",(0,t.jsx)(n.a,{href:"/3.1.0/configure-and-operate/operations/disaster-recovery-partitions",children:"disaster recovery"})," procedure, and you may need to reset the partitions manually."]}),"\n",(0,t.jsx)(n.h3,{id:"reading-data-from-replicas",children:"Reading Data From Replicas"}),"\n",(0,t.jsxs)(n.p,{children:["Reading data as part of a read-write ",(0,t.jsx)(n.a,{href:"/3.1.0/develop/work-with-data/transactions",children:"transaction"})," is always handled by the primary data replica."]}),"\n",(0,t.jsx)(n.p,{children:"Read-only transactions can be handled by either backup or primary replicas, depending on the specifics of the transaction."}),"\n",(0,t.jsx)(n.h2,{id:"version-storage",children:"Version Storage"}),"\n",(0,t.jsx)(n.p,{children:"Apache Ignite maintains multiple versions of each row to support MVCC (Multi-Version Concurrency Control). When a row is updated, the old version is preserved in a version chain rather than being deleted immediately."}),"\n",(0,t.jsx)(n.mermaid,{value:'flowchart LR\n    subgraph "Version Chain for Key: 42"\n        V3[Version 3<br/>ts: 1500<br/>value: 300]\n        V2[Version 2<br/>ts: 1200<br/>value: 200]\n        V1[Version 1<br/>ts: 1000<br/>value: 100]\n    end\n\n    V3 --\x3e V2 --\x3e V1\n\n    subgraph "Transactions"\n        RW[Read-Write TX<br/>sees: V3]\n        RO1[Read-Only TX<br/>at ts: 1400<br/>sees: V2]\n        RO2[Read-Only TX<br/>at ts: 1100<br/>sees: V1]\n    end\n\n    RW -.-> V3\n    RO1 -.-> V2\n    RO2 -.-> V1\n\n    LW[Low Watermark<br/>ts: 1100] --\x3e V1\n    GC[Garbage Collector] --\x3e LW'}),"\n",(0,t.jsx)(n.p,{children:"Version visibility:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Read-write transactions see the latest version"}),"\n",(0,t.jsx)(n.li,{children:"Read-only transactions see the version valid at their start timestamp"}),"\n",(0,t.jsx)(n.li,{children:"Versions older than the low watermark are eligible for garbage collection"}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"The low watermark defaults to 600000 ms (10 minutes). Versions beyond this threshold are cleaned up by the garbage collector, though cleanup is deferred if any active transaction still requires the data."}),"\n",(0,t.jsxs)(n.p,{children:["In a similar manner, ",(0,t.jsx)(n.a,{href:"/3.1.0/sql/reference/language-definition/ddl#drop-table",children:"dropped tables"})," are also not removed from disk until the low watermark point, however you can no longer write to these tables. Read-only transactions that try to get data from these tables will succeed if they read data at timestamp before the table was dropped, and will delay the low watermark point if it is necessary to complete the transaction."]}),"\n",(0,t.jsx)(n.p,{children:"Once the low watermark is reached, old versions of data are considered garbage and will be cleaned up by garbage collector during the next cleanup. This data may or may not be available, as garbage collection is not an immediate process. If a transaction was already started before the low watermark was reached, the required data will be kept available until the end of transaction even if the garbage collection happens. Additionally, Apache Ignite checks that old data is not required anywhere on the cluster before cleaning up the data."}),"\n",(0,t.jsx)(n.h2,{id:"distribution-reset",children:"Distribution Reset"}),"\n",(0,t.jsxs)(n.p,{children:["The SQL query performance can deteriorate in a cluster where tables had been created over a long period, alongside topology changes, due to sub-optimum data colocation. To resolve this issue, you can reset (recalculate) partition distribution using ",(0,t.jsx)(n.a,{href:"/3.1.0/tools/cli-commands",children:"CLI"})," or ",(0,t.jsx)(n.a,{href:"/3.1.0/tools/rest-api",children:"REST API"}),"."]}),"\n",(0,t.jsx)(n.admonition,{type:"note",children:(0,t.jsx)(n.p,{children:"Reset is likely to result in Partition Rebalance, which may take a long time."})}),"\n",(0,t.jsx)(n.h2,{id:"partition-rebalance",children:"Partition Rebalance"}),"\n",(0,t.jsxs)(n.p,{children:["When the ",(0,t.jsx)(n.a,{href:"/3.1.0/sql/reference/language-definition/distribution-zones",children:"cluster size changes"}),", Apache Ignite waits for the timeout specified in the ",(0,t.jsx)(n.code,{children:"AUTO SCALE UP"})," or ",(0,t.jsx)(n.code,{children:"AUTO SCALE DOWN"})," distribution zone properties, and then redistributes partitions according to partition distribution algorithm and transfers data to make it up-to-date with the replication group. This process is called ",(0,t.jsx)(n.em,{children:"data rebalance"}),"."]})]})}function h(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(c,{...e})}):c(e)}},28453:(e,n,i)=>{i.d(n,{R:()=>s,x:()=>o});var a=i(96540);const t={},r=a.createContext(t);function s(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);