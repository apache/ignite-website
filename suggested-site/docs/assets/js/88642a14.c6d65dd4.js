"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[325],{20995:(e,r,t)=>{t.r(r),t.d(r,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>s,metadata:()=>a,toc:()=>c});const a=JSON.parse('{"id":"api-reference/native-clients/java/data-streamer-api","title":"Data Streamer API","description":"The Data Streamer API provides high-throughput data ingestion into Ignite tables. Applications stream data using reactive publishers that batch records for efficient network transmission and processing. This approach achieves higher performance than individual put operations.","source":"@site/docs/api-reference/native-clients/java/data-streamer-api.md","sourceDirName":"api-reference/native-clients/java","slug":"/api-reference/native-clients/java/data-streamer-api","permalink":"/suggested-site/docs/3.1.0/api-reference/native-clients/java/data-streamer-api","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/ignite-3/tree/main/docs/docs/api-reference/native-clients/java/data-streamer-api.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"title":"Data Streamer API","id":"data-streamer-api","sidebar_position":4},"sidebar":"tutorialSidebar","previous":{"title":"Tables API","permalink":"/suggested-site/docs/3.1.0/api-reference/native-clients/java/tables-api"},"next":{"title":"SQL API","permalink":"/suggested-site/docs/3.1.0/api-reference/native-clients/java/sql-api"}}');var n=t(74848),i=t(28453);const s={title:"Data Streamer API",id:"data-streamer-api",sidebar_position:4},l="Data Streamer API",o={},c=[{value:"Key Concepts",id:"key-concepts",level:2},{value:"Basic Streaming",id:"basic-streaming",level:2},{value:"Stream Options",id:"stream-options",level:2},{value:"Operation Types",id:"operation-types",level:2},{value:"Custom Publishers",id:"custom-publishers",level:2},{value:"Receiver-Based Streaming",id:"receiver-based-streaming",level:2},{value:"Error Handling",id:"error-handling",level:2},{value:"Auto-Flush Interval",id:"auto-flush-interval",level:2},{value:"Key-Value View Streaming",id:"key-value-view-streaming",level:2},{value:"Performance Considerations",id:"performance-considerations",level:2},{value:"Reference",id:"reference",level:2},{value:"DataStreamerTarget Methods",id:"datastreamertarget-methods",level:3},{value:"DataStreamerOptions Configuration",id:"datastreameroptions-configuration",level:3},{value:"DataStreamerItem Operations",id:"datastreameritem-operations",level:3},{value:"DataStreamerReceiver Interface",id:"datastreamerreceiver-interface",level:3}];function d(e){const r={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(r.header,{children:(0,n.jsx)(r.h1,{id:"data-streamer-api",children:"Data Streamer API"})}),"\n",(0,n.jsx)(r.p,{children:"The Data Streamer API provides high-throughput data ingestion into Ignite tables. Applications stream data using reactive publishers that batch records for efficient network transmission and processing. This approach achieves higher performance than individual put operations."}),"\n",(0,n.jsx)(r.h2,{id:"key-concepts",children:"Key Concepts"}),"\n",(0,n.jsx)(r.p,{children:"Data streaming uses the Java Flow API for backpressure-aware data delivery. Publishers produce items that contain operation types and payloads. The streamer batches items, sends them to appropriate nodes, and executes operations in parallel across partitions."}),"\n",(0,n.jsx)(r.p,{children:"Both RecordView and KeyValueView implement DataStreamerTarget, enabling streaming to either view type. Configure streaming behavior through DataStreamerOptions to control batch sizes, parallelism, and retry behavior."}),"\n",(0,n.jsx)(r.h2,{id:"basic-streaming",children:"Basic Streaming"}),"\n",(0,n.jsx)(r.p,{children:"Stream data using a Flow publisher:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'RecordView<Tuple> view = table.recordView();\n\n// Create publisher\nList<DataStreamerItem<Tuple>> items = Arrays.asList(\n    DataStreamerItem.of(Tuple.create().set("id", 1).set("name", "Alice")),\n    DataStreamerItem.of(Tuple.create().set("id", 2).set("name", "Bob")),\n    DataStreamerItem.of(Tuple.create().set("id", 3).set("name", "Carol"))\n);\n\nSubmissionPublisher<DataStreamerItem<Tuple>> publisher =\n    new SubmissionPublisher<>();\n\n// Stream data\nCompletableFuture<Void> future = view.streamData(\n    publisher,\n    DataStreamerOptions.DEFAULT\n);\n\n// Submit items\nitems.forEach(publisher::submit);\npublisher.close();\n\nfuture.join();\n'})}),"\n",(0,n.jsx)(r.p,{children:"The operation completes when all items are processed."}),"\n",(0,n.jsx)(r.h2,{id:"stream-options",children:"Stream Options"}),"\n",(0,n.jsx)(r.p,{children:"Configure streaming behavior with options:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:"DataStreamerOptions options = DataStreamerOptions.builder()\n    .pageSize(1000)\n    .perPartitionParallelOperations(4)\n    .autoFlushInterval(1000)\n    .retryLimit(3)\n    .build();\n\nCompletableFuture<Void> future = view.streamData(publisher, options);\n"})}),"\n",(0,n.jsx)(r.p,{children:"The pageSize parameter controls batch size. Higher values increase throughput but consume more memory. The perPartitionParallelOperations setting determines concurrent operations per partition."}),"\n",(0,n.jsx)(r.h2,{id:"operation-types",children:"Operation Types"}),"\n",(0,n.jsx)(r.p,{children:"Specify operation types for each item:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'List<DataStreamerItem<Tuple>> items = Arrays.asList(\n    DataStreamerItem.of(\n        Tuple.create().set("id", 1).set("name", "Alice"),\n        DataStreamerOperationType.PUT\n    ),\n    DataStreamerItem.of(\n        Tuple.create().set("id", 2).set("status", "active"),\n        DataStreamerOperationType.PUT\n    ),\n    DataStreamerItem.removed(\n        Tuple.create().set("id", 3)\n    ),\n    DataStreamerItem.of(\n        Tuple.create().set("id", 4).set("name", "David")\n    )\n);\n'})}),"\n",(0,n.jsx)(r.p,{children:"Available operations:"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:["PUT: Insert or update records (default when using ",(0,n.jsx)(r.code,{children:"of()"})," method)"]}),"\n",(0,n.jsxs)(r.li,{children:["REMOVE: Remove records (use ",(0,n.jsx)(r.code,{children:"removed()"})," method or explicit ",(0,n.jsx)(r.code,{children:"DataStreamerOperationType.REMOVE"}),")"]}),"\n"]}),"\n",(0,n.jsx)(r.h2,{id:"custom-publishers",children:"Custom Publishers"}),"\n",(0,n.jsx)(r.p,{children:"Implement custom publishers for streaming from external sources:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'class FilePublisher implements Flow.Publisher<DataStreamerItem<Tuple>> {\n    private final Path file;\n\n    public FilePublisher(Path file) {\n        this.file = file;\n    }\n\n    public void subscribe(Flow.Subscriber<? super DataStreamerItem<Tuple>> subscriber) {\n        subscriber.onSubscribe(new Flow.Subscription() {\n            private BufferedReader reader;\n\n            public void request(long n) {\n                try {\n                    if (reader == null) {\n                        reader = Files.newBufferedReader(file);\n                    }\n\n                    for (long i = 0; i < n; i++) {\n                        String line = reader.readLine();\n                        if (line == null) {\n                            reader.close();\n                            subscriber.onComplete();\n                            return;\n                        }\n\n                        String[] parts = line.split(",");\n                        Tuple tuple = Tuple.create()\n                            .set("id", Integer.parseInt(parts[0]))\n                            .set("name", parts[1]);\n\n                        subscriber.onNext(DataStreamerItem.of(tuple));\n                    }\n                } catch (IOException e) {\n                    subscriber.onError(e);\n                }\n            }\n\n            public void cancel() {\n                try {\n                    if (reader != null) {\n                        reader.close();\n                    }\n                } catch (IOException e) {\n                    // Ignore\n                }\n            }\n        });\n    }\n}\n'})}),"\n",(0,n.jsx)(r.p,{children:"Publishers must respect backpressure signals to avoid overwhelming the system."}),"\n",(0,n.jsx)(r.h2,{id:"receiver-based-streaming",children:"Receiver-Based Streaming"}),"\n",(0,n.jsx)(r.p,{children:"Execute custom processing logic on server nodes using receivers:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'class AggregationReceiver\n    implements DataStreamerReceiver<Tuple, String, Integer> {\n\n    @Override\n    public CompletableFuture<List<Integer>> receive(\n        List<Tuple> items,\n        DataStreamerReceiverContext context,\n        String arg\n    ) {\n        Table table = context.ignite().tables().table("aggregates");\n        RecordView<Tuple> view = table.recordView();\n\n        Map<String, Integer> counts = new HashMap<>();\n        for (Tuple item : items) {\n            String category = item.stringValue("category");\n            counts.merge(category, 1, Integer::sum);\n        }\n\n        for (Map.Entry<String, Integer> entry : counts.entrySet()) {\n            Tuple record = Tuple.create()\n                .set("category", entry.getKey())\n                .set("count", entry.getValue());\n            view.put(null, record);\n        }\n\n        return CompletableFuture.completedFuture(\n            Collections.singletonList(counts.size())\n        );\n    }\n}\n'})}),"\n",(0,n.jsx)(r.p,{children:"Register and use the receiver:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'DataStreamerReceiverDescriptor<Tuple, String, Integer> descriptor =\n    DataStreamerReceiverDescriptor.<Tuple, String, Integer>builder(\n        "com.example.AggregationReceiver"\n    ).build();\n\nSubmissionPublisher<Tuple> publisher = new SubmissionPublisher<>();\n\nCompletableFuture<Void> future = view.streamData(\n    publisher,\n    descriptor,\n    tuple -> tuple.value("id"),\n    tuple -> tuple,\n    "aggregation-arg",\n    null,\n    DataStreamerOptions.DEFAULT\n);\n\n// Submit items\nList<Tuple> items = Arrays.asList(\n    Tuple.create().set("id", 1).set("category", "A"),\n    Tuple.create().set("id", 2).set("category", "B")\n);\nitems.forEach(publisher::submit);\npublisher.close();\n\nfuture.join();\n'})}),"\n",(0,n.jsx)(r.p,{children:"Receivers process batches on server nodes, enabling custom logic like aggregations or complex transformations."}),"\n",(0,n.jsx)(r.h2,{id:"error-handling",children:"Error Handling"}),"\n",(0,n.jsx)(r.p,{children:"Handle streaming errors through the returned future:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'CompletableFuture<Void> future = view.streamData(publisher, options);\n\nfuture.exceptionally(ex -> {\n    if (ex instanceof DataStreamerException) {\n        System.err.println("Streaming failed: " + ex.getMessage());\n    }\n    return null;\n});\n'})}),"\n",(0,n.jsx)(r.p,{children:"Configure retry behavior through DataStreamerOptions.retryLimit to automatically retry failed batches."}),"\n",(0,n.jsx)(r.h2,{id:"auto-flush-interval",children:"Auto-Flush Interval"}),"\n",(0,n.jsx)(r.p,{children:"Configure periodic flushing for low-volume streams:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:"DataStreamerOptions options = DataStreamerOptions.builder()\n    .autoFlushInterval(500)\n    .build();\n"})}),"\n",(0,n.jsx)(r.p,{children:"The streamer flushes incomplete batches after the specified interval in milliseconds. This prevents data from remaining buffered indefinitely in low-throughput scenarios."}),"\n",(0,n.jsx)(r.h2,{id:"key-value-view-streaming",children:"Key-Value View Streaming"}),"\n",(0,n.jsx)(r.p,{children:"Stream to key-value views using Entry payloads:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:'KeyValueView<Tuple, Tuple> kvView = table.keyValueView();\n\nList<DataStreamerItem<Map.Entry<Tuple, Tuple>>> items = Arrays.asList(\n    DataStreamerItem.of(Map.entry(\n        Tuple.create().set("id", 1),\n        Tuple.create().set("name", "Alice")\n    )),\n    DataStreamerItem.of(Map.entry(\n        Tuple.create().set("id", 2),\n        Tuple.create().set("name", "Bob")\n    ))\n);\n\nSubmissionPublisher<DataStreamerItem<Map.Entry<Tuple, Tuple>>> publisher =\n    new SubmissionPublisher<>();\n\nCompletableFuture<Void> future = kvView.streamData(publisher, DataStreamerOptions.DEFAULT);\n\nitems.forEach(publisher::submit);\npublisher.close();\n\nfuture.join();\n'})}),"\n",(0,n.jsx)(r.h2,{id:"performance-considerations",children:"Performance Considerations"}),"\n",(0,n.jsx)(r.p,{children:"Optimize streaming throughput by tuning configuration:"}),"\n",(0,n.jsx)(r.pre,{children:(0,n.jsx)(r.code,{className:"language-java",children:"DataStreamerOptions options = DataStreamerOptions.builder()\n    .pageSize(10000)\n    .perPartitionParallelOperations(8)\n    .retryLimit(5)\n    .build();\n"})}),"\n",(0,n.jsx)(r.p,{children:"Larger page sizes reduce per-batch overhead but increase memory usage. Higher parallelism improves throughput on multi-core systems but may cause resource contention."}),"\n",(0,n.jsx)(r.h2,{id:"reference",children:"Reference"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:["Streaming interface: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerTarget<T>"})]}),"\n",(0,n.jsxs)(r.li,{children:["Configuration: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerOptions"})]}),"\n",(0,n.jsxs)(r.li,{children:["Stream items: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerItem<T>"})]}),"\n",(0,n.jsxs)(r.li,{children:["Custom processing: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerReceiver<T, A, R>"})]}),"\n",(0,n.jsxs)(r.li,{children:["Receiver context: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerReceiverContext"})]}),"\n",(0,n.jsxs)(r.li,{children:["Receiver descriptor: ",(0,n.jsx)(r.code,{children:"org.apache.ignite.table.DataStreamerReceiverDescriptor<T, A, R>"})]}),"\n"]}),"\n",(0,n.jsx)(r.h3,{id:"datastreamertarget-methods",children:"DataStreamerTarget Methods"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"CompletableFuture<Void> streamData(Publisher<DataStreamerItem<T>>, DataStreamerOptions)"})," - Stream data to table"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"<E, V, A, R> CompletableFuture<Void> streamData(Publisher<E>, DataStreamerReceiverDescriptor<V, A, R>, Function<E, T>, Function<E, V>, A, Subscriber<R>, DataStreamerOptions)"})," - Stream with custom receiver"]}),"\n"]}),"\n",(0,n.jsx)(r.h3,{id:"datastreameroptions-configuration",children:"DataStreamerOptions Configuration"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"pageSize"})," - Number of items per batch (default: 1000)"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"perPartitionParallelOperations"})," - Concurrent operations per partition (default: 1)"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"autoFlushInterval"})," - Auto-flush interval in milliseconds (default: 5000)"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"retryLimit"})," - Number of retry attempts for failed batches (default: 16)"]}),"\n"]}),"\n",(0,n.jsx)(r.h3,{id:"datastreameritem-operations",children:"DataStreamerItem Operations"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"PUT"})," - Insert or update record"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"REMOVE"})," - Remove record"]}),"\n"]}),"\n",(0,n.jsx)(r.h3,{id:"datastreamerreceiver-interface",children:"DataStreamerReceiver Interface"}),"\n",(0,n.jsxs)(r.ul,{children:["\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"CompletableFuture<List<R>> receive(List<T>, DataStreamerReceiverContext, A)"})," - Process batch on server"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"Marshaller<T, byte[]> payloadMarshaller()"})," - Custom payload serialization"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"Marshaller<A, byte[]> argumentMarshaller()"})," - Custom argument serialization"]}),"\n",(0,n.jsxs)(r.li,{children:[(0,n.jsx)(r.code,{children:"Marshaller<R, byte[]> resultMarshaller()"})," - Custom result serialization"]}),"\n"]})]})}function u(e={}){const{wrapper:r}={...(0,i.R)(),...e.components};return r?(0,n.jsx)(r,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},28453:(e,r,t)=>{t.d(r,{R:()=>s,x:()=>l});var a=t(96540);const n={},i=a.createContext(n);function s(e){const r=a.useContext(i);return a.useMemo(function(){return"function"==typeof e?e(r):{...r,...e}},[r,e])}function l(e){let r;return r=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:s(e.components),a.createElement(i.Provider,{value:r},e.children)}}}]);