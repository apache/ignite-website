"use strict";(globalThis.webpackChunkdocs=globalThis.webpackChunkdocs||[]).push([[3309],{11470:(e,n,r)=>{r.d(n,{A:()=>S});var t=r(96540),s=r(34164),a=r(17559),i=r(23104),l=r(56347),o=r(205),c=r(57485),d=r(31682),u=r(70679);function h(e){return t.Children.toArray(e).filter(e=>"\n"!==e).map(e=>{if(!e||(0,t.isValidElement)(e)&&function(e){const{props:n}=e;return!!n&&"object"==typeof n&&"value"in n}(e))return e;throw new Error(`Docusaurus error: Bad <Tabs> child <${"string"==typeof e.type?e.type:e.type.name}>: all children of the <Tabs> component should be <TabItem>, and every <TabItem> should have a unique "value" prop.`)})?.filter(Boolean)??[]}function p(e){const{values:n,children:r}=e;return(0,t.useMemo)(()=>{const e=n??function(e){return h(e).map(({props:{value:e,label:n,attributes:r,default:t}})=>({value:e,label:n,attributes:r,default:t}))}(r);return function(e){const n=(0,d.XI)(e,(e,n)=>e.value===n.value);if(n.length>0)throw new Error(`Docusaurus error: Duplicate values "${n.map(e=>e.value).join(", ")}" found in <Tabs>. Every value needs to be unique.`)}(e),e},[n,r])}function m({value:e,tabValues:n}){return n.some(n=>n.value===e)}function b({queryString:e=!1,groupId:n}){const r=(0,l.W6)(),s=function({queryString:e=!1,groupId:n}){if("string"==typeof e)return e;if(!1===e)return null;if(!0===e&&!n)throw new Error('Docusaurus error: The <Tabs> component groupId prop is required if queryString=true, because this value is used as the search param name. You can also provide an explicit value such as queryString="my-search-param".');return n??null}({queryString:e,groupId:n});return[(0,c.aZ)(s),(0,t.useCallback)(e=>{if(!s)return;const n=new URLSearchParams(r.location.search);n.set(s,e),r.replace({...r.location,search:n.toString()})},[s,r])]}function g(e){const{defaultValue:n,queryString:r=!1,groupId:s}=e,a=p(e),[i,l]=(0,t.useState)(()=>function({defaultValue:e,tabValues:n}){if(0===n.length)throw new Error("Docusaurus error: the <Tabs> component requires at least one <TabItem> children component");if(e){if(!m({value:e,tabValues:n}))throw new Error(`Docusaurus error: The <Tabs> has a defaultValue "${e}" but none of its children has the corresponding value. Available values are: ${n.map(e=>e.value).join(", ")}. If you intend to show no default tab, use defaultValue={null} instead.`);return e}const r=n.find(e=>e.default)??n[0];if(!r)throw new Error("Unexpected error: 0 tabValues");return r.value}({defaultValue:n,tabValues:a})),[c,d]=b({queryString:r,groupId:s}),[h,g]=function({groupId:e}){const n=function(e){return e?`docusaurus.tab.${e}`:null}(e),[r,s]=(0,u.Dv)(n);return[r,(0,t.useCallback)(e=>{n&&s.set(e)},[n,s])]}({groupId:s}),x=(()=>{const e=c??h;return m({value:e,tabValues:a})?e:null})();(0,o.A)(()=>{x&&l(x)},[x]);return{selectedValue:i,selectValue:(0,t.useCallback)(e=>{if(!m({value:e,tabValues:a}))throw new Error(`Can't select invalid tab value=${e}`);l(e),d(e),g(e)},[d,g,a]),tabValues:a}}var x=r(92303);const f={tabList:"tabList__CuJ",tabItem:"tabItem_LNqP"};var v=r(74848);function j({className:e,block:n,selectedValue:r,selectValue:t,tabValues:a}){const l=[],{blockElementScrollPositionUntilNextRender:o}=(0,i.a_)(),c=e=>{const n=e.currentTarget,s=l.indexOf(n),i=a[s].value;i!==r&&(o(n),t(i))},d=e=>{let n=null;switch(e.key){case"Enter":c(e);break;case"ArrowRight":{const r=l.indexOf(e.currentTarget)+1;n=l[r]??l[0];break}case"ArrowLeft":{const r=l.indexOf(e.currentTarget)-1;n=l[r]??l[l.length-1];break}}n?.focus()};return(0,v.jsx)("ul",{role:"tablist","aria-orientation":"horizontal",className:(0,s.A)("tabs",{"tabs--block":n},e),children:a.map(({value:e,label:n,attributes:t})=>(0,v.jsx)("li",{role:"tab",tabIndex:r===e?0:-1,"aria-selected":r===e,ref:e=>{l.push(e)},onKeyDown:d,onClick:c,...t,className:(0,s.A)("tabs__item",f.tabItem,t?.className,{"tabs__item--active":r===e}),children:n??e},e))})}function w({lazy:e,children:n,selectedValue:r}){const a=(Array.isArray(n)?n:[n]).filter(Boolean);if(e){const e=a.find(e=>e.props.value===r);return e?(0,t.cloneElement)(e,{className:(0,s.A)("margin-top--md",e.props.className)}):null}return(0,v.jsx)("div",{className:"margin-top--md",children:a.map((e,n)=>(0,t.cloneElement)(e,{key:n,hidden:e.props.value!==r}))})}function y(e){const n=g(e);return(0,v.jsxs)("div",{className:(0,s.A)(a.G.tabs.container,"tabs-container",f.tabList),children:[(0,v.jsx)(j,{...n,...e}),(0,v.jsx)(w,{...n,...e})]})}function S(e){const n=(0,x.A)();return(0,v.jsx)(y,{...e,children:h(e.children)},String(n))}},19365:(e,n,r)=>{r.d(n,{A:()=>i});r(96540);var t=r(34164);const s={tabItem:"tabItem_Ymn6"};var a=r(74848);function i({children:e,hidden:n,className:r}){return(0,a.jsx)("div",{role:"tabpanel",className:(0,t.A)(s.tabItem,r),hidden:n,children:e})}},28453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>l});var t=r(96540);const s={},a=t.createContext(s);function i(e){const n=t.useContext(a);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),t.createElement(a.Provider,{value:n},e.children)}},82776:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>d,contentTitle:()=>c,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>u});const t=JSON.parse('{"id":"develop/work-with-data/streaming","title":"Streaming Data","description":"Apache Ignite participates in streaming architectures as both a data consumer and data source. The Data Streamer API accepts entries from any Java Flow publisher, partitions them by key, and delivers batches to cluster nodes for processing. You can stream data into tables, use receivers to transform and route data across multiple tables, or return computed results to downstream systems.","source":"@site/docs/develop/work-with-data/streaming.md","sourceDirName":"develop/work-with-data","slug":"/develop/work-with-data/streaming","permalink":"/suggested-site/docs/3.1.0/develop/work-with-data/streaming","draft":false,"unlisted":false,"editUrl":"https://github.com/apache/ignite-3/tree/main/docs/docs/develop/work-with-data/streaming.md","tags":[],"version":"current","frontMatter":{"id":"streaming","title":"Streaming Data"},"sidebar":"tutorialSidebar","previous":{"title":"Performing Transactions","permalink":"/suggested-site/docs/3.1.0/develop/work-with-data/transactions"},"next":{"title":"Distributed Computing","permalink":"/suggested-site/docs/3.1.0/develop/work-with-data/compute"}}');var s=r(74848),a=r(28453),i=r(11470),l=r(19365);const o={id:"streaming",title:"Streaming Data"},c=void 0,d={},u=[{value:"Using Data Streamer API",id:"using-data-streamer-api",level:2},{value:"Configuring Data Streamer",id:"configuring-data-streamer",level:3},{value:"Memory and Throughput Considerations",id:"memory-and-throughput-considerations",level:4},{value:"Streaming Data",id:"streaming-data",level:3},{value:"High-Volume Streaming",id:"high-volume-streaming",level:2},{value:"Understanding Backpressure",id:"understanding-backpressure",level:3},{value:"Choosing a Publisher Approach",id:"choosing-a-publisher-approach",level:3},{value:"Flow API Handshake",id:"flow-api-handshake",level:3},{value:"Demand-Driven Publisher",id:"demand-driven-publisher",level:3},{value:"Configuration Tuning",id:"configuration-tuning",level:3},{value:"Streaming Results Out",id:"streaming-results-out",level:2},{value:"Understanding Receivers",id:"understanding-receivers",level:3},{value:"Enrichment Example",id:"enrichment-example",level:3},{value:"Step 1: Create an Enrichment Receiver",id:"step-1-create-an-enrichment-receiver",level:4},{value:"Step 2: Create a Forwarding Subscriber",id:"step-2-create-a-forwarding-subscriber",level:4},{value:"Step 3: Wire the Pipeline",id:"step-3-wire-the-pipeline",level:4},{value:"Managing Result Flow",id:"managing-result-flow",level:3},{value:"Handling Failures",id:"handling-failures",level:2},{value:"Handling Persistent Failures",id:"handling-persistent-failures",level:3},{value:"Tuning Flush Behavior",id:"tuning-flush-behavior",level:2}];function h(e){const n={a:"a",code:"code",h2:"h2",h3:"h3",h4:"h4",li:"li",mermaid:"mermaid",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.p,{children:"Apache Ignite participates in streaming architectures as both a data consumer and data source. The Data Streamer API accepts entries from any Java Flow publisher, partitions them by key, and delivers batches to cluster nodes for processing. You can stream data into tables, use receivers to transform and route data across multiple tables, or return computed results to downstream systems."}),"\n",(0,s.jsx)(n.mermaid,{value:'flowchart LR\n    subgraph Sources["Data Sources"]\n        S1[Message Queues]\n        S2[Batch Files]\n        S3[Applications]\n        S1 ~~~ S2 ~~~ S3\n    end\n\n    subgraph Pipeline["Streaming Pipeline"]\n        PUB[Flow.Publisher] --\x3e STREAMER[Data Streamer]\n        STREAMER --\x3e BUF[(Partition<br/>Buffers)]\n    end\n\n    subgraph Ignite["Apache Ignite"]\n        N1[Node 1]\n        N2[Node 2]\n        N3[Node N]\n        N1 ~~~ N2 ~~~ N3\n    end\n\n    subgraph Targets["Data Targets"]\n        T1[Tables]\n        T2[Receivers]\n        T3[Result Subscribers]\n        T1 ~~~ T2 ~~~ T3\n    end\n\n    Sources --\x3e PUB\n    BUF --\x3e Ignite\n    Ignite --\x3e Targets'}),"\n",(0,s.jsx)(n.p,{children:"Data streaming provides at-least-once delivery guarantee. Under normal operation, each item is delivered exactly once. If a batch fails and is retried, some items in that batch may be delivered again. Design your data model to handle potential duplicates, either through idempotent operations (upsert) or by using primary keys that allow safe re-insertion."}),"\n",(0,s.jsx)(n.h2,{id:"using-data-streamer-api",children:"Using Data Streamer API"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.a,{href:"../../api-reference/native-clients/java/data-streamer-api",children:"Data Streamer API"})," uses the Java Flow API (",(0,s.jsx)(n.code,{children:"java.util.concurrent.Flow"}),") publisher-subscriber model. You create a publisher that streams data entries to a table view, and the streamer distributes these entries across the cluster. The ",(0,s.jsx)(n.code,{children:"DataStreamerOptions"})," object configures batch sizes, parallelism, auto-flush intervals, and retry limits."]}),"\n",(0,s.jsx)(n.h3,{id:"configuring-data-streamer",children:"Configuring Data Streamer"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.code,{children:"DataStreamerOptions"})," controls how data flows into your cluster:"]}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(l.A,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"DataStreamerOptions options = DataStreamerOptions.builder()\n    .pageSize(1000)\n    .perPartitionParallelOperations(1)\n    .autoFlushInterval(5000)\n    .retryLimit(16)\n    .build();\n"})})}),(0,s.jsx)(l.A,{value:"dotnet",label:".NET",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"var options = new DataStreamerOptions\n{\n    PageSize = 1000,\n    RetryLimit = 16,\n    AutoFlushInterval = TimeSpan.FromSeconds(5)\n};\n"})})})]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Option"}),(0,s.jsx)(n.th,{children:"Default"}),(0,s.jsx)(n.th,{children:"Description"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"pageSize"})}),(0,s.jsx)(n.td,{children:"1000"}),(0,s.jsx)(n.td,{children:"Number of entries per batch sent to the cluster."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"perPartitionParallelOperations"})}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"Concurrent batches allowed per partition."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"autoFlushInterval"})}),(0,s.jsx)(n.td,{children:"5000 ms"}),(0,s.jsx)(n.td,{children:"Time before incomplete buffers are flushed."})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"retryLimit"})}),(0,s.jsx)(n.td,{children:"16"}),(0,s.jsx)(n.td,{children:"Maximum retry attempts for failed submissions."})]})]})]}),"\n",(0,s.jsx)(n.h4,{id:"memory-and-throughput-considerations",children:"Memory and Throughput Considerations"}),"\n",(0,s.jsx)(n.p,{children:"The streamer allocates buffers based on these settings. Client memory usage per partition:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"memoryPerPartition = pageSize \xd7 perPartitionParallelOperations \xd7 avgRecordSize\n"})}),"\n",(0,s.jsx)(n.p,{children:"For a table with 25 partitions and 1KB average record size:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Configuration"}),(0,s.jsx)(n.th,{children:"Page Size"}),(0,s.jsx)(n.th,{children:"Parallel Ops"}),(0,s.jsx)(n.th,{children:"Memory"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Default"}),(0,s.jsx)(n.td,{children:"1,000"}),(0,s.jsx)(n.td,{children:"1"}),(0,s.jsx)(n.td,{children:"~25 MB"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"High throughput"}),(0,s.jsx)(n.td,{children:"500"}),(0,s.jsx)(n.td,{children:"8"}),(0,s.jsx)(n.td,{children:"~100 MB"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Smaller page sizes with higher parallelism create more frequent, smaller batches. This produces smoother I/O patterns that align with cluster checkpoint and replication cycles."}),"\n",(0,s.jsx)(n.h3,{id:"streaming-data",children:"Streaming Data"}),"\n",(0,s.jsxs)(n.p,{children:["Each entry must be wrapped in a ",(0,s.jsx)(n.code,{children:"DataStreamerItem<T>"})," before streaming:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"DataStreamerItem.of(entry)"})," performs an upsert: inserts the entry if the key does not exist, or updates it if the key already exists."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"DataStreamerItem.removed(entry)"})," deletes the entry by key. The entry object only needs to contain the primary key fields."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The example below uses ",(0,s.jsx)(n.code,{children:"SubmissionPublisher"})," to stream account records:"]}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(l.A,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'public class DataStreamerExample {\n    private static final int ACCOUNTS_COUNT = 1000;\n\n    public static void main(String[] args) throws Exception {\n        try (IgniteClient client = IgniteClient.builder()\n                .addresses("127.0.0.1:10800")\n                .build()) {\n            RecordView<Account> view = client.tables().table("accounts").recordView(Account.class);\n\n            streamAccountDataPut(view);\n            streamAccountDataRemove(view);\n        }\n    }\n\n    private static void streamAccountDataPut(RecordView<Account> view) {\n        DataStreamerOptions options = DataStreamerOptions.builder()\n                .pageSize(1000)\n                .perPartitionParallelOperations(1)\n                .autoFlushInterval(5000)\n                .retryLimit(16)\n                .build();\n\n        CompletableFuture<Void> streamerFut;\n        try (var publisher = new SubmissionPublisher<DataStreamerItem<Account>>()) {\n            streamerFut = view.streamData(publisher, options);\n            ThreadLocalRandom rnd = ThreadLocalRandom.current();\n            for (int i = 0; i < ACCOUNTS_COUNT; i++) {\n                Account entry = new Account(i, "name" + i, rnd.nextLong(100_000), rnd.nextBoolean());\n                publisher.submit(DataStreamerItem.of(entry));\n            }\n        }\n        streamerFut.join();\n    }\n\n    private static void streamAccountDataRemove(RecordView<Account> view) {\n        DataStreamerOptions options = DataStreamerOptions.builder()\n                .pageSize(1000)\n                .perPartitionParallelOperations(1)\n                .autoFlushInterval(5000)\n                .retryLimit(16)\n                .build();\n\n        CompletableFuture<Void> streamerFut;\n        try (var publisher = new SubmissionPublisher<DataStreamerItem<Account>>()) {\n            streamerFut = view.streamData(publisher, options);\n            for (int i = 0; i < ACCOUNTS_COUNT; i++) {\n                Account entry = new Account(i);\n                publisher.submit(DataStreamerItem.removed(entry));\n            }\n        }\n        streamerFut.join();\n    }\n}\n'})})}),(0,s.jsx)(l.A,{value:"dotnet",label:".NET",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'using Apache.Ignite;\nusing Apache.Ignite.Table;\n\nusing var client = await IgniteClient.StartAsync(new("localhost"));\nITable? table = await client.Tables.GetTableAsync("accounts");\nIRecordView<Account> view = table!.GetRecordView<Account>();\n\nvar options = new DataStreamerOptions\n{\n    PageSize = 10_000,\n    AutoFlushInterval = TimeSpan.FromSeconds(1),\n    RetryLimit = 32\n};\n\nawait view.StreamDataAsync(GetAccountsToAdd(5_000), options);\nawait view.StreamDataAsync(GetAccountsToRemove(1_000), options);\n\nasync IAsyncEnumerable<DataStreamerItem<Account>> GetAccountsToAdd(int count)\n{\n    for (int i = 0; i < count; i++)\n    {\n        yield return DataStreamerItem.Create(\n            new Account(i, $"Account {i}"));\n    }\n}\n\nasync IAsyncEnumerable<DataStreamerItem<Account>> GetAccountsToRemove(int count)\n{\n    for (int i = 0; i < count; i++)\n    {\n        yield return DataStreamerItem.Create(\n            new Account(i, string.Empty), DataStreamerOperationType.Remove);\n    }\n}\n\npublic record Account(int Id, string Name);\n'})})})]}),"\n",(0,s.jsx)(n.h2,{id:"high-volume-streaming",children:"High-Volume Streaming"}),"\n",(0,s.jsx)(n.p,{children:"The basic examples above work well for moderate data volumes. When streaming millions of records, the rate at which your application produces data typically exceeds the rate at which the cluster can persist it. This section covers flow control mechanisms that keep memory bounded and cluster load stable."}),"\n",(0,s.jsx)(n.h3,{id:"understanding-backpressure",children:"Understanding Backpressure"}),"\n",(0,s.jsx)(n.p,{children:"Backpressure is a flow control mechanism that allows a slower consumer to signal a faster producer to slow down. Without backpressure, a fast producer overwhelms a slow consumer, causing unbounded memory growth, increased latency, or dropped data."}),"\n",(0,s.jsx)(n.p,{children:"In data streaming, three components operate at different speeds:"}),"\n",(0,s.jsx)(n.mermaid,{value:'flowchart LR\n    subgraph Producer["Data Producer"]\n        GEN["Record Generation<br/>(CPU-bound)"]\n    end\n\n    subgraph Network["Network Layer"]\n        BUF["Partition Buffers<br/>(Memory-bound)"]\n    end\n\n    subgraph Storage["Cluster Storage"]\n        DISK["Disk Writes + Replication<br/>(I/O-bound)"]\n    end\n\n    GEN --\x3e|"Fast"| BUF\n    BUF --\x3e|"Varies"| DISK'}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Record generation"})," runs at CPU speed, potentially millions of records per second"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Network transmission"})," depends on bandwidth and batch size"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cluster storage"})," involves disk writes, replication, and consensus protocols"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The Java Flow API provides backpressure through a request-based model. The consumer (data streamer) tells the producer (your publisher) exactly how many items it can accept by calling ",(0,s.jsx)(n.code,{children:"subscription.request(n)"}),". The producer should deliver at most ",(0,s.jsx)(n.code,{children:"n"})," items, then wait for the next request. This creates a pull-based flow where the slowest component controls the overall rate."]}),"\n",(0,s.jsxs)(n.p,{children:["When the streamer's partition buffers fill, it stops calling ",(0,s.jsx)(n.code,{children:"request(n)"}),". A well-behaved publisher pauses generation until the next request arrives. This keeps memory bounded and prevents the producer from overwhelming downstream components."]}),"\n",(0,s.jsx)(n.h3,{id:"choosing-a-publisher-approach",children:"Choosing a Publisher Approach"}),"\n",(0,s.jsx)(n.p,{children:"How your publisher responds to backpressure signals determines memory usage and cluster load patterns. Two common approaches exist:"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"SubmissionPublisher"})," (shown in the basic example above) provides a ready-made implementation with an internal buffer (default 256 items). Records go into this buffer via ",(0,s.jsx)(n.code,{children:"submit()"}),", and the streamer pulls from it. When the buffer fills, ",(0,s.jsx)(n.code,{children:"submit()"})," blocks until space becomes available. This provides backpressure through blocking, which works well when your data source can tolerate pauses."]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Demand-driven publishers"})," generate records only when the streamer requests them via ",(0,s.jsx)(n.code,{children:"request(n)"}),". Instead of blocking on a full buffer, generation simply pauses when demand is zero and resumes when new requests arrive. This approach requires implementing ",(0,s.jsx)(n.code,{children:"Flow.Publisher"})," and ",(0,s.jsx)(n.code,{children:"Flow.Subscription"}),", but gives precise control over when data is created."]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Consideration"}),(0,s.jsx)(n.th,{children:"SubmissionPublisher"}),(0,s.jsx)(n.th,{children:"Demand-Driven Publisher"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Backpressure response"}),(0,s.jsxs)(n.td,{children:[(0,s.jsx)(n.code,{children:"submit()"})," blocks when buffer full"]}),(0,s.jsx)(n.td,{children:"Generation pauses when demand is zero"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Memory profile"}),(0,s.jsx)(n.td,{children:"Publisher buffer + partition buffers"}),(0,s.jsx)(n.td,{children:"Partition buffers only"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Implementation effort"}),(0,s.jsx)(n.td,{children:"Minimal (JDK provided)"}),(0,s.jsx)(n.td,{children:"Custom implementation required"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Use case"}),(0,s.jsx)(n.td,{children:"Existing data sources, moderate volumes"}),(0,s.jsx)(n.td,{children:"Synthetic data, large volumes, controlled generation"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:["For streaming from existing collections or external data sources, ",(0,s.jsx)(n.code,{children:"SubmissionPublisher"})," offers simplicity. For generating or transforming millions of records, a demand-driven publisher provides predictable memory usage by creating data only when the cluster is ready to accept it."]}),"\n",(0,s.jsx)(n.h3,{id:"flow-api-handshake",children:"Flow API Handshake"}),"\n",(0,s.jsx)(n.p,{children:"The streaming lifecycle follows this sequence:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"view.streamData(publisher, options)"})," initiates streaming"]}),"\n",(0,s.jsxs)(n.li,{children:["The streamer calls ",(0,s.jsx)(n.code,{children:"publisher.subscribe(subscriber)"})]}),"\n",(0,s.jsxs)(n.li,{children:["Your publisher calls ",(0,s.jsx)(n.code,{children:"subscriber.onSubscribe(subscription)"})]}),"\n",(0,s.jsxs)(n.li,{children:["The streamer calls ",(0,s.jsx)(n.code,{children:"subscription.request(n)"})," when ready for items"]}),"\n",(0,s.jsxs)(n.li,{children:["Your publisher generates ",(0,s.jsx)(n.code,{children:"n"})," items and calls ",(0,s.jsx)(n.code,{children:"subscriber.onNext(item)"})," for each"]}),"\n",(0,s.jsxs)(n.li,{children:["Steps 4-5 repeat until your publisher calls ",(0,s.jsx)(n.code,{children:"subscriber.onComplete()"})]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The streamer calculates how many items to request:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-text",children:"desiredInFlight = Math.max(1, buffers.size()) \xd7 pageSize \xd7 perPartitionParallelOperations\ntoRequest = desiredInFlight - inFlight - pending\n"})}),"\n",(0,s.jsxs)(n.p,{children:["When ",(0,s.jsx)(n.code,{children:"toRequest"})," is zero or negative, the streamer applies backpressure by not requesting more items."]}),"\n",(0,s.jsx)(n.h3,{id:"demand-driven-publisher",children:"Demand-Driven Publisher"}),"\n",(0,s.jsx)(n.p,{children:"A demand-driven publisher inverts the control flow. Instead of pushing records into a buffer and waiting for space, the publisher waits for the streamer to request records and generates them on demand. This requires implementing two interfaces:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Flow.Publisher<T>"})}),": The outer class that accepts a subscriber and creates a subscription"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:(0,s.jsx)(n.code,{children:"Flow.Subscription"})}),": The inner class that tracks demand and delivers items"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The subscription maintains an ",(0,s.jsx)(n.code,{children:"AtomicLong"})," counter for outstanding demand. When the streamer calls ",(0,s.jsx)(n.code,{children:"request(n)"}),", the subscription increments the counter and begins delivery. Each ",(0,s.jsx)(n.code,{children:"onNext()"})," call decrements the counter. When demand reaches zero, the publisher pauses until the next ",(0,s.jsx)(n.code,{children:"request(n)"})," call."]}),"\n",(0,s.jsx)(n.mermaid,{value:"sequenceDiagram\n    participant Pub as DemandDrivenPublisher\n    participant Sub as Subscription\n    participant Str as StreamerSubscriber\n    participant Buf as Partition Buffers\n    participant Cluster as Ignite Cluster\n\n    Pub->>Str: subscribe(subscriber)\n    Str->>Sub: onSubscribe(subscription)\n\n    loop Until onComplete()\n        Str->>Sub: request(n)\n        Note over Sub: demand.addAndGet(n)\n\n        loop While demand > 0\n            Sub->>Sub: generateItem()\n            Sub->>Str: onNext(item)\n            Note over Sub: demand.decrementAndGet()\n        end\n\n        Str->>Buf: buffer by partition\n\n        alt Buffer reaches pageSize\n            Buf->>Cluster: send batch\n            Cluster--\x3e>Str: batch complete\n        end\n    end\n\n    Sub->>Str: onComplete()"}),"\n",(0,s.jsx)(n.p,{children:"The following implementation demonstrates this pattern:"}),"\n",(0,s.jsx)(i.A,{children:(0,s.jsx)(l.A,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'public class DemandDrivenPublisher implements Flow.Publisher<DataStreamerItem<Tuple>> {\n\n    private final long totalRecords;\n    private final AtomicBoolean subscribed = new AtomicBoolean(false);\n\n    public DemandDrivenPublisher(long totalRecords) {\n        this.totalRecords = totalRecords;\n    }\n\n    @Override\n    public void subscribe(Flow.Subscriber<? super DataStreamerItem<Tuple>> subscriber) {\n        // Flow.Publisher allows only one subscriber\n        if (subscribed.compareAndSet(false, true)) {\n            subscriber.onSubscribe(new DemandDrivenSubscription(subscriber));\n        } else {\n            subscriber.onError(new IllegalStateException("Publisher already has a subscriber"));\n        }\n    }\n\n    private class DemandDrivenSubscription implements Flow.Subscription {\n        private final Flow.Subscriber<? super DataStreamerItem<Tuple>> subscriber;\n        private final AtomicLong demand = new AtomicLong(0);\n        private final AtomicLong generated = new AtomicLong(0);\n        private final AtomicBoolean cancelled = new AtomicBoolean(false);\n\n        DemandDrivenSubscription(Flow.Subscriber<? super DataStreamerItem<Tuple>> subscriber) {\n            this.subscriber = subscriber;\n        }\n\n        @Override\n        public void request(long n) {\n            if (n <= 0 || cancelled.get()) {\n                return;\n            }\n            // Accumulate demand from streamer\n            demand.addAndGet(n);\n            deliverItems();\n        }\n\n        @Override\n        public void cancel() {\n            cancelled.set(true);\n        }\n\n        private void deliverItems() {\n            // Generate items only while demand exists\n            while (demand.get() > 0 && !cancelled.get()) {\n                long index = generated.get();\n\n                if (index >= totalRecords) {\n                    // Signal completion when all records are generated\n                    subscriber.onComplete();\n                    return;\n                }\n\n                // Generate record on-demand (replace with your data source)\n                Tuple tuple = Tuple.create()\n                    .set("id", index)\n                    .set("data", "Record " + index);\n\n                subscriber.onNext(DataStreamerItem.of(tuple));\n                generated.incrementAndGet();\n                demand.decrementAndGet();\n            }\n            // Loop exits when demand reaches zero; publisher pauses until next request(n)\n        }\n    }\n}\n'})})})}),"\n",(0,s.jsxs)(n.p,{children:["Use the publisher with ",(0,s.jsx)(n.code,{children:"streamData"}),":"]}),"\n",(0,s.jsx)(i.A,{children:(0,s.jsx)(l.A,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'RecordView<Tuple> view = client.tables().table("records").recordView();\n\nDataStreamerOptions options = DataStreamerOptions.builder()\n    .pageSize(500)\n    .perPartitionParallelOperations(8)\n    .autoFlushInterval(1000)\n    .build();\n\nDemandDrivenPublisher publisher = new DemandDrivenPublisher(20_000_000);\nCompletableFuture<Void> streamFuture = view.streamData(publisher, options);\nstreamFuture.join();\n'})})})}),"\n",(0,s.jsx)(n.h3,{id:"configuration-tuning",children:"Configuration Tuning"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"pageSize"})," and ",(0,s.jsx)(n.code,{children:"perPartitionParallelOperations"})," settings control batch size and concurrency. Their interaction affects throughput and cluster load:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Configuration"}),(0,s.jsx)(n.th,{children:"Behavior"}),(0,s.jsx)(n.th,{children:"Tradeoff"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsxs)(n.td,{children:["Large ",(0,s.jsx)(n.code,{children:"pageSize"}),", low parallelism (e.g., 1000 \xd7 1)"]}),(0,s.jsx)(n.td,{children:"Fewer, larger batches"}),(0,s.jsx)(n.td,{children:"Lower network overhead, but uneven I/O"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsxs)(n.td,{children:["Small ",(0,s.jsx)(n.code,{children:"pageSize"}),", high parallelism (e.g., 500 \xd7 8)"]}),(0,s.jsx)(n.td,{children:"More frequent, smaller batches"}),(0,s.jsx)(n.td,{children:"Higher network overhead, but steadier I/O"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:"Cluster nodes periodically flush data to disk and synchronize replicas. Batch size and timing can affect how streaming interacts with these operations. Monitor your cluster's performance metrics and adjust settings based on observed behavior."}),"\n",(0,s.jsx)(n.h2,{id:"streaming-results-out",children:"Streaming Results Out"}),"\n",(0,s.jsx)(n.p,{children:"The previous sections covered streaming data into Ignite tables. To stream data through Ignite and forward results to downstream systems, use receivers with result subscribers. A receiver processes incoming batches on cluster nodes and returns results that flow back to your application for delivery to the next pipeline stage."}),"\n",(0,s.jsxs)(n.p,{children:["Receivers execute on cluster nodes, not on the client. The receiver class must be deployed to all cluster nodes before streaming begins. See ",(0,s.jsx)(n.a,{href:"./code-deployment",children:"Code Deployment"})," for deployment options."]}),"\n",(0,s.jsxs)(n.p,{children:["Result delivery differs from input streaming: the streamer does not respect backpressure signals from ",(0,s.jsx)(n.code,{children:"subscription.request(n)"})," on the result subscriber. Results arrive as fast as the cluster produces them, regardless of how many items your subscriber requests. Your subscriber must handle results without blocking or buffer them internally when downstream systems are slower than the cluster."]}),"\n",(0,s.jsx)(n.mermaid,{value:'flowchart LR\n    subgraph Client["Client Application"]\n        PUB[Flow.Publisher]\n        SUB[Result Subscriber]\n    end\n\n    subgraph Ignite["Ignite Cluster"]\n        RCV[Receiver]\n    end\n\n    subgraph Downstream["Downstream"]\n        DS[Message Queue<br/>HTTP API<br/>Database]\n    end\n\n    PUB --\x3e|"Data In"| RCV\n    RCV --\x3e|"Results Out"| SUB\n    SUB --\x3e|"Forward"| DS'}),"\n",(0,s.jsx)(n.h3,{id:"understanding-receivers",children:"Understanding Receivers"}),"\n",(0,s.jsxs)(n.p,{children:["A receiver implements ",(0,s.jsx)(n.code,{children:"DataStreamerReceiver<T, A, R>"})," with three type parameters:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"T"})," (payload type): The data type sent to cluster nodes for processing"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"A"})," (argument type): Optional parameters passed to every receiver invocation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"R"})," (result type): The data type returned from cluster nodes to your subscriber"]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["The receiver's ",(0,s.jsx)(n.code,{children:"receive()"})," method executes on cluster nodes, not on your client. Each invocation receives a batch of items that share the same partition, enabling data-local operations. The method returns a ",(0,s.jsx)(n.code,{children:"CompletableFuture<List<R>>"})," containing results for each processed item, or ",(0,s.jsx)(n.code,{children:"null"})," if no results are needed."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"DataStreamerReceiverContext"})," parameter provides access to the Ignite API within the receiver. Use ",(0,s.jsx)(n.code,{children:"ctx.ignite()"})," to access tables, execute SQL, or perform other cluster operations during processing."]}),"\n",(0,s.jsx)(n.h3,{id:"enrichment-example",children:"Enrichment Example"}),"\n",(0,s.jsx)(n.p,{children:"The following example demonstrates data flowing through Ignite. Account events enter the stream, receive processing metadata on cluster nodes, and continue to a downstream system. This complements the account streaming example from the previous section."}),"\n",(0,s.jsx)(n.h4,{id:"step-1-create-an-enrichment-receiver",children:"Step 1: Create an Enrichment Receiver"}),"\n",(0,s.jsx)(n.p,{children:"The receiver transforms each incoming record by adding processing metadata. This pattern applies to any enrichment scenario: adding timestamps, looking up related data, validating against cluster state, or computing derived fields."}),"\n",(0,s.jsxs)(n.p,{children:["The receiver class must be deployed to all cluster nodes before streaming begins. When the streamer sends a batch to a node, that node instantiates the receiver class and calls its ",(0,s.jsx)(n.code,{children:"receive()"})," method."]}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(l.A,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'public class EnrichmentReceiver implements DataStreamerReceiver<Tuple, Void, Tuple> {\n    @Override\n    public CompletableFuture<List<Tuple>> receive(\n            List<Tuple> page,\n            DataStreamerReceiverContext ctx,\n            Void arg) {\n\n        String nodeName = ctx.ignite().name();\n        Instant processedAt = Instant.now();\n\n        List<Tuple> enriched = new ArrayList<>(page.size());\n\n        for (Tuple item : page) {\n            Tuple result = Tuple.create()\n                .set("accountId", item.intValue("accountId"))\n                .set("eventType", item.stringValue("eventType"))\n                .set("amount", item.longValue("amount"))\n                .set("processedAt", processedAt.toString())\n                .set("processedBy", nodeName);\n\n            enriched.add(result);\n        }\n\n        return CompletableFuture.completedFuture(enriched);\n    }\n}\n'})})}),(0,s.jsx)(l.A,{value:"dotnet",label:".NET",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'public class EnrichmentReceiver : IDataStreamerReceiver<IIgniteTuple, object?, IIgniteTuple>\n{\n    public ValueTask<IList<IIgniteTuple>?> ReceiveAsync(\n        IList<IIgniteTuple> page,\n        object? arg,\n        IDataStreamerReceiverContext context,\n        CancellationToken cancellationToken)\n    {\n        var nodeName = context.Ignite.Name;\n        var processedAt = DateTime.UtcNow;\n\n        var enriched = new List<IIgniteTuple>(page.Count);\n\n        foreach (var item in page)\n        {\n            var result = new IgniteTuple\n            {\n                ["accountId"] = item["accountId"],\n                ["eventType"] = item["eventType"],\n                ["amount"] = item["amount"],\n                ["processedAt"] = processedAt.ToString("O"),\n                ["processedBy"] = nodeName\n            };\n\n            enriched.Add(result);\n        }\n\n        return ValueTask.FromResult<IList<IIgniteTuple>?>(enriched);\n    }\n}\n'})})})]}),"\n",(0,s.jsx)(n.h4,{id:"step-2-create-a-forwarding-subscriber",children:"Step 2: Create a Forwarding Subscriber"}),"\n",(0,s.jsx)(n.p,{children:"The result subscriber receives enriched records as they return from cluster nodes. Because the streamer ignores backpressure on the result side, your subscriber must keep pace with incoming results or buffer them internally."}),"\n",(0,s.jsx)(n.p,{children:"This subscriber implements a common pattern: buffer incoming results until a batch threshold is reached, then forward the batch to a downstream system. Batching amortizes the cost of downstream operations (network calls, disk writes) across multiple records."}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"onSubscribe()"})," method requests ",(0,s.jsx)(n.code,{children:"Long.MAX_VALUE"})," items because backpressure signals have no effect on result delivery. The ",(0,s.jsx)(n.code,{children:"onNext()"})," method accumulates results and flushes when the buffer fills. The ",(0,s.jsx)(n.code,{children:"onComplete()"})," and ",(0,s.jsx)(n.code,{children:"onError()"})," methods flush any remaining buffered items before the stream ends."]}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(l.A,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:"class ForwardingSubscriber implements Flow.Subscriber<Tuple> {\n    private final int batchSize;\n    private final List<Tuple> buffer;\n    private final Consumer<List<Tuple>> downstream;\n\n    ForwardingSubscriber(Consumer<List<Tuple>> downstream, int batchSize) {\n        this.downstream = downstream;\n        this.batchSize = batchSize;\n        this.buffer = new ArrayList<>(batchSize);\n    }\n\n    @Override\n    public void onSubscribe(Flow.Subscription subscription) {\n        subscription.request(Long.MAX_VALUE);\n    }\n\n    @Override\n    public void onNext(Tuple item) {\n        buffer.add(item);\n\n        if (buffer.size() >= batchSize) {\n            downstream.accept(new ArrayList<>(buffer));\n            buffer.clear();\n        }\n    }\n\n    @Override\n    public void onError(Throwable throwable) {\n        if (!buffer.isEmpty()) {\n            downstream.accept(new ArrayList<>(buffer));\n            buffer.clear();\n        }\n    }\n\n    @Override\n    public void onComplete() {\n        if (!buffer.isEmpty()) {\n            downstream.accept(new ArrayList<>(buffer));\n            buffer.clear();\n        }\n    }\n}\n"})})}),(0,s.jsxs)(l.A,{value:"dotnet",label:".NET",children:[(0,s.jsxs)(n.p,{children:["In .NET, the ",(0,s.jsx)(n.code,{children:"IAsyncEnumerable"})," pattern provides natural iteration over results. Buffering can be implemented inline:"]}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:"var buffer = new List<IIgniteTuple>(batchSize);\n\nawait foreach (IIgniteTuple result in results)\n{\n    buffer.Add(result);\n\n    if (buffer.Count >= batchSize)\n    {\n        await downstream.SendBatchAsync(buffer);\n        buffer.Clear();\n    }\n}\n\nif (buffer.Count > 0)\n{\n    await downstream.SendBatchAsync(buffer);\n}\n"})})]})]}),"\n",(0,s.jsx)(n.h4,{id:"step-3-wire-the-pipeline",children:"Step 3: Wire the Pipeline"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"streamData()"})," overload for receivers accepts additional parameters that control how data flows through the pipeline:"]}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Parameter"}),(0,s.jsx)(n.th,{children:"Purpose"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"publisher"})}),(0,s.jsxs)(n.td,{children:["Source of input data (your ",(0,s.jsx)(n.code,{children:"Flow.Publisher"})," or ",(0,s.jsx)(n.code,{children:"SubmissionPublisher"}),")"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"receiverDesc"})}),(0,s.jsx)(n.td,{children:"Descriptor identifying the receiver class and deployment units"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"keyFunc"})}),(0,s.jsx)(n.td,{children:"Extracts a key from each input item for partition routing"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"payloadFunc"})}),(0,s.jsx)(n.td,{children:"Transforms input items into the payload type expected by the receiver"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"receiverArg"})}),(0,s.jsx)(n.td,{children:"Optional argument passed to every receiver invocation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"resultSubscriber"})}),(0,s.jsxs)(n.td,{children:["Receives results returned by the receiver (can be ",(0,s.jsx)(n.code,{children:"null"})," if no results needed)"]})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.code,{children:"options"})}),(0,s.jsx)(n.td,{children:"Streaming configuration (batch size, parallelism, flush interval)"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"keyFunc"})," determines which cluster node processes each item. Items with keys that hash to the same partition are batched together and sent to that partition's primary node. This enables data-local operations when the receiver accesses tables that share the same partitioning scheme."]}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"payloadFunc"})," separates the key extraction concern from the data sent to receivers. Your input items might contain fields used only for routing, while the receiver needs a different shape. Use ",(0,s.jsx)(n.code,{children:"Function.identity()"})," when the input and payload types are the same."]}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(l.A,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'// Source: account events from your data source\nList<Tuple> accountEvents = List.of(\n    Tuple.create().set("accountId", 1).set("eventType", "deposit").set("amount", 500L),\n    Tuple.create().set("accountId", 2).set("eventType", "withdrawal").set("amount", 200L),\n    Tuple.create().set("accountId", 3).set("eventType", "transfer").set("amount", 1000L)\n);\n\n// Downstream: where enriched events go next\nConsumer<List<Tuple>> downstream = batch -> {\n    // Send to Kafka, HTTP endpoint, another database, etc.\n    messageBroker.send("enriched-events", batch);\n};\n\n// Configure receiver\nDataStreamerReceiverDescriptor<Tuple, Void, Tuple> receiverDesc =\n    DataStreamerReceiverDescriptor.builder(EnrichmentReceiver.class).build();\n\n// Configure subscriber\nForwardingSubscriber resultSubscriber = new ForwardingSubscriber(downstream, 100);\n\n// Use accounts table for partition routing\nRecordView<Tuple> accountsView = client.tables().table("accounts").recordView();\n\nFunction<Tuple, Tuple> keyFunc = event -> Tuple.create().set("id", event.intValue("accountId"));\nFunction<Tuple, Tuple> payloadFunc = Function.identity();\n\n// Stream data through Ignite\ntry (var publisher = new SubmissionPublisher<Tuple>()) {\n    CompletableFuture<Void> streamFuture = accountsView.streamData(\n        publisher,\n        receiverDesc,\n        keyFunc,\n        payloadFunc,\n        null,\n        resultSubscriber,\n        DataStreamerOptions.DEFAULT\n    );\n\n    accountEvents.forEach(publisher::submit);\n}\n\nstreamFuture.join();\n'})})}),(0,s.jsx)(l.A,{value:"dotnet",label:".NET",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'// Source: account events from your data source\nasync IAsyncEnumerable<IIgniteTuple> GetAccountEvents()\n{\n    yield return new IgniteTuple\n        { ["accountId"] = 1, ["eventType"] = "deposit", ["amount"] = 500L };\n    yield return new IgniteTuple\n        { ["accountId"] = 2, ["eventType"] = "withdrawal", ["amount"] = 200L };\n    yield return new IgniteTuple\n        { ["accountId"] = 3, ["eventType"] = "transfer", ["amount"] = 1000L };\n}\n\n// Configure receiver\nvar receiverDesc = ReceiverDescriptor.Of(new EnrichmentReceiver());\n\n// Use accounts table for partition routing\nvar accountsView = (await client.Tables.GetTableAsync("accounts"))!.RecordBinaryView;\n\n// Stream data through Ignite\nIAsyncEnumerable<IIgniteTuple> results = accountsView.StreamDataAsync(\n    GetAccountEvents(),\n    receiverDesc,\n    keySelector: e => new IgniteTuple { ["id"] = e["accountId"] },\n    payloadSelector: e => e,\n    receiverArg: null\n);\n\n// Forward to downstream\nvar buffer = new List<IIgniteTuple>(100);\n\nawait foreach (IIgniteTuple enrichedEvent in results)\n{\n    buffer.Add(enrichedEvent);\n\n    if (buffer.Count >= 100)\n    {\n        await messageBroker.SendAsync("enriched-events", buffer);\n        buffer.Clear();\n    }\n}\n\nif (buffer.Count > 0)\n{\n    await messageBroker.SendAsync("enriched-events", buffer);\n}\n'})})})]}),"\n",(0,s.jsx)(n.h3,{id:"managing-result-flow",children:"Managing Result Flow"}),"\n",(0,s.jsx)(n.p,{children:"For high-volume streams, the rate at which cluster nodes produce results may exceed the rate at which your downstream system can accept them. Unlike input streaming where backpressure signals control flow, result delivery continues regardless of subscriber readiness."}),"\n",(0,s.jsx)(n.p,{children:"Consider these patterns for managing result flow:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Bounded queue"}),": Place a ",(0,s.jsx)(n.code,{children:"BlockingQueue"})," between the subscriber and downstream operations. The subscriber adds to the queue (blocking if full), while a separate thread drains to downstream. This decouples cluster throughput from downstream latency."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Async forwarding"}),": If your downstream client supports async operations, use ",(0,s.jsx)(n.code,{children:"CompletableFuture"})," to pipeline multiple in-flight requests. This overlaps network latency with continued result processing."]}),"\n"]}),"\n",(0,s.jsxs)(n.li,{children:["\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Batch size alignment"}),": Match the forwarding subscriber's batch size to your downstream system's optimal batch size. Mismatched sizes cause either too many small operations or memory pressure from oversized batches."]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If results arrive faster than your subscriber can process them, memory usage grows until the JVM runs out of heap. Monitor result processing throughput relative to cluster output and adjust batch sizes or add buffering as needed."}),"\n",(0,s.jsx)(n.h2,{id:"handling-failures",children:"Handling Failures"}),"\n",(0,s.jsxs)(n.p,{children:["Whether streaming to tables or receivers, network failures, schema mismatches, or cluster issues can cause batch failures. The data streamer automatically retries failed batches up to ",(0,s.jsx)(n.code,{children:"retryLimit"})," times (default 16). Items that exhaust all retry attempts are collected in a ",(0,s.jsx)(n.code,{children:"DataStreamerException"}),"."]}),"\n",(0,s.jsxs)(n.p,{children:["Access failed entries using the ",(0,s.jsx)(n.code,{children:"failedItems()"})," method:"]}),"\n",(0,s.jsxs)(i.A,{children:[(0,s.jsx)(l.A,{value:"java",label:"Java",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-java",children:'RecordView<Account> view = client.tables().table("accounts").recordView(Account.class);\n\nList<DataStreamerItem<Account>> failedItems = new ArrayList<>();\nCompletableFuture<Void> streamerFut;\n\ntry (var publisher = new SubmissionPublisher<DataStreamerItem<Account>>()) {\n    streamerFut = view.streamData(publisher, options)\n        .exceptionally(e -> {\n            if (e.getCause() instanceof DataStreamerException dse) {\n                failedItems.addAll(dse.failedItems());\n            }\n            return null;\n        });\n\n    Account entry = new Account(1, "Account name", rnd.nextLong(100_000), rnd.nextBoolean());\n    publisher.submit(DataStreamerItem.of(entry));\n} catch (DataStreamerException e) {\n    failedItems.addAll(e.failedItems());\n}\n\nstreamerFut.join();\n\n// Handle failed items after streaming completes\nif (!failedItems.isEmpty()) {\n    // Log failures for investigation\n    logger.error("Failed to stream {} items after {} retries",\n        failedItems.size(), options.retryLimit());\n\n    // Option 1: Write to dead-letter queue for later processing\n    // Option 2: Retry with different options (e.g., smaller batch size)\n    // Option 3: Use direct table operations for individual items\n}\n'})})}),(0,s.jsx)(l.A,{value:"dotnet",label:".NET",children:(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-csharp",children:'ITable? table = await Client.Tables.GetTableAsync("my-table");\nIRecordView<IIgniteTuple> view = table!.RecordBinaryView;\nIList<IIgniteTuple> data = [new IgniteTuple { ["key"] = 1L, ["val"] = "v" }];\n\ntry\n{\n    await view.StreamDataAsync(data.ToAsyncEnumerable());\n}\ncatch (DataStreamerException e)\n{\n    Console.WriteLine("Failed items: " + string.Join(",", e.FailedItems));\n\n    // Items have exhausted retryLimit attempts\n    // Investigate failure cause before retrying\n}\n'})})})]}),"\n",(0,s.jsx)(n.h3,{id:"handling-persistent-failures",children:"Handling Persistent Failures"}),"\n",(0,s.jsx)(n.p,{children:"Items fail after exhausting retries for reasons such as:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Schema mismatch"}),": Entry fields do not match the table schema"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Constraint violations"}),": Duplicate keys or foreign key violations"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Cluster unavailability"}),": Partition leader unavailable during all retry attempts"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Before retrying failed items:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Examine the exception cause to determine the failure reason"}),"\n",(0,s.jsx)(n.li,{children:"Fix data issues (schema mismatches, constraint violations)"}),"\n",(0,s.jsx)(n.li,{children:"Verify cluster health if failures were caused by unavailability"}),"\n",(0,s.jsxs)(n.li,{children:["Consider using direct table operations (",(0,s.jsx)(n.code,{children:"upsert"}),", ",(0,s.jsx)(n.code,{children:"insert"}),") for small numbers of failed items rather than restarting the streamer"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"tuning-flush-behavior",children:"Tuning Flush Behavior"}),"\n",(0,s.jsxs)(n.p,{children:["The ",(0,s.jsx)(n.code,{children:"autoFlushInterval"})," setting (default 5000ms) controls how long the streamer waits before sending incomplete batches. This addresses two scenarios:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Slow data sources"}),": When your publisher produces data slowly, buffers may not fill to ",(0,s.jsx)(n.code,{children:"pageSize"})," within a reasonable time. The auto-flush ensures data reaches the cluster without waiting for a full batch."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Uneven partition distribution"}),": If your data keys cluster into a few partitions, some partition buffers fill while others remain nearly empty. Auto-flush sends partial batches from slow-filling partitions."]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Set a shorter interval for latency-sensitive workloads. Set a longer interval (or increase ",(0,s.jsx)(n.code,{children:"pageSize"}),") for throughput-focused bulk loads where latency is less critical."]})]})}function p(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(h,{...e})}):h(e)}}}]);