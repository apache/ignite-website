"use strict";(globalThis.webpackChunkdocusaurus=globalThis.webpackChunkdocusaurus||[]).push([[867],{14025:e=>{e.exports=JSON.parse('{"archive":{"blogPosts":[{"id":"/2026/01/13/ignite3-architecture-p8","metadata":{"permalink":"/suggested-site/blog/2026/01/13/ignite3-architecture-p8","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2026-01-13-ignite3-architecture-p8.md","source":"@site/blog/2026-01-13-ignite3-architecture-p8.md","title":"The Business Case for Architectural Evolution: Platform Consolidation Benefits","description":"Your high-velocity application\'s architectural decisions directly impact business metrics. Response times affect customer satisfaction. System complexity drives operational overhead. Development velocity determines competitive advantage. When architecture limits business outcomes, evolution isn\'t a technical choice. It\'s a business necessity.","date":"2026-01-13T00:00:00.000Z","tags":[{"inline":true,"label":"architecture","permalink":"/suggested-site/blog/tags/architecture"},{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":10.58,"hasTruncateMarker":true,"authors":[{"name":"Michael Aglietti","title":"Apache Ignite Contributor and DevRel","url":"https://github.com/maglietti","imageURL":"/suggested-site/img/authors/maglietti.png","key":"maglietti","page":null}],"frontMatter":{"title":"The Business Case for Architectural Evolution: Platform Consolidation Benefits","authors":["maglietti"],"date":"2026-01-13T00:00:00.000Z","tags":["architecture","technical","ignite","ignite3"]},"unlisted":false,"nextItem":{"title":"MVCC Transactions for High-Frequency Processing: ACID at Scale","permalink":"/suggested-site/blog/2026/01/06/ignite3-architecture-p7"}},"content":"Your high-velocity application\'s architectural decisions directly impact business metrics. Response times affect customer satisfaction. System complexity drives operational overhead. Development velocity determines competitive advantage. When architecture limits business outcomes, evolution isn\'t a technical choice. It\'s a business necessity.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n_Part 8 of 8 in the Apache Ignite 3 Architecture Series_\\n\\n---\\n\\nThis series explored Apache Ignite 3\'s integrated platform approach to high-velocity application challenges. Now we examine the business impact: operational efficiency, performance gains, and competitive advantages that matter to both engineering teams and business stakeholders.\\n\\n**The bottom line: platform consolidation delivers measurable benefits through reduced complexity and enhanced performance.**\\n\\n## The Business Cost of Multi-System Complexity\\n\\n### Infrastructure and Operational Overhead\\n\\n**Traditional High-Velocity Application Stack:**\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"Application Layer\\"\\n        App[High-Velocity Application<br/>10,000+ events/sec]\\n    end\\n\\n    subgraph \\"Infrastructure Requirements\\"\\n        Redis[Redis Clusters 3 nodes + failover]\\n        PostgreSQL[PostgreSQL Clusters 5 nodes + replicas]\\n        Custom[Custom Processing 8 nodes + orchestration]\\n        Network[High-Bandwidth Network Inter-system communication]\\n    end\\n\\n    App --\x3e Redis\\n    App --\x3e PostgreSQL\\n    App --\x3e Custom\\n\\n    Redis -.-> Network\\n    PostgreSQL -.-> Network\\n    Custom -.-> Network\\n```\\n\\n**Hidden Costs:**\\n\\n- **Downtime Risk**: Each system adds failure modes (3x complexity = 3x risk)\\n- **Development Velocity**: Integration complexity slows feature delivery\\n- **Compliance**: Multiple audit trails, backup strategies, security models\\n\\n### Performance Impact on Business Metrics\\n\\n**E-commerce Platform Example** (peak load: 50,000 orders per second):\\n\\n**Multi-System Performance Characteristics:**\\n\\n- Order processing latency: 8-15ms average\\n- Peak load degradation: 25-50ms during traffic spikes\\n- System coordination failures: 0.1% order processing errors\\n- Customer abandonment: +2% cart abandonment per second of delay\\n\\n**Business Impact Calculation:**\\n\\n- Peak hour abandonment increase: 4% (2 seconds typical delay)\\n- Customer conversion loss during high-traffic periods\\n- Competitive disadvantage during critical sales periods\\n- Revenue impact from performance-related cart abandonment\\n\\n## Apache Ignite 3 Platform Consolidation Benefits\\n\\n### Infrastructure Cost Reduction\\n\\n**Consolidated Platform Architecture:**\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"Application Layer\\"\\n        App[High-Velocity Application<br/>10,000+ events/sec]\\n    end\\n\\n    subgraph \\"Apache Ignite 3 Cluster\\"\\n        Node1[Node 1<br/>Memory + Compute + Storage]\\n        Node2[Node 2<br/>Memory + Compute + Storage]\\n        Node3[Node 3<br/>Memory + Compute + Storage]\\n    end\\n\\n    App --\x3e Node1\\n    App --\x3e Node2\\n    App --\x3e Node3\\n\\n    Node1 <-.-> Node2\\n    Node2 <-.-> Node3\\n    Node3 <-.-> Node1\\n```\\n\\n### Performance Improvement Business Impact\\n\\n**Here\'s what the performance improvement looks like in practice:**\\n\\n```java\\n// Before: Multi-system order processing\\npublic OrderResult processOrder(OrderId orderId) {\\n    Order order = orderService.getOrder(orderId);                            // Network: 2ms\\n    Customer customer = customerService.getCustomer(order.customerId);       // Network: 2ms\\n    Inventory inventory = inventoryService.checkInventory(order.productId);  // Network: 2ms\\n    Payment payment = paymentService.processPayment(order);                  // Network: 3ms\\n\\n    return new OrderResult(order, customer, inventory, payment);\\n}\\n// Total: 9ms average, 25ms during peaks\\n// After: Integrated platform processing using colocated compute\\npublic OrderResult processOrderIntegrated(OrderId orderId) {\\n    // Create colocated job target\\n    JobTarget target = JobTarget.colocated(\\"orders\\", orderId);\\n\\n    // Execute colocated computation\\n    return ignite.compute().execute(target,\\n        JobDescriptor.builder(OrderProcessingJob.class).build(),\\n        orderId);\\n}\\n// Colocated compute job implementation\\npublic class OrderProcessingJob implements ComputeJob<OrderId, OrderResult> {\\n\\n    @Override\\n    public CompletableFuture<OrderResult> executeAsync(JobExecutionContext context, OrderId orderId) {\\n        return context.ignite().transactions().runInTransactionAsync(tx -> {\\n            // All data access is local to the executing node\\n            Order order = ordersTable.getAsync(tx, orderId);                    // Local: ~0.1ms\\n            Customer customer = customersTable.getAsync(tx, order.customerId);  // Local: ~0.1ms\\n            Inventory inventory = inventoryTable.getAsync(tx, order.productId); // Local: ~0.1ms\\n\\n            // Process payment logic locally\\n            PaymentResult payment = processPaymentLocally(order);               // Local: ~0.5ms\\n\\n            return CompletableFuture.completedFuture(new OrderResult(order, customer, inventory, payment));\\n        });\\n    }\\n}\\n// Total: ~0.8ms average, consistent under load (10x+ improvement)\\n```\\n\\n**Business Impact:**\\n\\n- Cart abandonment reduction: 8% improvement (faster checkout)\\n- Customer satisfaction: 15% improvement in NPS scores\\n- Peak load handling: 10x capacity without performance degradation\\n\\n**Revenue Impact Results:**\\n\\n- 8% improvement in cart abandonment rates during peak traffic\\n- Increased capacity: Handle Black Friday traffic without infrastructure scaling\\n- Sustained performance during high-demand periods enables revenue capture\\n\\n### Developer Productivity Gains\\n\\n**Development Velocity Metrics:**\\n\\n**Here\'s what complex feature development looks like across multiple systems:**\\n\\n```java\\n// Feature: Add customer loyalty points to order processing\\n// Requires changes across 3 systems with coordination\\n// 1. Redis cache layer changes\\n@Component\\npublic class LoyaltyPointsCache {\\n    public void updatePoints(CustomerId customerId, int points) {\\n        // Redis-specific caching logic\\n        redisTemplate.opsForValue().set(\\"loyalty:\\" + customerId, points, Duration.ofMinutes(30));\\n    }\\n}\\n// 2. PostgreSQL database changes\\n@Entity\\npublic class Customer {\\n    // Add loyalty points column\\n    @Column(name = \\"loyalty_points\\")\\n    private int loyaltyPoints;\\n\\n    // Database migration required\\n    // Downtime for schema changes\\n}\\n// 3. Custom processing service changes\\n@Service\\npublic class OrderProcessor {\\n    public void processOrder(Order order) {\\n        // Coordinate between Redis, PostgreSQL, and processing logic\\n        // Handle consistency edge cases\\n        // Manage distributed transactions\\n    }\\n}\\n// Development time: 3 weeks across 3 systems\\n// Testing complexity: Integration testing across all systems\\n// Deployment risk: Coordinated deployment required\\n```\\n\\n**Here\'s the same feature with integrated platform simplicity:**\\n\\n```java\\n// Feature: Add customer loyalty points to order processing\\n// Single system change with integrated transaction\\npublic class OrderProcessingJob implements ComputeJob<OrderId, OrderResult> {\\n\\n    @Override\\n    public CompletableFuture<OrderResult> executeAsync(JobExecutionContext context, OrderId orderId) {\\n        return context.ignite().transactions().runInTransactionAsync(tx -> {\\n            // Single integrated transaction\\n            Order order = ordersTable.getAsync(tx, orderId);\\n            Customer customer = customersTable.getAsync(tx, order.customerId);\\n\\n            // Add loyalty points logic\\n            int earnedPoints = calculateLoyaltyPoints(order);\\n            customer.setLoyaltyPoints(customer.getLoyaltyPoints() + earnedPoints);\\n            customersTable.putAsync(tx, customer.customerId(), customer);\\n\\n            // Everything happens atomically in one place\\n            return CompletableFuture.completedFuture(new OrderResult(order, customer, earnedPoints));\\n        });\\n    }\\n}\\n// Development time: 3 days in single system\\n// Testing: Unit test with integrated platform\\n// Deployment: Single system deployment\\n```\\n\\n**Productivity Impact:**\\n\\n- **Feature Development**: 85% faster (3 days vs 3 weeks)\\n- **Testing Effort**: 70% reduction (no integration complexity)\\n- **Deployment Risk**: 90% reduction (single system)\\n\\n**Business Value:**\\n\\n- Time to market: 6x faster feature delivery\\n- Engineering capacity: 40% more features per quarter\\n- Competitive advantage: Faster response to market opportunities\\n\\n### The Development Velocity Transformation\\n\\n**Here\'s what the 3 weeks to 3 days transformation looks like in practice:**\\n\\n```java\\n// Feature Request: Real-time customer segment analytics for personalization\\n// Multi-System Approach (3 weeks):\\n// Week 1: Update PostgreSQL schema, coordinate with DBA team\\n// Week 1: Modify Redis caching layer, handle cache invalidation\\n// Week 2: Update ETL pipelines for analytics warehouse\\n// Week 2: Modify application APIs across three different codebases\\n// Week 3: Integration testing across all systems\\n// Week 3: Coordinated deployment with rollback procedures\\n// Integrated Platform Approach (3 days):\\n// Day 1: Add schema columns, immediately available across all APIs\\nclient.sql().execute(null,\\n    \\"ALTER TABLE customers ADD COLUMN engagement_score DECIMAL(3,2)\\");\\n// Day 2: Update single codebase with new analytics logic\\nTable customers = client.tables().table(\\"customers\\");\\nResultSet<SqlRow> segments = client.sql().execute(tx,\\n    \\"SELECT segment, AVG(engagement_score) FROM customers GROUP BY segment\\");\\n// Day 3: Deploy single system, feature immediately available\\n// All access patterns (key-value, SQL, records) see new data instantly\\n```\\n\\n**Development Acceleration:**\\n\\n- **Schema changes**: Hours instead of weeks of coordination\\n- **Feature implementation**: Single codebase instead of three separate systems\\n- **Testing complexity**: Unit tests instead of full integration testing\\n- **Deployment risk**: Single system deployment instead of coordinated rollout\\n- **Time to value**: 3 days instead of 3 weeks from concept to customer value\\n\\n**The velocity multiplier effect**: When every feature takes 3 days instead of 3 weeks, your team delivers 7x more customer value per quarter. This transforms competitive positioning from \\"catching up\\" to \\"setting pace.\\"\\n\\n## Quantified Business Benefits\\n\\n### Operational Efficiency Analysis\\n\\n**3-Year Resource Comparison:**\\n\\n**Multi-System Approach:**\\n\\n- Infrastructure: Growing complexity and costs with scale\\n- Personnel: 10 FTE across database, DevOps, and development teams\\n- Performance impact: Revenue loss from system limitations\\n- Operational overhead: High maintenance and coordination costs\\n\\n**Apache Ignite 3 Platform:**\\n\\n- Infrastructure: Efficient scaling with unified platform\\n- Personnel: 5 FTE with simplified operational model\\n- Platform investment: Initial migration and licensing costs\\n- Operational efficiency: Streamlined maintenance and development\\n\\n**Resource Optimization**: Significant reduction in operational complexity\\n\\n**Business Value**: Measurable improvement in development velocity and system reliability\\n\\n### Risk Reduction Benefits\\n\\n**Operational Risk Mitigation:**\\n\\n- **System availability**: 99.9% vs 99.5% (fewer failure modes)\\n- **Data consistency**: Eliminate sync issues between systems\\n- **Security surface**: Single system vs multiple attack vectors\\n- **Compliance**: Unified audit trail vs distributed logs\\n\\n**Business Continuity Impact:**\\n\\n- Reduced downtime: Fewer system failure modes and coordination points\\n- Faster recovery: Single system restart vs multi-system coordination\\n- Simplified disaster recovery: Unified backup strategy vs multiple approaches\\n\\n### Competitive Advantage Metrics\\n\\n**Market Responsiveness:**\\n\\n- Feature delivery: 6x faster time to market\\n- Customer experience: Sub-millisecond response times enable new features\\n- Scale handling: 10x traffic spikes without performance degradation\\n- Resource efficiency: Streamlined operations enable competitive advantage\\n\\n**Innovation Capacity:**\\n\\n- Engineering focus: 70% more time on features vs integration maintenance\\n- Experimentation velocity: Real-time A/B testing with consistent data\\n- Customer insights: Microsecond analytics enable personalization at scale\\n\\n## Implementation Strategy and Success Metrics\\n\\n### Migration Approach\\n\\n**Phase 1: Non-Critical Workloads** (Month 1-2)\\n\\n- Migrate development/staging environments\\n- Validate performance characteristics\\n- Train development team on integrated platform\\n- **Success Metric**: Feature parity with 2x performance improvement\\n\\n**Phase 2: Read-Heavy Workloads** (Month 3-4)\\n\\n- Move analytical queries and reporting\\n- Implement real-time dashboards\\n- Validate data consistency and durability\\n- **Success Metric**: 95% query performance improvement\\n\\n**Phase 3: Transactional Workloads** (Month 5-6)\\n\\n- Migrate core transaction processing\\n- Implement integrated event processing\\n- Full platform consolidation\\n- **Success Metric**: Significant infrastructure simplification\\n\\n### Key Performance Indicators\\n\\n**Technical Metrics:**\\n\\n- Response time: Target sub-millisecond for 95% of operations\\n- Throughput: 10x capacity improvement over baseline\\n- Availability: 99.9%+ uptime\\n- Cost efficiency: 50% infrastructure reduction\\n\\n**Business Metrics:**\\n\\n- Customer satisfaction: 15% NPS improvement\\n- Revenue impact: Measurable performance-driven uplift\\n- Development velocity: 6x faster feature delivery\\n- Operational efficiency: 50% team size reduction\\n\\n**Here\'s how you validate business impact through continuous monitoring:**\\n\\n```java\\n// Continuous performance monitoring\\npublic class BusinessMetricsCollector {\\n\\n    public void trackOrderProcessingMetrics() {\\n        // Response time monitoring\\n        long responseTime = measureOrderProcessingTime();\\n        assert responseTime < 1_000_000; // Target <1ms in nanoseconds\\n\\n        // Throughput validation\\n        int ordersPerSecond = measureOrderThroughput();\\n        assert ordersPerSecond > 10_000; // 10x improvement target\\n\\n        // Business impact tracking\\n        double revenuePerHour = calculateRevenueImpact();\\n        double efficiencyRatio = calculateResourceEfficiency();\\n\\n        // Efficiency calculation\\n        double efficiencyGain = measureOperationalEfficiency();\\n        assert efficiencyGain > 2.5; // 250% efficiency improvement target\\n    }\\n}\\n```\\n\\n## The Architectural Evolution Decision\\n\\n### When Platform Consolidation Makes Business Sense\\n\\n**Clear Indicators:**\\n\\n- Infrastructure complexity across multiple specialized systems\\n- Development team spending 50%+ time on integration vs features\\n- Performance requirements approaching multi-system limits\\n- Compliance/audit complexity from distributed data\\n\\n**Business Readiness Factors:**\\n\\n- Executive support for platform migration\\n- Engineering team capacity for 6-month evolution\\n- Customer tolerance for planned maintenance windows\\n- Resources allocated for platform migration and team training\\n\\n### Expected Business Outcomes\\n\\n**Year 1: Foundation and Efficiency**\\n\\n- Significant operational simplification\\n- 85% faster feature development\\n- 15x performance improvement\\n- **Business impact**: Major operational efficiency gains and revenue optimization\\n\\n**Year 2: Competitive Advantage**\\n\\n- Market-leading response times enable new features\\n- Real-time personalization at scale\\n- 99.9% availability competitive advantage\\n- **Business impact**: Enhanced market competitiveness and customer satisfaction\\n\\n**Year 3: Innovation Platform**\\n\\n- Architecture enables impossible features\\n- Cost advantage funds additional innovation\\n- Technical debt elimination frees engineering capacity\\n- **Business impact**: Sustained competitive advantage through operational excellence\\n\\n## The Evolution Imperative\\n\\nYour current architecture enabled your initial success. But architecture that works at 1,000 events/second faces different challenges at 10,000 events/second. The question isn\'t whether you\'ll need to evolve. It\'s whether you\'ll evolve proactively or reactively.\\n\\n**Proactive Evolution Benefits:**\\n\\n- Planned migration vs emergency response\\n- Competitive advantage vs catch-up mode\\n- Engineering team growth vs crisis management\\n- Platform enablement vs operational overhead\\n\\n**The Choice:**\\n\\nEvery high-velocity application reaches this decision point. Continue managing the compound complexity of multi-system architectures, or consolidate around a platform designed for your performance requirements.\\n\\nApache Ignite 3 represents architectural evolution that eliminates the fundamental constraints of traditional approaches. Your winning application doesn\'t need to be limited by yesterday\'s architectural decisions. It can evolve into tomorrow\'s competitive advantage.\\n\\n## Series Summary: Architecture as Business Strategy\\n\\nThis series demonstrated how Apache Ignite 3\'s integrated platform addresses the compound challenges of high-velocity applications:\\n\\n1. **Multi-system complexity** creates performance bottlenecks that grow with success\\n2. **Memory-first architecture** eliminates disk I/O constraints for event processing\\n3. **Flexible schemas** enable operational evolution without downtime\\n4. **Integrated performance** maintains characteristics across processing types\\n5. **Data colocation** eliminates network overhead through intelligent placement\\n6. **Distributed consensus** ensures consistency during high-frequency operations\\n7. **MVCC transactions** provide ACID guarantees without performance sacrifice\\n8. **Platform consolidation** delivers measurable business benefits\\n\\n**The architectural principle**: When your application\'s success outgrows your architecture\'s design limits, evolution becomes a business imperative.\\n\\nYour high-velocity application can scale past traditional constraints. The technical foundation exists. The business case is compelling. The only question is whether you\'ll evolve your architecture as intentionally as you\'ve evolved your application.\\n\\n*Thank you for following this architectural journey. Your applications deserve platforms that scale with your business success, not against it.*\\n\\n---\\n\\n## Apache Ignite 3 Architecture Series\\n\\nThis post is part of the Apache Ignite 3 Architecture Series exploring how modern distributed systems address high-velocity application challenges:\\n\\n- **Part 1**: [The Compounding Satisficer Problem](/blog/2025/11/25/ignite3-architecture-p1) - Why traditional architectures fail at scale\\n- **Part 2**: [Memory-First Architecture](/blog/2025/12/02/ignite3-architecture-p2) - Eliminating disk I/O constraints\\n- **Part 3**: [Schema Flexibility](/blog/2025/12/09/ignite3-architecture-p3) - Evolution without downtime\\n- **Part 4**: [Unified Data Access](/blog/2025/12/16/ignite3-architecture-p4) - Consistent performance across access patterns\\n- **Part 5**: [Data Colocation](/blog/2025/12/23/ignite3-architecture-p5) - Eliminating network overhead\\n- **Part 6**: [Distributed Consensus](/blog/2025/12/30/ignite3-architecture-p6) - Consistency during high-frequency operations\\n- **Part 7**: [MVCC Transactions](/blog/2026/01/06/ignite3-architecture-p7) - ACID guarantees without performance sacrifice\\n- **Part 8**: The Business Case for Architectural Evolution (this post)"},{"id":"/2026/01/06/ignite3-architecture-p7","metadata":{"permalink":"/suggested-site/blog/2026/01/06/ignite3-architecture-p7","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2026-01-06-ignite3-architecture-p7.md","source":"@site/blog/2026-01-06-ignite3-architecture-p7.md","title":"MVCC Transactions for High-Frequency Processing: ACID at Scale","description":"Traditional databases force a choice between fast transactions and concurrent analytics, but Apache Ignite\'s universal transaction model delivers both simultaneously.","date":"2026-01-06T00:00:00.000Z","tags":[{"inline":true,"label":"architecture","permalink":"/suggested-site/blog/tags/architecture"},{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":8.61,"hasTruncateMarker":true,"authors":[{"name":"Michael Aglietti","title":"Apache Ignite Contributor and DevRel","url":"https://github.com/maglietti","imageURL":"/suggested-site/img/authors/maglietti.png","key":"maglietti","page":null}],"frontMatter":{"title":"MVCC Transactions for High-Frequency Processing: ACID at Scale","authors":["maglietti"],"date":"2026-01-06T00:00:00.000Z","tags":["architecture","technical","ignite","ignite3"]},"unlisted":false,"prevItem":{"title":"The Business Case for Architectural Evolution: Platform Consolidation Benefits","permalink":"/suggested-site/blog/2026/01/13/ignite3-architecture-p8"},"nextItem":{"title":"Distributed Consistency Under Load: When High-Velocity Meets High-Availability","permalink":"/suggested-site/blog/2025/12/30/ignite3-architecture-p6"}},"content":"Traditional databases force a choice between fast transactions and concurrent analytics, but Apache Ignite\'s universal transaction model delivers both simultaneously.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n_Part 7 of 8 in the Apache Ignite 3 Architecture Series_\\n\\n---\\n\\n**The transformation:** Traditional systems force you to choose between fast operations and consistent data. Apache Ignite eliminates this choice through universal transaction consistency that works across all APIs.\\n\\n**Business scenario:** Your bank transfer application needs both instant balance updates for customers and concurrent fraud analytics on live transaction data. Traditional databases lock accounts during transfers, blocking fraud detection exactly when you need it most.\\n\\n**The solution:** Universal transaction model ensures balance updates and fraud queries both get full ACID guarantees while running concurrently without blocking each other.\\n\\n## Why Traditional Transactions Fail at Scale\\n\\n### The Blocking Problem\\n\\n**Traditional transaction problems compound at scale:**\\n\\n1. **Locking blocks analytics**: Account transfers lock balances, preventing concurrent fraud detection\\n2. **Eventually consistent risks**: Stale balance data enables overdrafts during high-frequency transfers\\n3. **System coordination complexity**: Multiple databases with different consistency guarantees create reconciliation problems\\n\\n**Business impact:** Transaction processing and analytical queries become mutually exclusive during peak load.\\n\\n**Here\'s what the blocking problem looks like in practice:**\\n\\n```java\\n// Traditional locking blocks concurrent operations\\npublic TransferResult processTransfer(TransferRequest transfer) {\\n    // Exclusive locks block all concurrent access\\n    return database.withExclusiveLock(transfer.fromAccountId, () -> {\\n        Account account = getAccount(transfer.fromAccountId);  // Lock acquired\\n\\n        if (account.balance >= transfer.amount) {\\n            account.balance -= transfer.amount;                // Account locked during update\\n            logTransaction(transfer);                          // Transaction log locked\\n\\n            // Problem: All queries blocked during this processing\\n            // Fraud detection: BLOCKED (can\'t read account during transfer)\\n            // Balance inquiry: BLOCKED (can\'t check balance)\\n            // Analytics: BLOCKED (can\'t read transaction history)\\n\\n            return TransferResult.SUCCESS;\\n        }\\n        return TransferResult.INSUFFICIENT_FUNDS;\\n    });\\n}\\n```\\n\\n**Eventual Consistency Problems:**\\n\\n```java\\n// Eventually consistent systems create business risks\\npublic void processConcurrentTransfers(String accountId) {\\n    // Operations see different data states\\n    CompletableFuture<Void> transfer1 = CompletableFuture.runAsync(() -> {\\n        Account account = getAccount(accountId);      // Version 1: $1,000 balance\\n        if (account.balance >= 600) {\\n            processTransfer(accountId, 600);          // Transfer $600\\n        }\\n    });\\n\\n    CompletableFuture<Void> transfer2 = CompletableFuture.runAsync(() -> {\\n        Account account = getAccount(accountId);      // Version 1: $1,000 balance (stale)\\n        if (account.balance >= 500) {\\n            processTransfer(accountId, 500);          // Transfer $500 (overdraft!)\\n        }\\n    });\\n\\n    CompletableFuture.allOf(transfer1, transfer2).join();\\n    // Result: $1,100 transferred from $1,000 account (business error)\\n}\\n```\\n\\n### Business Requirements That Traditional Systems Can\'t Meet\\n\\n**Banking platform needs:**\\n\\n- **Transfer processing**: 10,000 transfers/second with ACID guarantees\\n- **Fraud detection**: Real-time analytics on live transaction data\\n- **Balance inquiries**: Customer balance checks without blocking transfers\\n- **Compliance reporting**: Regulatory queries on active transaction streams\\n- **Performance requirement**: All operations sub-millisecond response time\\n\\n## Apache Ignite Universal Transaction Model\\n\\n**The breakthrough:** Every API provides identical ACID guarantees. No complexity tiers, no eventual consistency compromises.\\n\\n### How Universal Transactions Work\\n\\nApache Ignite combines distributed coordination with concurrent access control to solve both consistency and performance problems:\\n\\n**Universal benefits:**\\n\\n- **Same consistency everywhere**: Cache operations, SQL queries, and compute jobs all provide identical ACID guarantees\\n- **Concurrent access**: Analytics run simultaneously with transaction processing without blocking\\n- **Distributed safety**: All nodes maintain consistency during network partitions\\n- **Performance optimized**: Background coordination doesn\'t block application operations\\n\\n### The One Transaction Model Advantage\\n\\n**Here\'s the unified transaction guarantee that changes everything:**\\n\\n```java\\n// ALL operations share the same transaction model and ACID guarantees\\nTable accountsTable = client.tables().table(\\"accounts\\");\\n// Key-value operation - full ACID guarantees\\nTuple account = accountsTable.keyValueView().get(tx, accountKey);\\n// SQL operation - full ACID guarantees (concurrent fraud detection)\\nResultSet<SqlRow> fraudCheck = client.sql().execute(tx,\\n    \\"SELECT account_id, COUNT(*) FROM transfers WHERE amount > 5000 GROUP BY account_id\\");\\n// Record operation - full ACID guarantees\\nTuple transfer = accountsTable.recordView().get(tx,\\n    Tuple.create().set(\\"transfer_id\\", transferId));\\n// Compute operation - accesses same transactional data\\nCompletableFuture<String> riskFuture = client.compute().executeAsync(\\n    JobTarget.colocated(\\"trades\\", tradeKey), RiskJob.class, tradeId);\\n// Every operation: same consistency model, same isolation level, same durability\\n```\\n\\n**Universal Transaction Benefits:**\\n\\n- **Same safety everywhere**: Cache operations, SQL queries, and compute jobs all provide identical ACID safety\\n- **No complexity tiers**: Every operation gets enterprise-grade transaction safety automatically\\n- **Consistent behavior**: Operations and analytics see identical consistency guarantees\\n- **Unified recovery**: All operations participate in the same failure recovery mechanism\\n\\n**The breakthrough**: One transaction model handles everything from microsecond lookups to complex analytical queries. No complexity tiers, no eventual consistency compromises.\\n\\n### How Universal Transactions Handle Mixed Workloads\\n\\n**Apache Ignite solves this with API patterns that provide both consistency and concurrency:**\\n\\n```java\\n// Read-only transactions for analytics (don\'t block updates)\\npublic FraudAnalytics checkFraudPatterns(String accountId) {\\n    // Read-only transaction gets consistent snapshot automatically\\n    return ignite.transactions().runInTransaction(tx -> {\\n        // All reads see the same consistent point-in-time snapshot\\n        Account account = accountTable.get(tx, accountId);\\n        List<Transfer> recentTransfers = transferTable.query(tx,\\n            \\"SELECT * FROM transfers WHERE account_id = ? AND timestamp > ?\\",\\n            accountId, Instant.now().minus(Duration.ofHours(24)));\\n\\n        // Fraud analysis on consistent data\\n        return new FraudAnalytics(account, recentTransfers);\\n    }, new TransactionOptions().readOnly(true));\\n}\\n// Read-write transactions provide ACID guarantees\\npublic TransferResult executeTransfer(TransferRequest transfer) {\\n    return ignite.transactions().runInTransaction(tx -> {\\n        // Read current account state\\n        Account account = accountTable.get(tx, transfer.fromAccountId);\\n\\n        if (account.balance >= transfer.amount) {\\n            // Update account balance atomically\\n            account.balance -= transfer.amount;\\n            accountTable.put(tx, transfer.fromAccountId, account);\\n\\n            // Record transfer execution\\n            Transfer executedTransfer = new Transfer(transfer, TransferStatus.COMPLETED);\\n            transferTable.put(tx, executedTransfer.transferId, executedTransfer);\\n\\n            return TransferResult.SUCCESS;\\n        }\\n        return TransferResult.INSUFFICIENT_FUNDS;\\n    }); // Automatic coordination and commit\\n}\\n```\\n\\n### How Concurrent Access Works\\n\\nApache Ignite maintains multiple data versions to enable true concurrency:\\n\\n**Concurrent processing benefits:**\\n\\n- **Analytics queries**: Read from stable data versions without blocking active transfers\\n- **Transfer updates**: Create new versions while fraud detection continues accessing stable snapshots\\n- **Memory efficiency**: Automatic cleanup of old versions no longer needed for consistent reads\\n- **Performance optimization**: Both real-time transfers and historical analysis operate simultaneously\\n\\n**The breakthrough:** Instead of forcing operations to wait for each other, multiple versions let both transfer processing and fraud analytics operate simultaneously against the same logical data.\\n\\n## Performance Under High Load\\n\\n### Real-World Performance Characteristics\\n\\n**The performance impact becomes clear in a typical trade execution:**\\n\\n```java\\n// Single trade execution demonstrating performance characteristics\\npublic TradeResult processTradeWithMVCC(TradeRequest tradeRequest) {\\n    return ignite.transactions().runInTransaction(tx -> {\\n        // Read account data (MVCC snapshot access)\\n        Account account = accountTable.get(tx, tradeRequest.accountId);\\n\\n        // Validate and execute trade (MVCC write + RAFT coordination)\\n        if (account.balance >= tradeRequest.amount) {\\n            account.balance -= tradeRequest.amount;\\n            accountTable.put(tx, account.accountId, account);\\n\\n            Trade trade = new Trade(tradeRequest, TradeStatus.EXECUTED);\\n            tradeTable.put(tx, trade.tradeId, trade);\\n            return TradeResult.EXECUTED;\\n        }\\n        return TradeResult.INSUFFICIENT_FUNDS;\\n    });\\n}\\n```\\n\\n**Performance Outcomes:**\\n\\n- **Transaction latency**: Sub-millisecond execution with ACID guarantees\\n- **Concurrent analytics**: Portfolio calculations run simultaneously without blocking trades\\n- **Throughput capacity**: 12,500 trades/second + 2,500 analytics queries/second\\n- **Performance interference**: Less than 5% mutual impact during mixed workloads\\n\\n**The Performance Advantage:** Traditional systems force you to choose between transaction speed and analytical access. RAFT + MVCC provides both simultaneously.\\n\\n## Real-World Scenarios\\n\\n### High-Volume Event Handling\\n\\n**Market Volatility Processing:**\\n\\nDuring flash crash events or extreme market volatility, trading systems face spikes to 50,000+ trades per minute while risk monitoring systems need continuous portfolio analysis. Traditional systems either sacrifice transaction accuracy or block analytical queries when they\'re needed most.\\n\\n**RAFT + MVCC Response:**\\n\\n- **Transaction processing**: Maintains ACID guarantees even during volume spikes\\n- **Risk monitoring**: Continues real-time portfolio calculations without blocking trades\\n- **Performance stability**: Consistent sub-millisecond trade execution regardless of analytical load\\n- **Data accuracy**: No account overdrafts or position errors even under extreme conditions\\n\\n### Regulatory Compliance Operations\\n\\n**Live Compliance Reporting:**\\n\\nRegulatory requirements demand real-time transaction monitoring and complex daily reporting while trading operations continue uninterrupted. Traditional approaches force compliance teams to wait for low-volume periods or accept stale data that might miss violations.\\n\\n**The Integration Solution:**\\n\\n- **Concurrent reporting**: Compliance queries process millions of trades without affecting live operations\\n- **Data consistency**: Point-in-time snapshots ensure accurate violation detection\\n- **Operational continuity**: Trading performance remains stable during report generation\\n- **Audit accuracy**: Transaction ordering maintained across distributed nodes\\n\\n**Business Impact:** These scenarios demonstrate why RAFT + MVCC matters for distributed architectures. It\'s not just about performance. It\'s about maintaining business operations and regulatory compliance during the high-stress periods when accurate data matters most.\\n\\n## Business Impact of MVCC at Scale\\n\\n### Financial Risk Mitigation\\n\\n**Trading Firm Benefits:**\\n\\n- **ACID guarantees**: Zero account overdrafts or position errors under high load\\n- **Real-time risk management**: Continuous monitoring without trading performance impact\\n- **Regulatory compliance**: Accurate transaction ordering and reporting during volatility\\n- **Operational reliability**: Consistent performance during market stress events\\n\\n**Cost Avoidance Through Accuracy:**\\n\\n- **Regulatory fines**: Reduced penalties for compliance violations\\n- **Trading errors**: Eliminated losses from account overdrafts\\n- **System downtime**: High availability during peak trading periods\\n- **Manual reconciliation**: Reduced trade settlement errors\\n\\n### Performance-Enabled Business Capabilities\\n\\n**New Business Opportunities:**\\n\\n- **High-frequency trading**: Low-latency execution enables competitive strategies\\n- **Real-time analytics**: Traders get instant portfolio insights during active trading\\n- **Regulatory reporting**: Sophisticated compliance without operational impact\\n- **Market making**: Provide liquidity during volatility with confidence in data accuracy\\n\\n**Competitive Differentiation:**\\n\\n- **Customer experience**: Fast trade confirmations with real-time position updates\\n- **Risk management**: Superior risk controls enable larger position limits\\n- **Market participation**: Participate in volatile events while maintaining safety\\n- **Operational efficiency**: Single platform serves trading, analytics, and compliance\\n\\n### Development and Operations Benefits\\n\\n**Engineering Productivity:**\\n\\n- **Single transaction model**: No coordination between OLTP and OLAP systems\\n- **Consistent behavior**: MVCC semantics eliminate race condition bugs\\n- **Simplified testing**: Integrated transactions easier to validate than distributed coordination\\n- **Faster deployment**: Single system reduces coordination complexity\\n\\n**Operational Simplification:**\\n\\n- **Unified monitoring**: Single consistency model across all operations\\n- **Predictable performance**: MVCC performance characteristics remain stable under load\\n- **Reduced complexity**: Eliminate eventual consistency edge case handling\\n- **Automatic recovery**: MVCC built on RAFT provides automatic failure handling\\n\\n## Breaking Free from Distributed Architecture Constraints\\n\\nYour application\'s evolution from single database to distributed architecture created the transaction concurrency problem. Traditional solutions force you to accept either performance compromises or consistency risks. Each system you added to handle scale introduced new coordination challenges.\\n\\n**The RAFT + MVCC Integration Advantage:**\\n\\nApache Ignite\'s approach is fundamentally different. Instead of managing consistency between separate transaction and analytical systems, RAFT-backed MVCC provides both transaction isolation and concurrent analytical access within the same distributed platform.\\n\\n**Why This Matters for Distributed Architectures:**\\n\\n- **Eliminates system coordination**: No synchronization between transaction and analytical systems\\n- **Reduces consistency windows**: RAFT ensures all nodes see the same transaction order\\n- **Enables concurrent workloads**: MVCC snapshots support analytics without blocking transactions\\n- **Maintains ACID guarantees**: Full transaction safety at distributed scale\\n\\n**The architectural evolution principle**: Your transaction processing should enable operational intelligence, not constrain it.\\n\\nWhen applications outgrow single databases, they typically sacrifice either consistency or concurrency. Apache Ignite\'s RAFT + MVCC architecture preserves both, enabling the operational and analytical capabilities your business requires without the complexity of coordinating separate systems.\\n\\nThis isn\'t just better transaction processing. It\'s the foundation that lets high-velocity applications scale past the limitations that force architectural fragmentation.\\n\\n---\\n\\n*Return next Tuesday for Part 8 that concludes our series by examining the business impact of architectural consolidation. This demonstrates how the technical capabilities explored throughout this series translate into measurable competitive advantages and operational benefits that matter to both engineering teams and business stakeholders.*"},{"id":"/2025/12/30/ignite3-architecture-p6","metadata":{"permalink":"/suggested-site/blog/2025/12/30/ignite3-architecture-p6","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-12-30-ignite3-architecture-p6.md","source":"@site/blog/2025-12-30-ignite3-architecture-p6.md","title":"Distributed Consistency Under Load: When High-Velocity Meets High-Availability","description":"Distributed systems traditionally force a choice between consistency and speed. Apache Ignite\'s RAFT implementation delivers both: strong guarantees that protect your business without the coordination penalties that limit throughput.","date":"2025-12-30T00:00:00.000Z","tags":[{"inline":true,"label":"architecture","permalink":"/suggested-site/blog/tags/architecture"},{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":10.3,"hasTruncateMarker":true,"authors":[{"name":"Michael Aglietti","title":"Apache Ignite Contributor and DevRel","url":"https://github.com/maglietti","imageURL":"/suggested-site/img/authors/maglietti.png","key":"maglietti","page":null}],"frontMatter":{"title":"Distributed Consistency Under Load: When High-Velocity Meets High-Availability","authors":["maglietti"],"date":"2025-12-30T00:00:00.000Z","tags":["architecture","technical","ignite","ignite3"]},"unlisted":false,"prevItem":{"title":"MVCC Transactions for High-Frequency Processing: ACID at Scale","permalink":"/suggested-site/blog/2026/01/06/ignite3-architecture-p7"},"nextItem":{"title":"Eliminating Data Movement: The Hidden Cost of Distributed Event Processing","permalink":"/suggested-site/blog/2025/12/23/ignite3-architecture-p5"}},"content":"Distributed systems traditionally force a choice between consistency and speed. Apache Ignite\'s RAFT implementation delivers both: strong guarantees that protect your business without the coordination penalties that limit throughput.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n_Part 6 of 8 in the Apache Ignite 3 Architecture Series_\\n\\n---\\n\\nWhen network partitions split your cluster, duplicate payments happen. Apache Ignite prevents this through automatic coordination that maintains consistency without performance penalties.\\n\\n**Business scenario:** Your payment processing system handles 10,000 transactions per second across 5 nodes. Network issues isolate 2 nodes from the remaining 3 nodes. Both groups continue processing payments. Customer payment gets charged twice.\\n\\n**The solution:** Automatic consensus ensures only one group can process payments during network splits. Your application code stays simple while the platform handles distributed coordination automatically.\\n\\n**Performance guarantee:** Consensus happens in the background without blocking transaction processing.\\n\\n## Business Problems from Inconsistent Data\\n\\n### Duplicate Payment Scenario\\n\\nNetwork partitions create business-critical consistency problems:\\n\\n**What happens during network partition without proper coordination:**\\n\\n```java\\n// Network partition splits payment cluster\\n// Both partitions continue processing payments independently\\n// Partition A (3 nodes) processes payment:\\nPaymentResult resultA = processPayment(payment); // SUCCESS: Balance updated to $500\\n// Partition B (2 nodes) processes same payment:\\nPaymentResult resultB = processPayment(payment); // SUCCESS: Balance updated to $500\\n// When network heals: customer charged twice, balance corrupted\\n// Expected: $500 final balance\\n// Actual: $0 final balance (double charge)\\n```\\n\\n**Business impact of consistency failures:**\\n\\n- **Customer complaints**: Duplicate charges from partition processing\\n- **Financial exposure**: Regulatory penalties for transaction sequence errors\\n- **System downtime**: Manual reconciliation required when partitions heal\\n- **Business risk**: Revenue loss during extended partition scenarios\\n\\n### Why Traditional Solutions Fail\\n\\n**Performance penalty:** Traditional consensus adds 20-50ms to every transaction through multiple network round-trips. At 10,000 payments per second, this creates a 200-500 second processing backlog.\\n\\n**Failure problems:**\\n\\n- **Network delays**: Every transaction requires multiple round-trips to all nodes\\n- **Processing blocks**: All nodes wait for slowest participant\\n- **Failure stops processing**: Single node failure blocks all transactions\\n- **Partition stops everything**: Network partition stops all processing\\n\\n## Apache Ignite Automatic Coordination Solution\\n\\n### Zero-Code Consistency for Normal Operations\\n\\n**Your application code stays simple. Apache Ignite handles distributed coordination automatically.**\\n\\n```java\\n// Your application code - simple and clean\\npublic PaymentResult processPayment(PaymentRequest payment) {\\n    return ignite.transactions().runInTransaction(tx -> {\\n        Table accounts = ignite.tables().table(\\"accounts\\");\\n\\n        // Standard database operations\\n        Tuple account = accounts.recordView().get(tx,\\n            Tuple.create().set(\\"account_id\\", payment.accountId));\\n\\n        if (account.decimalValue(\\"balance\\").compareTo(payment.amount) < 0) {\\n            return PaymentResult.INSUFFICIENT_FUNDS;\\n        }\\n\\n        // Update account balance\\n        accounts.recordView().upsert(tx, Tuple.create()\\n            .set(\\"account_id\\", payment.accountId)\\n            .set(\\"balance\\", account.decimalValue(\\"balance\\").subtract(payment.amount)));\\n\\n        return PaymentResult.SUCCESS;\\n    });\\n    // Apache Ignite handles all distributed coordination automatically:\\n    // - Prevents duplicate processing during network partitions\\n    // - Ensures balance consistency across all nodes\\n    // - Maintains transaction ordering for compliance\\n    // - Provides automatic failure recovery\\n}\\n```\\n\\n**Automatic coordination benefits:**\\n\\n- **No coordination code**: Write business logic, Ignite handles distributed consistency\\n- **No duplicate processing**: Network partitions cannot create duplicate payments\\n- **No configuration needed**: Consistency works automatically without setup\\n- **No performance penalty**: Coordination happens in the background without blocking operations\\n\\n## Advanced Features: Manual Coordination Control\\n\\nFor specialized use cases, Apache Ignite provides manual control over distributed coordination. Most applications never need this.\\n\\n### When Manual Control Matters\\n\\n- Custom distributed workflows with specific ordering requirements\\n- Multi-step operations requiring atomic coordination across steps\\n- Application-specific conflict resolution logic\\n- Performance-critical operations needing direct coordination control\\n\\n```java\\n// Advanced RAFT control for custom distributed algorithms\\npublic class CustomPaymentWorkflowRaft {\\n\\n    private final RaftGroupService paymentRaftGroup;\\n    private final ClusterService clusterService;\\n\\n    public CompletableFuture<PaymentResult> processCustomPaymentWorkflow(PaymentRequest payment) {\\n        // Custom multi-step payment workflow requiring specific consensus behavior\\n        CustomPaymentCommand paymentCommand = new CustomPaymentCommand(payment);\\n\\n        // Direct RAFT group control for specialized workflow\\n        return paymentRaftGroup.run(paymentCommand)\\n            .thenApply(result -> {\\n                // Custom workflow processed with specialized consensus logic\\n                return (PaymentResult) result;\\n            });\\n    }\\n\\n    // Custom command for specialized payment workflows\\n    public static class CustomPaymentCommand implements Command {\\n        private final PaymentRequest payment;\\n\\n        public CustomPaymentCommand(PaymentRequest payment) {\\n            this.payment = payment;\\n        }\\n\\n        public PaymentRequest getPayment() {\\n            return payment;\\n        }\\n    }\\n\\n    // Custom RAFT group listener for specialized processing\\n    public class CustomPaymentRaftGroupListener implements RaftGroupListener {\\n\\n        @Override\\n        public void onWrite(Iterator<CommandClosure<WriteCommand>> iterator) {\\n            while (iterator.hasNext()) {\\n                CommandClosure<WriteCommand> closure = iterator.next();\\n\\n                if (closure.command() instanceof CustomPaymentCommand) {\\n                    CustomPaymentCommand cmd = (CustomPaymentCommand) closure.command();\\n                    PaymentResult result = processCustomPaymentWorkflow(cmd.getPayment());\\n                    closure.result(result);\\n                }\\n            }\\n        }\\n\\n        private PaymentResult processCustomPaymentWorkflow(PaymentRequest payment) {\\n            // Custom distributed workflow requiring specific consensus behavior\\n            // This would implement specialized logic not available through standard transactions\\n            return new PaymentResult();\\n        }\\n    }\\n}\\n```\\n\\n**Advanced Control Use Cases:**\\n\\n- Custom distributed state machines\\n- Specialized ordering requirements\\n- Performance-critical consensus operations\\n- Application-specific replication logic\\n\\n**RAFT Performance Advantages:**\\n\\n- **Leader-based**: Single node processes operations, eliminating coordination overhead\\n- **Pipeline efficiency**: Leaders process operations without blocking\\n- **Majority consensus**: Only majority nodes required (faster than unanimous)\\n- **Log-based replication**: Efficient state machine replication\\n\\n### RAFT Group Configuration\\n\\nRAFT groups are created and managed through RaftManager for specialized processing requirements:\\n\\n```java\\n// RAFT group configuration for specialized processing\\npublic class CustomRaftConfiguration {\\n\\n    private final RaftManager raftManager;\\n    private final ClusterService clusterService;\\n    private final String groupId = \\"custom-processing-group\\";\\n\\n    public RaftGroupService createCustomRaftGroup() {\\n        ClusterNode localNode = clusterService.topologyService().localMember();\\n        RaftNodeId nodeId = new RaftNodeId(groupId, localNode);\\n        PeersAndLearners configuration = selectProcessingNodes();\\n\\n        CustomRaftGroupListener listener = new CustomRaftGroupListener();\\n        RaftGroupEventsListener eventsListener = new CustomRaftGroupEventsListener();\\n\\n        return raftManager.startRaftGroupNode(\\n            nodeId,\\n            configuration,\\n            listener,\\n            eventsListener,\\n            RaftGroupService::new,\\n            createRaftOptionsConfigurer()\\n        );\\n    }\\n\\n    private PeersAndLearners selectProcessingNodes() {\\n        // Select available cluster nodes for RAFT group\\n        List<Peer> peers = clusterService.topologyService().allMembers().stream()\\n            .limit(5) // Optimal RAFT group size for consensus\\n            .map(node -> new Peer(node.name()))\\n            .collect(Collectors.toList());\\n\\n        // Create learners list (can be empty for basic setup)\\n        List<Peer> learners = List.of();\\n\\n        return PeersAndLearners.fromPeers(peers, learners);\\n    }\\n\\n    private RaftGroupOptionsConfigurer createRaftOptionsConfigurer() {\\n        return options -> {\\n            // Configure RAFT group options for specialized processing\\n            RaftGroupOptions groupOptions = (RaftGroupOptions) options;\\n            groupOptions.serverDataPath(Paths.get(\\"/path/to/raft/data\\"));\\n        };\\n    }\\n}\\n```\\n\\n## Consensus Performance Under High-Velocity Load\\n\\n### RAFT Consensus Performance Impact\\n\\n**Standard Transaction Performance:**\\n\\n```java\\n@Benchmark\\npublic class StandardTransactionPerformance {\\n\\n    @Benchmark\\n    public PaymentResult processStandardPayment() {\\n        long startTime = System.nanoTime();\\n\\n        // Standard transaction - RAFT consensus handled automatically\\n        PaymentResult result = ignite.transactions().runInTransaction(tx - {\\n            // Business logic - Ignite handles distributed consistency\\n            return processPaymentLogic(paymentRequest, tx);\\n        });\\n\\n        long processingTime = System.nanoTime() - startTime;\\n        // Performance includes automatic RAFT consensus overhead\\n\\n        return result;\\n    }\\n}\\n```\\n\\n**Standard Operation Performance:**\\n\\n- **Transaction processing**: Milliseconds range depending on cluster configuration\\n- **Automatic consensus**: Ignite optimizes RAFT for table operations\\n- **Developer simplicity**: No consensus code required\\n- **Throughput**: Scales with cluster capacity\\n\\n**Advanced RAFT Performance** (when using direct control):\\n\\n- **Custom processing**: Microseconds for specialized logic\\n- **Direct consensus**: Application controls consensus behavior\\n- **Specialized optimization**: Custom performance tuning possible\\n- **Complex workflows**: Multi-step distributed operations\\n\\n### Handling Network Partitions During High Load\\n\\n**Partition Tolerance in Distributed Processing:**\\n\\n```java\\npublic class RaftPartitionHandling {\\n\\n    private final RaftGroupService raftGroupService;\\n    private final ClusterService clusterService;\\n\\n    public CompletableFuture<String> handleOperationDuringPartition(String operationData) {\\n        // RAFT automatically handles network partitions through leader-based consensus\\n        Peer currentLeader = raftGroupService.leader();\\n\\n        if (currentLeader != null && isNodeInCluster()) {\\n            // This node can communicate with cluster - process operation\\n            CustomCommand command = new CustomCommand(operationData);\\n            return raftGroupService.run(command)\\n                .thenApply(result -> \\"Operation completed: \\" + result);\\n        } else {\\n            // This node is isolated or no leader available - reject operation\\n            return CompletableFuture.completedFuture(\\"Operation temporarily unavailable\\");\\n        }\\n    }\\n\\n    private boolean isNodeInCluster() {\\n        // Check if this node is still part of the active cluster topology\\n        ClusterNode localNode = clusterService.topologyService().localMember();\\n        Collection<ClusterNode> allMembers = clusterService.topologyService().allMembers();\\n\\n        return allMembers.contains(localNode);\\n    }\\n\\n    // Custom command implementation for RAFT processing\\n    public static class CustomCommand implements Command {\\n        private final String data;\\n\\n        public CustomCommand(String data) {\\n            this.data = data;\\n        }\\n\\n        public String getData() {\\n            return data;\\n        }\\n    }\\n}\\n```\\n\\n**Partition Behavior:**\\n\\n- **Majority partition**: Continues processing operations with full consistency\\n- **Minority partition**: Stops processing to prevent split-brain scenarios\\n- **Recovery**: Partitions automatically reconcile when network heals\\n- **Data safety**: No data loss or duplication across partitions\\n\\n### Leader Election Performance\\n\\n**Fast Leader Election for Business Continuity:**\\n\\n```java\\npublic class RaftLeaderElection {\\n\\n    public void handleLeaderFailureDuringPeakLoad() {\\n        // RAFT leader election for distributed processing\\n        long electionStart = System.currentTimeMillis();\\n\\n        raftGroup.refreshLeader().thenAccept(newLeader -> {\\n            long electionTime = System.currentTimeMillis() - electionStart;\\n\\n            // Election time depends on network conditions and RAFT configuration\\n            log.info(\\"Leader election completed in {}ms\\", electionTime);\\n\\n            // Resume processing immediately\\n            resumeProcessing(newLeader);\\n        });\\n    }\\n\\n    private void resumeProcessing(Peer newLeader) {\\n        // New leader immediately continues from RAFT log state\\n        // No data loss or inconsistency during leadership change\\n        log.info(\\"Processing resumed under new leader: {}\\", newLeader.consistentId());\\n    }\\n}\\n```\\n\\n**Leader Election Characteristics:**\\n\\n- **Election time**: Configurable based on network conditions and RAFT settings\\n- **Service continuity**: Processing resumes immediately after election\\n- **Data consistency**: All committed operations preserved across leader changes\\n- **Automatic recovery**: No manual intervention required\\n\\n## Real-World Consensus Scenarios\\n\\n### Bank Payment Processing Under Load\\n\\n**Daily Payment Volume**: 1 million payments across 24 hours\\n\\n**Standard Bank Payment Processing:**\\n\\n```java\\n// Bank payment processing - Ignite handles RAFT automatically\\n@Service\\npublic class BankPaymentProcessor {\\n\\n    public PaymentResult processInterBankTransfer(TransferRequest transfer) {\\n        // Standard transaction processing with automatic consistency\\n        return ignite.transactions().runInTransaction(tx -> {\\n            Table senderAccounts = ignite.tables().table(\\"sender_accounts\\");\\n            Table receiverAccounts = ignite.tables().table(\\"receiver_accounts\\");\\n            Table auditLog = ignite.tables().table(\\"audit_log\\");\\n\\n            // Atomic transfer with automatic RAFT consensus\\n            Tuple senderKey = Tuple.create().set(\\"account_id\\", transfer.senderAccountId);\\n            Tuple senderAccount = senderAccounts.recordView().get(tx, senderKey);\\n\\n            if (senderAccount.decimalValue(\\"balance\\").compareTo(transfer.amount) < 0) {\\n                return PaymentResult.INSUFFICIENT_FUNDS;\\n            }\\n\\n            // Update both accounts atomically with automatic consensus\\n            BigDecimal newSenderBalance = senderAccount.decimalValue(\\"balance\\").subtract(transfer.amount);\\n            senderAccounts.recordView().upsert(tx,\\n                Tuple.create()\\n                    .set(\\"account_id\\", transfer.senderAccountId)\\n                    .set(\\"balance\\", newSenderBalance));\\n\\n            receiverAccounts.recordView().upsert(tx,\\n                Tuple.create()\\n                    .set(\\"account_id\\", transfer.receiverAccountId)\\n                    .set(\\"balance\\", transfer.amount));\\n\\n            // Audit logging with automatic replication\\n            auditLog.recordView().insert(tx, createAuditRecord(transfer));\\n\\n            return PaymentResult.SUCCESS;\\n        });\\n    }\\n}\\n```\\n\\n**For Complex Multi-Bank Workflows** (advanced scenarios):\\n\\n```java\\n// Custom consensus for specialized inter-bank protocols\\npublic class AdvancedInterBankProcessor {\\n\\n    public PaymentResult processComplexInterBankWorkflow(TransferRequest transfer) {\\n        // Specialized workflow requiring custom consensus behavior\\n        InterBankWorkflowCommand command = new InterBankWorkflowCommand(transfer);\\n\\n        return paymentRaftGroup.run(command)\\n            .thenApply(result -> {\\n                // Custom workflow with specialized consistency requirements\\n                return (PaymentResult) result;\\n            });\\n    }\\n}\\n```\\n\\n**Performance Under Peak Load:**\\n\\n- **Peak hour volume**: 100,000 payments/hour (27/second average, 200/second peak)\\n- **Consensus latency**: Depends on network and storage configuration\\n- **System availability**: High availability through RAFT consensus and automatic failover\\n- **Regulatory compliance**: Guaranteed transaction ordering accuracy\\n\\n### E-commerce Order Processing\\n\\n**Flash Sale Event**: 50,000 orders in 1 hour\\n\\n**Standard Order Processing:**\\n\\n```java\\n// E-commerce order processing - automatic consistency\\npublic class FlashSaleOrderProcessor {\\n\\n    public OrderResult processFlashSaleOrder(OrderRequest order) {\\n        // Standard transaction handling flash sale inventory\\n        return ignite.transactions().runInTransaction(tx -> {\\n            Table inventory = ignite.tables().table(\\"inventory\\");\\n            Table orders = ignite.tables().table(\\"orders\\");\\n            Table payments = ignite.tables().table(\\"payments\\");\\n\\n            // Inventory check with automatic consistency\\n            Tuple productKey = Tuple.create().set(\\"product_id\\", order.productId);\\n            Tuple product = inventory.recordView().get(tx, productKey);\\n\\n            int availableQuantity = product.intValue(\\"quantity\\");\\n            if (availableQuantity < order.quantity) {\\n                return OrderResult.OUT_OF_STOCK;\\n            }\\n\\n            // Atomic inventory update, order creation, payment processing\\n            inventory.recordView().upsert(tx,\\n                Tuple.create()\\n                    .set(\\"product_id\\", order.productId)\\n                    .set(\\"quantity\\", availableQuantity - order.quantity));\\n\\n            orders.recordView().insert(tx, createOrderRecord(order));\\n            payments.recordView().insert(tx, createPaymentRecord(order.payment));\\n\\n            return OrderResult.SUCCESS;\\n        });\\n    }\\n}\\n```\\n\\n**For Specialized Inventory Algorithms** (advanced scenarios):\\n\\n```java\\n// Custom consensus for complex inventory management\\npublic class AdvancedInventoryProcessor {\\n\\n    public OrderResult processWithCustomInventoryLogic(OrderRequest order) {\\n        // Specialized inventory workflow requiring custom consensus\\n        CustomInventoryCommand command = new CustomInventoryCommand(order);\\n        return inventoryRaftGroup.run(command)\\n            .thenApply(result -> (OrderResult) result);\\n    }\\n}\\n```\\n\\n**Flash Sale Performance:**\\n\\n- **Order processing rate**: Scales with cluster capacity and configuration\\n- **Inventory consistency**: Maintained through RAFT consensus (prevents overselling)\\n- **Payment consistency**: Guaranteed through RAFT consensus (prevents duplicate charges)\\n- **Customer experience**: Response time depends on network and processing configuration\\n\\n## Business Impact of Distributed Consistency\\n\\n### Risk Mitigation\\n\\n**Financial Risk Reduction:**\\n\\n- **Payment accuracy**: Eliminates duplicate payments and lost transactions\\n- **Regulatory compliance**: Guaranteed transaction ordering for audit trails\\n- **System reliability**: Automatic failover maintains business continuity\\n- **Data protection**: Strong consistency prevents data corruption\\n\\n**Operational Risk Reduction:**\\n\\n- **Manual intervention**: Consensus automation reduces human error\\n- **System recovery**: Automatic partition healing reduces downtime\\n- **Capacity planning**: Predictable consensus behavior enables accurate sizing\\n- **Incident response**: Built-in fault tolerance reduces emergency responses\\n\\n### Revenue Protection\\n\\n**Payment Processing Firm Benefits:**\\n\\n- **Transaction volume**: Supports high-volume payment processing\\n- **System availability**: RAFT consensus provides automatic failover\\n- **Consistency guarantees**: Eliminates payment errors that damage customer trust\\n- **Scalability**: Horizontal scaling through RAFT group partitioning\\n\\n**E-commerce Platform Benefits:**\\n\\n- **Peak load handling**: Flash sales process high volumes without consistency issues\\n- **Customer trust**: Zero payment errors maintain customer confidence\\n- **Competitive advantage**: Reliable order processing during high-demand events\\n- **Revenue capture**: Consistent order processing vs eventual consistency risks\\n\\n### Operational Efficiency\\n\\n**Development Team Benefits:**\\n\\n- **Simplified architecture**: Single consistency model across all operations\\n- **Reduced debugging**: Strong consistency eliminates race condition bugs\\n- **Faster deployment**: Consensus handles failure scenarios automatically\\n- **Predictable behavior**: Deterministic failure modes simplify testing\\n\\n**Operations Team Benefits:**\\n\\n- **Automated failover**: RAFT consensus reduces manual intervention requirements\\n- **Consistent monitoring**: Single consistency model simplifies observability\\n- **Predictable recovery**: Automated partition healing reduces incident response time\\n- **Scalable operations**: Consensus scales with cluster size\\n\\n## The Consistency-Performance Balance\\n\\nTraditional distributed systems force trade-offs between consistency and performance. Strong consistency requires coordination overhead. Eventual consistency risks business-critical errors.\\n\\nApache Ignite\'s RAFT implementation optimizes for both consistency and performance. Strong consistency guarantees protect business operations while consensus performance supports high-velocity application requirements.\\n\\n**The principle: Consistency should enable performance, not limit it.**\\n\\nWhen your distributed system maintains strong consistency without significant performance penalties, you eliminate the architectural compromises that force trade-offs between business safety and operational speed.\\n\\nHigh-velocity applications need both consistency guarantees and performance characteristics. RAFT consensus provides the distributed coordination foundation that enables both requirements simultaneously.\\n\\n*Return next Tuesday for Part 7, that examines how MVCC transactions build on the RAFT consensus foundation to provide ACID guarantees optimized for high-frequency operations. This ensures that distributed consistency enables rather than constrains transaction processing performance.*"},{"id":"/2025/12/23/ignite3-architecture-p5","metadata":{"permalink":"/suggested-site/blog/2025/12/23/ignite3-architecture-p5","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-12-23-ignite3-architecture-p5.md","source":"@site/blog/2025-12-23-ignite3-architecture-p5.md","title":"Eliminating Data Movement: The Hidden Cost of Distributed Event Processing","description":"Your high-velocity application processes events fast enough until it doesn\'t. The bottleneck isn\'t CPU or memory. It\'s data movement. Every time event processing requires data from another node, network latency adds milliseconds that compound into seconds of delay under load.","date":"2025-12-23T00:00:00.000Z","tags":[{"inline":true,"label":"architecture","permalink":"/suggested-site/blog/tags/architecture"},{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":8.65,"hasTruncateMarker":true,"authors":[{"name":"Michael Aglietti","title":"Apache Ignite Contributor and DevRel","url":"https://github.com/maglietti","imageURL":"/suggested-site/img/authors/maglietti.png","key":"maglietti","page":null}],"frontMatter":{"title":"Eliminating Data Movement: The Hidden Cost of Distributed Event Processing","authors":["maglietti"],"date":"2025-12-23T00:00:00.000Z","tags":["architecture","technical","ignite","ignite3"]},"unlisted":false,"prevItem":{"title":"Distributed Consistency Under Load: When High-Velocity Meets High-Availability","permalink":"/suggested-site/blog/2025/12/30/ignite3-architecture-p6"},"nextItem":{"title":"Integrated Platform Performance: Maintaining Speed Under Pressure","permalink":"/suggested-site/blog/2025/12/16/ignite3-architecture-p4"}},"content":"Your high-velocity application processes events fast enough until it doesn\'t. The bottleneck isn\'t CPU or memory. It\'s data movement. Every time event processing requires data from another node, network latency adds milliseconds that compound into seconds of delay under load.\\n\\n\x3c!-- truncate --\x3e\\n\\n---\\n\\n_Part 5 of 8 in the Apache Ignite 3 Architecture Series_\\n\\n---\\n\\nConsider a financial trading system processing peak loads of 10,000 trades per second. Each trade requires risk calculations against a customer\'s portfolio. If portfolio data lives on different nodes than the processing logic, network round-trips create an impossible performance equation: 10,000 trades \xd7 2ms network latency = 20 seconds of network delay per second of wall clock time.\\n\\nApache Ignite eliminates this constraint through data collocation. Related data and processing live on the same nodes, transforming distributed operations into local memory operations.\\n\\n**The result: distributed system performance without distributed system overhead.**\\n\\n## The Data Movement Tax on High-Velocity Applications\\n\\n### Network Latency Arithmetic\\n\\nAt scale, network latency creates mathematical impossibilities:\\n\\n**Here\'s what distributed processing costs in practice:**\\n\\n```java\\n// Traditional distributed processing (data on different nodes)\\nlong startTime = System.nanoTime();\\n// 1. Fetch event data (potentially remote: 0.5-2 ms)\\nEventData event = eventService.getEvent(eventId);                       // Network: 0.5-2 ms\\n// 2. Fetch related customer data (potentially remote: 0.5-2 ms)\\nCustomerData customer = customerService.getCustomer(event.customerId);  // Network: 0.5-2 ms\\n// 3. Fetch processing rules (potentially remote: 0.5-2 ms)\\nProcessingRules rules = rulesService.getRules(customer.segment);        // Network: 0.5-2 ms\\n// 4. Execute processing logic (local: 0.1 ms)\\nProcessingResult result = processEvent(event, customer, rules);         // CPU: 0.1 ms\\n// 5. Store results (potentially remote: 0.5-2 ms)\\nresultService.storeResult(eventId, result);                             // Network: 0.5-2 ms\\nlong totalTime = System.nanoTime() - startTime;\\n// Total: 2.6-10.1 ms per event (90%+ network overhead)\\n```\\n\\n**At Scale:**\\n\\n- 1,000 events/sec \xd7 5 ms average = 5 seconds processing time per second (impossible)\\n- 10,000 events/sec \xd7 5 ms average = 50 seconds processing time per second (catastrophic)\\n\\n### The Compound Effect of Distribution\\n\\nReal applications don\'t just move data once per event. They move data multiple times:\\n\\n**Here\'s how the cascade effect compounds:**\\n\\n```java\\n// Multi-hop data movement for single order\\nOrderEvent order = getOrder(orderId);                                    // Network hop 1: 1 ms\\nCustomerData customer = getCustomer(order.customerId);                   // Network hop 2: 1 ms\\nInventoryData inventory = getInventory(order.productId);                 // Network hop 3: 1 ms\\nPricingData pricing = getPricing(order.productId, customer.segment);     // Network hop 4: 1 ms\\nPaymentData payment = processPayment(order.amount, customer.paymentId);  // Network hop 5: 2 ms\\nShippingData shipping = calculateShipping(order, customer.address);      // Network hop 6: 1 ms\\nPromotionData promotions = applyPromotions(order, customer);             // Network hop 7: 1 ms\\n// Total network overhead: 8ms per order (before any business logic)\\n```\\n\\n**The cascade effect**: Each data dependency creates another network round-trip. Complex event processing can require 10+ network operations per event.\\n\\n## Strategic Data Placement Through Collocation\\n\\n### Apache Ignite Collocation Architecture\\n\\nApache Ignite uses deterministic hash distribution to ensure related data lands on the same nodes. The platform automatically generates consistent hash values for collocation keys, ensuring all data with the same collocation key always lands on the same node. This deterministic placement means that once data is collocated, subsequent access patterns benefit from data locality without manual coordination.\\n\\n### Table Design for Event Processing Collocation\\n\\n```sql\\n-- Create distribution zone for customer-based collocation\\nCREATE ZONE customer_zone WITH\\n    partitions=64,\\n    replicas=3;\\n\\n-- Orders table using customer-based distribution zone\\nCREATE TABLE orders (\\n    order_id BIGINT PRIMARY KEY,\\n    customer_id BIGINT,\\n    product_id BIGINT,\\n    amount DECIMAL(10,2),\\n    order_date TIMESTAMP\\n) WITH ZONE=\'customer_zone\';\\n\\n-- Customer data using same distribution zone for collocation\\nCREATE TABLE customers (\\n    customer_id BIGINT PRIMARY KEY,\\n    name VARCHAR(100),\\n    segment VARCHAR(20),\\n    payment_method VARCHAR(50)\\n) WITH ZONE=\'customer_zone\';\\n\\n-- Customer pricing using same zone for collocation\\nCREATE TABLE customer_pricing (\\n    customer_id BIGINT,\\n    product_id BIGINT,\\n    price DECIMAL(10,2),\\n    discount_rate DECIMAL(5,2),\\n    PRIMARY KEY (customer_id, product_id)\\n) WITH ZONE=\'customer_zone\';\\n```\\n\\n**Result**: All tables using the same distribution zone share the same partitioning strategy. Data for customer 12345 distributes to the same partition across all tables, enabling local processing without network communication.\\n\\n### Compute Collocation for Event Processing\\n\\n**Processing Moves to Where Data Lives:**\\n\\nInstead of moving data to processing logic, compute jobs execute on the nodes where related data already exists. For customer order processing, the compute job runs on the node containing the customer\'s data, orders, and pricing information. All data access becomes local memory operations rather than network calls.\\n\\n**Simple Collocation Example:**\\n\\n```java\\n// Execute processing where customer data lives\\nTuple customerKey = Tuple.create().set(\\"customer_id\\", customerId);\\nCompletableFuture<OrderResult> future = client.compute().executeAsync(\\n    JobTarget.colocated(\\"customers\\", customerKey),  // Run on node with customer data\\n    OrderProcessingJob.class,\\n    customerId\\n);\\n```\\n\\n**Performance Impact**: 8ms distributed processing becomes sub-millisecond collocated processing through data locality.\\n\\n## Real-World Collocation Performance Impact\\n\\n### Financial Risk Calculation Example\\n\\n**Problem**: Trading system needs real-time portfolio risk calculation for each trade.\\n\\n**Here\'s what the distributed approach costs:**\\n\\nTraditional risk calculations require multiple network calls: fetch trade details (1ms), retrieve portfolio data (2ms), get current market prices (1ms), and load risk rules (1ms). The actual risk calculation takes 0.2ms, but network overhead dominates at 5.2ms total. At 10,000 trades per second, this creates 52 seconds of processing time per second. This is mathematically impossible.\\n\\n**Collocated Risk Processing:**\\n\\nWhen account portfolios, trade histories, and risk rules collocate by account ID, risk calculations become local operations. All required data lives on the same node where the processing executes. Network overhead disappears, transforming 5.2ms distributed operations into sub-millisecond local calculations.\\n\\n**Business Impact:**\\n\\n- **Trading velocity**: Process 10,000+ trades per second with real-time risk assessment\\n- **Risk accuracy**: Use complete portfolio context without stale data\\n- **Regulatory compliance**: Meet sub-second risk calculation requirements\\n\\n### IoT Event Processing Example\\n\\n**Problem**: Manufacturing system processes sensor events requiring contextual data for anomaly detection.\\n\\n**Collocated Design**:\\n\\n```sql\\n-- Create distribution zone for equipment-based collocation\\nCREATE ZONE equipment_zone WITH\\n    partitions=32,\\n    replicas=2;\\n\\n-- Sensor data using equipment-based distribution zone\\nCREATE TABLE sensor_readings (\\n    sensor_id BIGINT,\\n    equipment_id BIGINT,\\n    timestamp TIMESTAMP,\\n    temperature DECIMAL(5,2),\\n    pressure DECIMAL(8,2),\\n    vibration DECIMAL(6,3),\\n    PRIMARY KEY (sensor_id, timestamp)\\n) WITH ZONE=\'equipment_zone\';\\n\\n-- Equipment specifications using same zone for collocation\\nCREATE TABLE equipment_specs (\\n    equipment_id BIGINT PRIMARY KEY,\\n    max_temperature DECIMAL(5,2),\\n    max_pressure DECIMAL(8,2),\\n    max_vibration DECIMAL(6,3),\\n    maintenance_schedule VARCHAR(50)\\n) WITH ZONE=\'equipment_zone\';\\n```\\n\\n**Processing Performance:**\\n\\nAnomaly detection jobs execute on the nodes containing the equipment data they analyze. Current sensor readings, historical patterns, and equipment specifications all reside locally. The processing accesses recent sensor data, compares against equipment tolerances, and detects anomalies without any network calls.\\n\\n**Performance Outcome**: Sub-millisecond anomaly detection vs multi-millisecond distributed processing. Single cluster processes tens of thousands of sensor readings per second with real-time anomaly detection.\\n\\n## Collocation Strategy Selection\\n\\n### Event-Driven Collocation Patterns\\n\\n**Customer-Centric Applications**:\\n\\n```sql\\n-- Customer-focused distribution zone\\nCREATE ZONE customer_zone WITH partitions=64;\\nCREATE TABLE orders (...) WITH ZONE=\'customer_zone\';\\n```\\n\\n- Orders, payments, preferences, history distributed by customer key\\n- Customer service queries access data from same partition\\n- Personalization engines process complete customer context locally\\n\\n**Time-Series Event Processing**:\\n\\n```sql\\n-- Time-based distribution zone\\nCREATE ZONE hourly_zone WITH partitions=24;\\nCREATE TABLE events (...) WITH ZONE=\'hourly_zone\';\\n```\\n\\n- Recent events distribute based on time windows\\n- Historical analysis accesses time-coherent partitions\\n- Event correlation happens without cross-node communication\\n\\n**Geographic Distribution**:\\n\\n```sql\\n-- Region-based distribution zone\\nCREATE ZONE regional_zone WITH partitions=16;\\nCREATE TABLE locations (...) WITH ZONE=\'regional_zone\';\\n```\\n\\n- Regional data partitions to regional node groups\\n- Location-aware services access local partition data\\n- Geographic analytics minimize cross-region data movement\\n\\n### Automatic Query Optimization Through Collocation\\n\\n**When related data lives together, query performance transforms:**\\n\\n```sql\\n-- Before collocation: expensive cross-node JOINs\\nSELECT c.name, o.order_total, p.amount\\nFROM customers c\\n  JOIN orders o ON c.customer_id = o.customer_id\\n  JOIN payments p ON o.order_id = p.order_id\\nWHERE c.customer_id = 12345;\\n-- Network overhead: 3 tables \xd7 potential cross-node fetches = high latency\\n\\n-- After collocation: local memory JOINs\\n-- Same query, but all customer 12345 data lives on same node\\n-- Result: JOIN operations become local memory operations\\n```\\n\\n**Complex Analytics Become Local Operations:**\\n\\n```java\\n// Complex analytical query becomes local operation\\nResultSet<SqlRow> customerAnalysis = client.sql().execute(tx, \\"\\"\\"\\n    SELECT\\n        c.segment,\\n        COUNT(o.order_id) as order_count,\\n        SUM(o.amount) as total_spent,\\n        AVG(p.processing_time) as avg_payment_time\\n    FROM customers c\\n      JOIN orders o ON c.customer_id = o.customer_id\\n      JOIN payments p ON o.order_id = p.order_id\\n    WHERE c.registration_date >= ?\\n    GROUP BY c.segment\\n    HAVING total_spent > 10000\\n\\"\\"\\", lastMonth);\\n// When all three tables share the same distribution zone:\\n// - Multi-table JOINs execute locally per partition\\n// - No network overhead for related data access\\n// - Query performance scales with CPU, not network bandwidth\\n```\\n\\n**Query Performance Transformation:**\\n\\n- **JOIN operations**: Cross-node network calls \u2192 local memory operations\\n- **Complex analytics**: Network-bound \u2192 CPU-bound (much faster)\\n- **Query planning**: Distributed execution \u2192 local partition execution\\n- **Performance scaling**: Limited by network \u2192 limited by CPU/memory\\n\\n**The performance transformation**: Query optimization through data placement. When related data lives together, complex queries become simple local operations, fundamentally changing performance characteristics.\\n\\n### Performance Validation\\n\\n**Collocation Effectiveness Monitoring:**\\n\\nApache Ignite provides built-in metrics to monitor collocation effectiveness: query response times, network traffic patterns, and CPU utilization versus network wait time. Effective collocation strategies achieve specific performance indicators that demonstrate data locality success.\\n\\n**Success Indicators:**\\n\\n- **Local execution**: 95%+ of queries execute locally without network hops\\n- **Memory-speed access**: Average query latency under 1 ms for collocated data\\n- **CPU utilization**: 80%+ processing time versus network waiting\\n- **Predictable performance**: Consistent response times independent of cluster size\\n\\n## The Business Impact of Eliminating Data Movement\\n\\n### Cost Reduction\\n\\n- **Network Infrastructure**: 10x reduction in inter-node bandwidth requirements\\n- **Hardware Efficiency**: Higher CPU/memory utilization vs network-bound systems\\n- **Operational Complexity**: Fewer moving parts in event processing pipelines\\n\\n### Performance Gains\\n\\n- **Response Time**: 10-50x improvement in event processing latency\\n- **Throughput**: 5-10x higher event processing capacity with same hardware\\n- **Predictability**: Consistent performance independent of network conditions\\n\\n### Application Capabilities\\n\\n- **Real-Time Analytics**: Sub-millisecond analytics on live transactional event streams\\n- **Complex Event Processing**: Multi-step event processing without coordination overhead\\n- **Interactive Applications**: User-facing features with database-backed logic at cache speeds\\n\\n## The Architectural Evolution\\n\\nTraditional distributed systems accept network overhead as inevitable. Apache Ignite eliminates it through intelligent data placement.\\n\\nYour high-velocity application doesn\'t need to choose between distributed scale and local performance. Collocation provides both: the data capacity and fault tolerance of distributed systems with the performance characteristics of single-node processing.\\n\\n**The principle**: Collocate related data, localize dependent processing.\\n\\nEvery network hop you eliminate returns performance to your application\'s processing budget. At high event volumes, those performance gains determine whether your architecture scales with your business success or becomes the constraint that limits it.\\n\\n---\\n\\n*Return next Tuesday for Part 6 to discover how distributed consensus maintains data consistency during high-frequency operations. We\'ll explore how to preserve the performance gains from collocation while ensuring your high-velocity applications remain both fast and reliable.*"},{"id":"/2025/12/16/ignite3-architecture-p4","metadata":{"permalink":"/suggested-site/blog/2025/12/16/ignite3-architecture-p4","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-12-16-ignite3-architecture-p4.md","source":"@site/blog/2025-12-16-ignite3-architecture-p4.md","title":"Integrated Platform Performance: Maintaining Speed Under Pressure","description":"Traditional systems force a choice: real-time analytics or fast transactions. Apache Ignite eliminates this trade-off with integrated platform performance that delivers both simultaneously.","date":"2025-12-16T00:00:00.000Z","tags":[{"inline":true,"label":"architecture","permalink":"/suggested-site/blog/tags/architecture"},{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":11.37,"hasTruncateMarker":true,"authors":[{"name":"Michael Aglietti","title":"Apache Ignite Contributor and DevRel","url":"https://github.com/maglietti","imageURL":"/suggested-site/img/authors/maglietti.png","key":"maglietti","page":null}],"frontMatter":{"title":"Integrated Platform Performance: Maintaining Speed Under Pressure","authors":["maglietti"],"date":"2025-12-16T00:00:00.000Z","tags":["architecture","technical","ignite","ignite3"]},"unlisted":false,"prevItem":{"title":"Eliminating Data Movement: The Hidden Cost of Distributed Event Processing","permalink":"/suggested-site/blog/2025/12/23/ignite3-architecture-p5"},"nextItem":{"title":"How many client connections can Apache Ignite 3 handle?","permalink":"/suggested-site/blog/2025/12/10/ignite3-client-connections"}},"content":"Traditional systems force a choice: real-time analytics or fast transactions. Apache Ignite eliminates this trade-off with integrated platform performance that delivers both simultaneously.\\n\\n\x3c!-- truncate --\x3e\\n\\nFinancial trades execute in microseconds while risk analytics run concurrently on the same live data. Compliance reporting processes millions of transactions without blocking operational processing. This happens through a unified performance architecture that maintains speed characteristics across all workload types.\\n\\n**Real-time analytics breakthrough: query live transactional data without ETL delays or performance interference.**\\n\\n---\\n\\n_Part 4 of 8 in the Apache Ignite 3 Architecture Series_\\n\\n---\\n\\n## Performance Comparison: Traditional vs Integrated\\n\\n### Traditional Multi-System Performance\\n\\n**Concrete performance degradation in financial trading systems:**\\n\\n**Resource Competition Example** (Financial Trading System):\\n\\n```java\\n// Traditional system: workloads interfere with each other\\npublic class TradingSystemBottlenecks {\\n\\n    public void processTradeWithInterference() {\\n        long tradeStart = System.nanoTime();\\n\\n        // 1. High-frequency trading (requires <100\u03bcs)\\n        Trade trade = executeTradeLogic();  // Target: 50\u03bcs\\n\\n        // But system is also running:\\n        // - Risk analytics (heavy CPU usage)\\n        // - Compliance reporting (heavy I/O)\\n        // - Position calculations (memory pressure)\\n\\n        long actualTime = System.nanoTime() - tradeStart;\\n        // Result: 300-2000 microseconds instead of 50 microseconds (6-40x degradation)\\n    }\\n}\\n```\\n\\n**Performance interference causes:**\\n\\n- **CPU competition**: Analytics consume CPU needed for microsecond trading\\n- **Memory competition**: Large queries trigger garbage collection during trades\\n- **I/O competition**: Batch reports block transaction log writes\\n- **Network competition**: Cross-system data movement affects all operations\\n\\n### Multi-System Performance Fragmentation\\n\\nSeparating workloads creates different performance problems:\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"Separate Systems\\"\\n        Trading[Trading System<br/>50\u03bcs trades<br/>BUT: Stale risk data]\\n        Analytics[Analytics System<br/>Fast queries<br/>BUT: Delayed trade data]\\n        Reporting[Reporting System<br/>Complete data<br/>BUT: 10-30min lag]\\n    end\\n\\n    subgraph \\"Data Movement Overhead\\"\\n        Sync1[Trading \u2192 Analytics<br/>2-5ms synchronization]\\n        Sync2[Analytics \u2192 Reporting<br/>Batch ETL delays]\\n        Sync3[Cross-system validation<br/>10-50ms coordination]\\n    end\\n\\n    Trading --\x3e|Real-time feed| Sync1\\n    Sync1 --\x3e Analytics\\n    Analytics --\x3e|Hourly batch| Sync2\\n    Sync2 --\x3e Reporting\\n    Reporting --\x3e|Compliance check| Sync3\\n    Sync3 --\x3e Trading\\n```\\n\\n**Performance Trade-off Reality:**\\n\\n- Fast trading with stale risk calculations (compliance risk)\\n- Fresh analytics with trading delays (performance risk)\\n- Complete reporting with operational lag (business risk)\\n\\n---\\n\\n## Apache Ignite Integrated Performance Architecture\\n\\n### Before/After Performance Transformation\\n\\n**Traditional: Choose between fast transactions OR real-time analytics**\\n\\n- Trading: 50 microseconds per trade (when analytics are OFF)\\n- Analytics: 2-5 second query response (when trading is PAUSED)\\n- Reports: 10-30 minute ETL delay before data availability\\n\\n**Integrated Platform: Fast transactions AND real-time analytics simultaneously**\\n\\n- Trading: 50 microseconds per trade (while analytics run concurrently)\\n- Analytics: 100-500 milliseconds query response (on live trading data)\\n- Reports: Real-time query results (no ETL delays)\\n\\n### Storage Engine Performance Strategy\\n\\nApache Ignite provides two storage engines that solve different performance challenges:\\n\\n**Memory-Only Storage (aimem)**: Optimized for latency-critical workloads requiring maximum speed. High-frequency trading operations, real-time analytics calculations, and session data management benefit from pure memory operations without persistence overhead. Performance characteristics focus on microsecond response times.\\n\\n**Memory-First Persistence (aipersist)**: Combines memory-speed access with durability guarantees through asynchronous persistence. Financial transactions, audit logs, and regulatory data maintain ACID properties while achieving memory-speed performance. Background checkpointing provides durability without blocking operations.\\n\\n**Storage Engine Benefits:**\\n\\n- **Performance optimization**: Each engine optimizes for specific workload characteristics\\n- **Operational flexibility**: Choose durability vs speed based on data requirements\\n- **Resource efficiency**: Avoid persistence overhead when durability isn\'t required\\n- **Mixed workload support**: Different tables use different engines within the same cluster\\n\\n### Memory Management for Consistent Performance\\n\\nPage-based memory management eliminates the serialization overhead and garbage collection interference that affects performance predictability in traditional systems:\\n\\n**Performance Architecture Benefits:**\\n\\n- **Predictable access times**: Direct memory operations without GC interference\\n- **Zero serialization overhead**: Binary operations on native memory layouts\\n- **Cache efficiency**: Page-based data organization optimizes CPU cache usage\\n- **Linear memory scaling**: Performance grows directly with available RAM\\n\\n**The Integration Advantage**: Instead of managing memory efficiency at the application level, the platform handles memory optimization automatically while maintaining the simple APIs your business logic requires.\\n\\n### Asynchronous API Design for Concurrent Processing\\n\\nHigh-velocity applications require non-blocking operations to maximize resource utilization:\\n\\n```java\\n// Concurrent processing without blocking\\npublic class ConcurrentTradingProcessor {\\n\\n    public CompletableFuture<TradingResult> processConcurrentWorkloads(IgniteClient client) {\\n        // Execute multiple operations concurrently\\n        CompletableFuture<Trade> tradeExecution = client.transactions().runInTransactionAsync(tx ->\\n            client.sql().executeAsync(tx, \\"INSERT INTO trades VALUES (?, ?, ?)\\", tradeId, amount, timestamp)\\n        );\\n\\n        CompletableFuture<RiskMetrics> riskCalculation = client.compute().executeAsync(\\n            JobTarget.colocated(\\"trades\\", tradeId),\\n            RiskCalculationJob.class, tradeId\\n        );\\n\\n        CompletableFuture<ComplianceResult> complianceCheck = client.compute().executeAsync(\\n            JobTarget.colocated(\\"trades\\", tradeId),\\n            ComplianceValidationJob.class, tradeId\\n        );\\n\\n        // Combine results when all complete\\n        return CompletableFuture.allOf(tradeExecution, riskCalculation, complianceCheck)\\n            .thenApply(v -> new TradingResult(\\n                tradeExecution.join(),\\n                riskCalculation.join(),\\n                complianceCheck.join()\\n            ));\\n    }\\n}\\n```\\n\\n**Concurrency Benefits:**\\n\\n- **Resource utilization**: CPU cores stay busy while I/O completes\\n- **Throughput scaling**: Process multiple operations per thread\\n- **Latency hiding**: Overlapping operations reduce total processing time\\n\\n---\\n\\n## Performance Under Real-World Load Conditions\\n\\n### Performance Under Mixed Workloads\\n\\n**High-Frequency Trading Performance:**\\n\\nTrade processing achieves sub-microsecond operations through memory-resident data access. Portfolio validation, risk calculations, and trade execution happen locally without network calls. Performance scales linearly with available CPU cores while maintaining consistent latency.\\n\\n**Concurrent Analytics Performance:**\\n\\nRisk analytics run simultaneously with live trading without mutual interference. Portfolio analysis queries process the same data that trading operations update, but analytics access consistent snapshots without blocking trade execution. Complex SQL aggregations complete while high-frequency trading continues at full speed.\\n\\n**The Performance Integration**: Traditional systems force trade-offs between transaction speed and analytical capability. Integrated platform performance eliminates these constraints by supporting both workload types within the same optimized architecture.\\n\\n### Unified Access Performance Characteristics\\n\\n**Here\'s how the same data maintains optimal performance across different access patterns:**\\n\\n```java\\n// Trading data accessed through optimal API for each use case\\nTable tradesTable = client.tables().table(\\"trades\\");\\n// 1. Key-value access for high-frequency lookups\\nTrade trade = tradesTable.keyValueView()\\n    .get(tx, Tuple.create().set(\\"trade_id\\", tradeId));     // <1 ms lookup\\n// 2. SQL access for complex risk analytics\\nResultSet<SqlRow> riskAnalysis = client.sql().execute(tx,\\n    \\"SELECT account_id, SUM(quantity * price) as exposure \\" +\\n    \\"FROM trades WHERE trade_date = CURRENT_DATE \\" +\\n    \\"GROUP BY account_id HAVING exposure > 1000000\\");      // Parallel execution\\n// 3. Record access for type-safe transaction processing\\nTradeRecord record = tradesTable.recordView()\\n    .get(tx, new TradeRecord(tradeId));                     // Type-safe operations\\n```\\n\\n**Performance Optimization per Access Pattern:**\\n\\n- **Key-value operations**: Direct memory access with microsecond response times\\n- **SQL operations**: Query optimization with parallel execution and colocation\\n- **Record operations**: Type-safe processing without serialization overhead\\n- **All operations**: Share same memory-resident data and transaction guarantees\\n\\n**The unified performance advantage**: Each access method optimizes for its specific use case while operating against the same high-performance data store. No performance compromises, no data movement overhead.\\n\\n### Compliance Reporting Performance\\n\\n**Regulatory Reporting Without Operational Impact:**\\n\\nCompliance reporting processes millions of transactions for regulatory analysis while live trading continues at full speed. Complex aggregation queries scan large datasets to identify position limit violations, unusual trading patterns, and risk exposure calculations. These operations complete without blocking operational processing through concurrent access control.\\n\\n**Performance Characteristics:**\\n\\n- **High data scan rate**: Reports process complete trading datasets efficiently\\n- **Minimal operational impact**: Compliance queries run concurrently with live trading\\n- **Memory efficient processing**: Large-scale aggregations use streaming patterns\\n- **Transactional consistency**: Reports reflect exact point-in-time data state\\n\\n### Real-Time Analytics Without ETL\\n\\n**Live transactional data analytics without traditional ETL delays:**\\n\\n```java\\n// Analytics query running against live trading data\\n// Read-only transaction for snapshot isolation\\nTransaction readOnlyTx = client.transactions().begin(new TransactionOptions().readOnly(true));\\nResultSet<SqlRow> liveRiskAnalysis = client.sql().execute(readOnlyTx, \\"\\"\\"\\n    SELECT\\n        account_id,\\n        symbol,\\n        SUM(quantity * price) as current_exposure,\\n        COUNT(*) as trade_count,\\n        MAX(timestamp) as last_trade\\n    FROM trades\\n    WHERE trade_date = CURRENT_DATE\\n      AND status = \'EXECUTED\'\\n    GROUP BY account_id, symbol\\n    HAVING current_exposure > 1000000\\n    ORDER BY current_exposure DESC\\n\\"\\"\\");\\n// This query runs concurrently with:\\n// - Live trading operations inserting new trades\\n// - Risk systems updating positions\\n// - Settlement processes modifying trade status\\n// - All without blocking any operational processing\\n```\\n\\n**The Real-Time Analytics Breakthrough:**\\n\\n- **No ETL delays**: Analytics queries run directly against operational data\\n- **No data staleness**: See trades and positions as they happen, not hours later\\n- **No system separation**: Same data serves both operations and analytics\\n- **No performance interference**: Concurrent access prevents analytics from blocking operations\\n\\n**Traditional ETL Problem Solved:**\\n\\n```java\\n// Traditional: Wait for ETL, work with stale data\\n// 1. Trading system writes trades\\n// 2. ETL process extracts (30 min delay)\\n// 3. Analytics warehouse loads (60 min delay)\\n// 4. Dashboard shows 90-minute-old data\\n// Ignite 3: Instant analytics on live data\\n// 1. Trading system writes trades\\n// 2. Analytics queries see data immediately\\n// 3. Dashboard shows real-time positions\\n```\\n\\n**The integrated approach**: The operational database becomes the analytics database when designed for both workloads. No more choosing between fresh data and analytical power.\\n\\n### Intelligent Flow Control Under Extreme Load\\n\\n**The system pressure challenge**: During traffic spikes (market volatility, Black Friday, viral events), traditional systems either drop connections or crash. Apache Ignite responds intelligently with automatic backpressure that maintains system stability while preserving data integrity.\\n\\n```java\\n// Intelligent backpressure prevents system collapse during traffic spikes\\nDataStreamerOptions options = DataStreamerOptions.builder()\\n    .pageSize(10_000)                    // Batch size control\\n    .perPartitionParallelOperations(4)   // Concurrency limits per partition\\n    .autoFlushInterval(1000)             // Memory pressure relief (1 second)\\n    .retryLimit(32)                      // Resilience under pressure\\n    .build();\\n// Publisher provides intelligent flow control - automatically throttles when system pressure detected\\ntry (var publisher = new SubmissionPublisher<DataStreamerItem<TradeRecord>>()) {\\n    CompletableFuture<Void> streamerFut = tradesTable.recordView().streamData(publisher, options);\\n\\n    // System automatically applies backpressure when memory pressure or partition limits reached\\n    // Instead of dropping connections, Ignite 3 intelligently throttles input rates\\n    publisher.submit(DataStreamerItem.of(tradeRecord));\\n\\n    streamerFut.join(); // Completes when all data processed with backpressure applied\\n}\\n```\\n\\n**Backpressure vs Traditional Approaches:**\\n\\n```mermaid\\nflowchart LR\\n    subgraph \\"System Under Load\\"\\n        Events1[High Traffic Spike 50,000 events/sec]\\n        DB1[(Traditional Database)]\\n        Crash1[System Overload 503 Service Unavailable]\\n        Loss1[Data Lost Revenue Impact Customer Complaints]\\n\\n        Events1 --\x3e|Overwhelms| DB1\\n        DB1 --\x3e|Cannot Handle Load| Crash1\\n        Crash1 --\x3e Loss1\\n    end\\n```\\n\\n```mermaid\\nflowchart LR\\n    subgraph \\"Ignite 3 Backpressure\\"\\n        Events2[High Traffic Spike 50,000 events/sec]\\n        Ignite[Apache Ignite Backpressure Control]\\n        Throttle[Intelligent Throttling Flow Control Active]\\n        Success[System Stable Data Preserved 99.9% Uptime]\\n\\n        Events2 --\x3e|High Load Detected| Ignite\\n        Ignite --\x3e|Applies Backpressure| Throttle\\n        Throttle --\x3e|Maintains Stability| Success\\n    end\\n```\\n\\n**Traditional Database**: \\"503 Service Unavailable\\" -> Connections dropped, data lost\\n\\n**Apache Ignite**: \\"Intelligent throttling\\" -> System stays up, data preserved\\n\\n**Real-world scenarios where backpressure saves systems:**\\n\\n- **Market volatility**: 10x normal trading volume handled gracefully without transaction loss\\n- **IoT sensor bursts**: 50M device readings processed without memory exhaustion\\n- **E-commerce spikes**: Black Friday traffic managed without dropped orders\\n- **Data migration**: 100TB+ datasets streamed without overwhelming target systems\\n\\n**The wow moment**: While competitors\' systems crash during peak demand, yours maintains 99.9% uptime through intelligent flow control that automatically adapts to system pressure.\\n\\n---\\n\\n## Performance Optimization Strategies\\n\\n### Workload-Specific Optimization\\n\\n**Trading Workload Optimization:**\\n\\nHigh-frequency trading tables use the volatile memory engine (aimem) for maximum speed. Tables configure colocation by account ID to ensure related trades process on the same nodes. Distribution zones optimize partition count and replica settings for trading-specific access patterns.\\n\\n**Analytics Workload Optimization:**\\n\\nAnalytical processing uses the persistent memory engine (aipersist) for durability with memory-speed access. Market data tables colocate by symbol to optimize time-series queries. Higher partition counts distribute analytical workloads across more nodes for better parallelization.\\n\\n**Configuration Benefits:**\\n\\n- **Workload isolation**: Different storage engines prevent interference between workload types\\n- **Access optimization**: Colocation strategies minimize network overhead for common query patterns\\n- **Resource utilization**: Optimized partition counts maximize hardware utilization\\n- **Performance predictability**: Configuration choices align with specific performance requirements\\n\\n### Performance Monitoring and Validation\\n\\n**Continuous Performance Validation:**\\n\\nIntegrated performance monitoring tracks latency histograms and throughput gauges across all workload types. The system measures interference patterns between trading, analytics, and reporting operations to ensure performance isolation. Automated validation confirms that trading latency remains below microsecond thresholds while analytical queries maintain their target response times.\\n\\n**Performance Validation Results:**\\n\\n- **Trading performance**: Sub-microsecond 99.9th percentile latency under mixed loads\\n- **Analytics performance**: Consistent query response times regardless of trading volume\\n- **Interference detection**: Less than 5% mutual performance impact between workload types\\n- **Capacity planning**: Predictable scaling characteristics enable accurate resource allocation\\n\\n---\\n\\n## Business Impact of Consistent Performance\\n\\n### Risk Reduction Through Performance Predictability\\n\\n**Financial Risk Mitigation:**\\n\\n- **Trading execution**: Consistent low-latency execution prevents slippage\\n- **Risk calculations**: Real-time risk assessment prevents overexposure\\n- **Compliance monitoring**: Immediate violation detection prevents penalties\\n\\n**Operational Risk Mitigation:**\\n\\n- **System capacity**: Predictable performance enables accurate capacity planning\\n- **SLA compliance**: Consistent performance characteristics enable SLA guarantees\\n- **Incident reduction**: Performance predictability reduces operational incidents\\n\\n### Revenue Impact of Performance Consistency\\n\\n**High-Frequency Trading Firm Benefits:**\\n\\n- **Execution advantage**: Microsecond latency improvements translate to competitive advantage\\n- **Risk management**: Real-time risk assessment prevents significant financial exposure\\n- **Operational efficiency**: Consistent performance reduces manual intervention needs\\n\\n**E-commerce Platform Benefits:**\\n\\n- **Response time consistency**: Low-latency checkout processes improve conversion rates\\n- **Analytics availability**: Real-time insights enable rapid revenue optimization\\n- **System reliability**: High availability during peak load prevents revenue loss\\n\\n### Competitive Advantage Through Integration\\n\\n**Market Differentiation:**\\n\\n- **Customer experience**: Millisecond response times vs competitor delays\\n- **Operational agility**: Real-time decision making vs batch processing delays\\n- **Cost efficiency**: Single platform vs multi-system operational overhead\\n\\n**Innovation Enablement:**\\n\\n- **New product capabilities**: Performance characteristics enable previously impossible features\\n- **Market expansion**: Consistent performance supports higher-volume markets\\n- **Technical differentiation**: Platform capabilities become competitive advantages\\n\\n---\\n\\n## The Performance Integration Advantage\\n\\nTraditional architectures force performance trade-offs between workload types. Fast operations require dedicated systems. Analytical processing needs separate infrastructure. Reporting workloads get isolated environments.\\n\\nApache Ignite eliminates these trade-offs through integrated platform performance. All workload types achieve their required performance characteristics within the same system, using the same data, without interference.\\n\\n**The principle: Performance consistency enables operational simplicity.**\\n\\nWhen all workloads perform predictably within the same platform, you eliminate the operational complexity of managing performance trade-offs across multiple systems. Your architecture supports business requirements instead of constraining them.\\n\\nHigh-velocity applications need performance characteristics they can depend on. Integrated platform performance provides both the speed individual operations require and the consistency mixed workloads demand.\\n\\n---\\n\\n_Next: Part 5 explores how data colocation eliminates the network overhead that traditional distributed systems accept as inevitable. This transforms distributed processing into local memory operations while maintaining the scale and fault tolerance benefits of distributed architecture._"},{"id":"/2025/12/10/ignite3-client-connections","metadata":{"permalink":"/suggested-site/blog/2025/12/10/ignite3-client-connections","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-12-10-ignite3-client-connections.md","source":"@site/blog/2025-12-10-ignite3-client-connections.md","title":"How many client connections can Apache Ignite 3 handle?","description":"Apache Ignite 3 manages client connections so efficiently that the scaling limits common in database-style systems simply aren\'t a factor.","date":"2025-12-11T00:00:00.000Z","tags":[{"inline":true,"label":"performance","permalink":"/suggested-site/blog/tags/performance"},{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":2.93,"hasTruncateMarker":true,"authors":[{"name":"Pavel Tupitsyn","title":"Apache Ignite Committer","url":"https://github.com/ptupitsyn","imageURL":"/suggested-site/img/authors/ptupitsyn.png","key":"pavel","page":null}],"frontMatter":{"title":"How many client connections can Apache Ignite 3 handle?","authors":["pavel"],"date":"2025-12-11T00:00:00.000Z","tags":["performance","technical","ignite","ignite3"]},"unlisted":false,"prevItem":{"title":"Integrated Platform Performance: Maintaining Speed Under Pressure","permalink":"/suggested-site/blog/2025/12/16/ignite3-architecture-p4"},"nextItem":{"title":"Schema Evolution Under Operational Pressure: When Downtime Isn\'t an Option","permalink":"/suggested-site/blog/2025/12/09/ignite3-architecture-p3"}},"content":"Apache Ignite 3 manages client connections so efficiently that the scaling limits common in database-style systems simply aren\'t a factor.\\n\\n\x3c!-- truncate --\x3e\\n\\n## The Question\\n\\nA common capacity planning question we get from users is: \\"How many client connections can one Ignite node maintain?\\"\\n\\nWith traditional relational databases, the common knowledge is:\\n\\n- Client connection is typically single-threaded and short-lived. \\"Open -> Do work -> Close\\" is the usual pattern.\\n- The server can handle a limited number of concurrent connections.\\n  - [Postgres defaults to 100](https://www.postgresql.org/docs/current/runtime-config-connection.html#GUC-MAX-CONNECTIONS) `max_connections`.\\n  - Each connection has significant memory overhead ([a few MBs](https://blog.anarazel.de/2020/10/07/measuring-the-memory-overhead-of-a-postgres-connection/)).\\n- An external connection pool (like [PgBouncer](https://www.pgbouncer.org/)) is recommended to improve scalability.\\n\\n[Apache Ignite 3](https://ignite.apache.org/) is quite different:\\n\\n- Client connections are long-lived, multiplexed, and thread-safe. Quite often, a single client connection is enough for the entire application lifetime.\\n- On the server side, each client connection has a small memory footprint (a few KB).\\n\\nThis approach with cheap long-lived connections provides low latency and great scalability for applications:\\n\\n- The connection is always open and responds to queries immediately.\\n- Multiple queries can be executed concurrently over the same connection (multiplexing).\\n- No need for an external connection pool.\\n- Query metadata is cached by the client connection, improving performance for repeated queries.\\n\\nLet\'s see how many concurrent client connections a single Apache Ignite 3 node can handle.\\n\\n---\\n\\n## Testing Setup\\n\\n### Server\\n\\nI\'m going to use the [binary distribution](https://ignite.apache.org/download.cgi) of Apache Ignite 3.1.0 for this test.\\n\\nThe default node configuration is good enough, the only thing I changed was the logging level in `etc/ignite.java.util.logging.properties` to reduce logging overhead.\\n\\n### Client\\n\\nTo establish the connections, I\'m using the [Ignite.NET client](https://www.nuget.org/packages/Apache.Ignite/3.1.0) in a simple console app that connects to the server in a loop and keeps the connections open. After the loop we verify that all connections are still alive.\\n\\nFull program is on GitHub: [https://gist.github.com/ptupitsyn/86056d4143811ba5dde6b2d1704fa948](https://gist.github.com/ptupitsyn/86056d4143811ba5dde6b2d1704fa948)\\n\\n### Ephemeral Port Exhaustion\\n\\nIn the program you can notice the trick with multiple localhost addresses (`127.0.0.1`, `127.0.0.2`, etc). Without it, after about 28k connections, the program fails with a `SocketException (99): Cannot assign requested address`.\\n\\nBasically, every TCP connection has a source `IP:port` pair and the port is chosen from the ephemeral port range (typically 32768\u201360999 on Linux). We can\'t have more connections on the same address than the number of ephemeral ports available. Using multiple localhost addresses works around this limitation.\\n\\n---\\n\\n## Results\\n\\nI\'m starting to get weird errors and timeouts at about 250k (yes, 250 thousand) connections with default settings. At **200k connections** the system is stable and responsive, so I decided to stop the test there.\\n\\nInitial memory usage of the Apache Ignite node was about 200 MB, and with 200k active connections it was about 900 MB after a full GC, about **3.5 KB per connection**.\\n\\nVisualVM screenshot:\\n\\n![VisualVM memory usage with 200k client connections](/img/blog/2025-12-04-How-Many-Client-Connections-Can-Ignite-Handle.png)\\n\\nClient log:\\n\\n```\\nConnected 200000 connections in 00:02:49.2601996\\nVerified connectivity in 00:00:09.1446883\\n```\\n\\nNote that each connection exchanges a heartbeat message every 10 seconds, so the system is not completely idle. We have about 20k small requests per second, but this barely requires any CPU.\\n\\n---\\n\\n## Summary\\n\\nApache Ignite client connections are very lightweight, so open as many as your application requires and keep them open for the best performance!"},{"id":"/2025/12/09/ignite3-architecture-p3","metadata":{"permalink":"/suggested-site/blog/2025/12/09/ignite3-architecture-p3","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-12-09-ignite3-architecture-p3.md","source":"@site/blog/2025-12-09-ignite3-architecture-p3.md","title":"Schema Evolution Under Operational Pressure: When Downtime Isn\'t an Option","description":"Schema changes in traditional databases mean downtime, lost revenue, and deployment chaos across multiple systems. This piece demonstrates how Apache Ignite\'s flexible schema approach helps lets data model evolve at the pace of your business requirements.","date":"2025-12-10T00:00:00.000Z","tags":[{"inline":true,"label":"architecture","permalink":"/suggested-site/blog/tags/architecture"},{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":8.72,"hasTruncateMarker":true,"authors":[{"name":"Michael Aglietti","title":"Apache Ignite Contributor and DevRel","url":"https://github.com/maglietti","imageURL":"/suggested-site/img/authors/maglietti.png","key":"maglietti","page":null}],"frontMatter":{"title":"Schema Evolution Under Operational Pressure: When Downtime Isn\'t an Option","authors":["maglietti"],"date":"2025-12-10T00:00:00.000Z","tags":["architecture","technical","ignite","ignite3"]},"unlisted":false,"prevItem":{"title":"How many client connections can Apache Ignite 3 handle?","permalink":"/suggested-site/blog/2025/12/10/ignite3-client-connections"},"nextItem":{"title":"Memory-First Architecture: The Foundation for High-Velocity Event Processing","permalink":"/suggested-site/blog/2025/12/02/ignite3-architecture-p2"}},"content":"Schema changes in traditional databases mean downtime, lost revenue, and deployment chaos across multiple systems. This piece demonstrates how Apache Ignite\'s flexible schema approach helps lets data model evolve at the pace of your business requirements.\\n\\n\x3c!-- truncate --\x3e\\n\\nYour high-velocity application runs 24/7. Customer expectations don\'t pause for maintenance windows. Functional requirements evolve continuously. Yet traditional database schema changes require downtime, coordination across teams, and careful rollback planning.\\n\\nConsider a payment processing system handling peak loads of 50,000 transactions per second. A new compliance rule requires additional fraud-detection fields. Traditional schema changes would require:\\n\\n- **Coordinate downtime** across payment processing, fraud detection, and reporting systems\\n- **Apply schema changes** to primary database, read replicas, and cache layers\\n- **Deploy application updates** that work with new schema\\n- **Validate data consistency** across all systems\\n\\n**Total downtime**: 2-4 hours. **Lost revenue**: $500K+ for a payment processor.\\n\\nApache Ignite 3 eliminates this constraint through flexible schema evolution. Schema changes apply without downtime, applications adjust automatically, and system operations continue uninterrupted.\\n\\n**The result: operational evolution without operational interruption.**\\n\\n---\\n\\n_Part 3 of 8 in the Apache Ignite 3 Architecture Series_\\n\\n---\\n\\n## The Schema Rigidity Problem at Scale\\n\\n### Traditional Schema Change Overhead\\n\\nHigh-velocity applications face schema evolution challenges that compound with scale:\\n\\n**Multi-System Schema Coordination:**\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"Schema Change Impact\\"\\n        Downtime[Planned Downtime<br/>2-4 hours]\\n        Revenue[Lost Revenue<br/>$500K+ per hour]\\n        Risk[Deployment Risk<br/>Rollback complexity]\\n    end\\n\\n    subgraph \\"Systems Requiring Updates\\"\\n        Primary[(Primary Database<br/>Schema + Data Migration)]\\n        Replica[(Read Replicas<br/>Replication Catch-up)]\\n        Cache[(Cache Layer<br/>Schema Invalidation)]\\n        Analytics[(Analytics Store<br/>ETL Pipeline Updates)]\\n        Apps[Applications<br/>Code + Config Changes]\\n    end\\n\\n    Primary --\x3e Replica\\n    Replica --\x3e Cache\\n    Cache --\x3e Analytics\\n    Analytics --\x3e Apps\\n\\n    Apps --\x3e Downtime\\n    Downtime --\x3e Revenue\\n    Revenue --\x3e Risk\\n```\\n\\n**The Compound Effect:**\\n\\n- Each system adds coordination complexity\\n- Rollback procedures multiply with system count\\n- Testing requires full integration validation\\n- Deployment windows must accommodate slowest system\\n\\n### Real-World Schema Evolution Pressure\\n\\n**E-commerce Platform Evolution** (peak traffic: 100,000 orders per second):\\n\\n**Month 1**: Basic order processing\\n\\n```sql\\nCREATE TABLE orders (\\n    order_id BIGINT PRIMARY KEY,\\n    customer_id BIGINT,\\n    product_id BIGINT,\\n    amount DECIMAL(10,2),\\n    status VARCHAR(20)\\n);\\n```\\n\\n**Month 6**: Add payment processing compliance\\n\\n```sql\\n-- Traditional approach: downtime required\\nALTER TABLE orders ADD COLUMN payment_method VARCHAR(50);\\nALTER TABLE orders ADD COLUMN payment_processor VARCHAR(30);\\nALTER TABLE orders ADD COLUMN compliance_data VARCHAR(500);\\n-- Requires: 2-hour maintenance window\\n-- Impact: $200K lost sales during downtime\\n```\\n\\n**Month 12**: International expansion requirements\\n\\n```sql\\n-- More complexity: multiple system coordination\\nALTER TABLE orders ADD COLUMN currency_code CHAR(3);\\nALTER TABLE orders ADD COLUMN exchange_rate DECIMAL(10,6);\\nALTER TABLE orders ADD COLUMN tax_jurisdiction VARCHAR(50);\\nALTER TABLE orders ADD COLUMN shipping_region VARCHAR(50);\\n-- Requires: 4-hour coordinated deployment\\n-- Impact: $800K lost sales, customer complaints\\n```\\n\\n**The Pattern**: Each functional change requires operational disruption that grows with system complexity.\\n\\n---\\n\\n## Apache Ignite Flexible Schema Architecture\\n\\n### Catalog-Driven Schema Management\\n\\nApache Ignite manages schema evolution through versioned metadata catalogs that coordinate changes across distributed nodes using hybrid logical clocks for timestamp ordering:\\n\\n**Schema Evolution Process:**\\n\\n- **Validation Phase**: New columns validated against existing schema\\n- **Atomic Update**: Schema change applied as single `NewColumnsEntry` operation\\n- **Version Management**: Applications operate against appropriate schema version\\n- **Cluster Coordination**: HybridTimestamp ensures consistent activation\\n\\n**Key Advantages:**\\n\\n- **Atomic Updates**: Schema changes apply as single operations\\n- **Version Management**: Applications can operate against different schema versions\\n- **Validation**: Automatic conflict detection and resolution\\n- **Rollback**: Schema changes are reversible without data loss\\n\\n### Time-Based Schema Consistency\\n\\nSchema changes coordinate across distributed nodes using cluster-wide timestamps:\\n\\n**Consistency Guarantees:**\\n\\n- **Point-in-Time Activation**: All nodes apply schema changes simultaneously\\n- **Transaction Safety**: In-flight operations complete with original schema\\n- **Application Compatibility**: Gradual adoption without breaking changes\\n\\nThe catalog management system uses `HybridTimestamp` values to ensure schema versions activate consistently across all cluster nodes, preventing race conditions and maintaining data integrity during schema evolution.\\n\\n---\\n\\n## Schema Evolution in Production\\n\\n### Adding Fraud Detection Fields (Real-Time)\\n\\n**Business Requirement**: New fraud detection requires additional order data fields.\\n\\n**Traditional Approach**:\\n\\n```sql\\n-- Requires 2-hour maintenance window\\nBEGIN;\\n  ALTER TABLE orders ADD COLUMN fraud_score DECIMAL(5,2);\\n  ALTER TABLE orders ADD COLUMN risk_factors VARCHAR(500);\\n  ALTER TABLE orders ADD COLUMN verification_status VARCHAR(30);\\nCOMMIT;\\n-- Update application code (coordinated deployment)\\n-- Update cache schemas (cache invalidation)\\n-- Update analytics pipelines (ETL modifications)\\n-- Test end-to-end integration\\n```\\n\\n**Apache Ignite Approach**:\\n\\n```java\\n// Schema evolution during live operations\\ntry (IgniteClient client = IgniteClient.builder().addresses(\\"cluster:10800\\").build()) {\\n    // Add fraud detection columns without downtime\\n    client.sql().execute(null, \\"\\"\\"\\n        ALTER TABLE orders ADD COLUMN (\\n            fraud_score DECIMAL(5,2) DEFAULT 0.0,\\n            risk_factors VARCHAR(500) DEFAULT \'\',\\n            verification_status VARCHAR(30) DEFAULT \'PENDING\'\\n        )\\n    \\"\\"\\");\\n\\n    // Applications immediately see new schema\\n    // Existing queries continue working\\n    // New functionality can use additional fields\\n}\\n// Application code adapts automatically\\nclient.transactions().runInTransaction(tx -> {\\n    // Existing order processing continues\\n    client.sql().execute(tx, \\"INSERT INTO orders (order_id, customer_id, amount) VALUES (?, ?, ?)\\",\\n                        orderId, customerId, amount);\\n\\n    // New fraud detection can use additional fields when ready\\n    if (fraudDetectionEnabled) {\\n        client.sql().execute(tx, \\"UPDATE orders SET fraud_score = ?, risk_factors = ? WHERE order_id = ?\\",\\n                           fraudScore, riskFactors, orderId);\\n    }\\n});\\n```\\n\\n**Result**:\\n\\n- **Downtime**: Zero\\n- **Deployment coordination**: None required\\n- **Revenue impact**: Zero\\n- **Time to production**: Minutes instead of hours\\n\\n### Unified Schema Access Across APIs\\n\\n**Here\'s how the evolved schema works seamlessly across different access patterns:**\\n\\n```java\\n// The SAME evolved schema accessible through all APIs immediately\\nTable ordersTable = client.tables().table(\\"orders\\");\\n// 1. Key-value access automatically sees new schema\\nTuple orderTuple = ordersTable.keyValueView()\\n    .get(tx, Tuple.create().set(\\"order_id\\", orderId));\\n// New fields available: orderTuple.stringValue(\\"verification_status\\")\\n// 2. SQL access uses new fraud detection fields immediately\\nResultSet<SqlRow> suspiciousOrders = client.sql().execute(tx,\\n    \\"SELECT order_id, fraud_score, risk_factors \\" +\\n    \\"FROM orders WHERE fraud_score > 0.8 AND verification_status = \'REVIEW\'\\");\\n// 3. Record access handles new fields through schema evolution\\nOrderRecord record = ordersTable.recordView()\\n    .get(tx, new OrderRecord(orderId));\\n// OrderRecord.fraudScore now available without code changes\\n```\\n\\n**Schema Evolution Benefits:**\\n\\n- **No API fragmentation**: Same schema changes work across key-value, SQL, and record APIs\\n- **No deployment coordination**: All access patterns see schema changes immediately\\n- **No data migration**: New fields populate automatically with defaults\\n- **No downtime**: Live applications continue operating during schema evolution\\n\\n**The unified schema advantage**: Schema changes apply once and work immediately across all data access patterns, eliminating the multi-system coordination that creates downtime.\\n\\n### International Expansion Schema Evolution\\n\\n**Business Requirement**: Support multiple currencies and tax jurisdictions.\\n\\n```java\\n// Progressive schema evolution for international expansion\\npublic class InternationalExpansionEvolution {\\n\\n    public void addCurrencySupport(IgniteClient client) {\\n        // Phase 1: Add currency fields (no downtime)\\n        client.sql().execute(null, \\"\\"\\"\\n            ALTER TABLE orders ADD COLUMN (\\n                currency_code CHAR(3) DEFAULT \'USD\',\\n                exchange_rate DECIMAL(10,6) DEFAULT 1.0,\\n                base_amount DECIMAL(10,2)\\n            )\\n        \\"\\"\\");\\n\\n        // Applications continue working with existing USD logic\\n        // New international orders can specify currency\\n    }\\n\\n    public void addTaxSupport(IgniteClient client) {\\n        // Phase 2: Add tax jurisdiction fields (no downtime)\\n        client.sql().execute(null, \\"\\"\\"\\n            ALTER TABLE orders ADD COLUMN (\\n                tax_jurisdiction VARCHAR(50) DEFAULT \'US-FEDERAL\',\\n                tax_rate DECIMAL(5,4) DEFAULT 0.0875,\\n                tax_amount DECIMAL(10,2) DEFAULT 0.0\\n            )\\n        \\"\\"\\");\\n\\n        // Tax calculations adapt automatically\\n    }\\n\\n    public void addShippingSupport(IgniteClient client) {\\n        // Phase 3: Add regional shipping (no downtime)\\n        client.sql().execute(null, \\"\\"\\"\\n            ALTER TABLE orders ADD COLUMN (\\n                shipping_region VARCHAR(50) DEFAULT \'DOMESTIC\',\\n                customs_data VARCHAR(500),\\n                estimated_delivery_days INT DEFAULT 3\\n            )\\n        \\"\\"\\");\\n    }\\n}\\n```\\n\\n**Business Benefits:**\\n\\n- **Continuous Deployment**: Feature releases independent of schema changes\\n- **A/B Testing**: Test international features with subset of traffic\\n- **Risk Reduction**: Gradual rollout instead of big-bang deployment\\n- **Revenue Protection**: No downtime for existing operations\\n\\n---\\n\\n## Schema Evolution Performance Impact\\n\\n### Traditional Schema Change Performance Cost\\n\\n**Large Table Schema Changes** (100M+ records):\\n\\n```sql\\n-- Traditional ALTER TABLE on 100M records\\nALTER TABLE orders ADD COLUMN fraud_score DECIMAL(5,2);\\n-- Performance impact:\\n-- - Table lock: 30-60 minutes\\n-- - I/O overhead: Rewrite entire table\\n-- - Replication lag: Hours to catch up\\n-- - Application unavailability: Complete downtime\\n```\\n\\n**Cost Analysis:**\\n\\n- **Revenue loss**: $500K-$2M per hour of downtime\\n- **Customer impact**: Service unavailable during business hours\\n- **Engineering cost**: 20+ engineer hours for coordination\\n- **Risk**: Single point of failure for rollback\\n\\n### Apache Ignite Schema Evolution Performance\\n\\n**Zero-Downtime Schema Changes** (100M+ records):\\n\\nApache Ignite\'s catalog-based approach enables rapid schema changes by updating metadata rather than restructuring data:\\n\\n**Performance Characteristics:**\\n\\n- **Schema change time**: Fast metadata operations (typically under 100ms)\\n- **Application downtime**: Zero\\n- **Throughput impact**: Minimal during change operation\\n- **Recovery time**: Immediate (no recovery needed)\\n\\nPerformance improves by separating schema metadata management from data storage, allowing schema evolution without touching existing data structures.\\n\\n---\\n\\n## Business Impact of Schema Flexibility\\n\\n### Revenue Protection\\n\\n**E-commerce Platform Example** (processing $10M/month):\\n\\n**Traditional Approach:**\\n\\n- 4 schema changes per year \xd7 3 hours downtime = 12 hours total downtime\\n- Revenue impact: $10M/month \xf7 730 hours/month \xd7 12 hours = $164K lost annually\\n- Engineering overhead: 80 hours coordination \xd7 $150/hour = $12K annually\\n- **Total cost**: $176K annually\\n\\n**Apache Ignite Approach:**\\n\\n- 4 schema changes per year \xd7 0 hours downtime = 0 hours total downtime\\n- Revenue impact: $0 lost\\n- Engineering overhead: 4 hours \xd7 $150/hour = $600 annually\\n- **Total cost**: $600 annually\\n\\n**Annual savings**: $175K+ (99.7% reduction)\\n\\n### Development Velocity Impact\\n\\n**Feature Development Acceleration:**\\n\\n```java\\n// Traditional: Schema change blocks feature development\\npublic class TraditionalFeatureDevelopment {\\n    // Week 1-2: Plan schema changes across systems\\n    // Week 3-4: Coordinate deployment windows\\n    // Week 5: Execute schema changes during downtime\\n    // Week 6-8: Deploy application changes\\n    // Week 9: Validate integration across systems\\n\\n    // Total: 9 weeks from idea to production\\n}\\n// Apache Ignite: Schema and features evolve together\\npublic class FlexibleFeatureDevelopment {\\n    public void developFeatureWithSchemaEvolution() {\\n        // Day 1: Add required schema fields\\n        client.sql().execute(null, \\"ALTER TABLE customers ADD COLUMN loyalty_tier VARCHAR(20) DEFAULT \'STANDARD\'\\");\\n\\n        // Day 1-3: Implement feature logic\\n        // Day 4: Deploy to production (no coordination needed)\\n        // Day 5: Monitor and iterate\\n\\n        // Total: 1 week from idea to production\\n    }\\n}\\n```\\n\\n**Impact**: 9x faster feature delivery through schema flexibility.\\n\\n### Competitive Advantage Through Agility\\n\\n**Market Response Speed:**\\n\\n- **Regulatory compliance**: Adapt to new requirements within hours\\n- **Customer feedback**: Implement requested features without deployment delays\\n- **Competitive pressure**: Launch counter-features without schema coordination overhead\\n\\n**Innovation Capability:**\\n\\n- **A/B testing**: Try schema variations without impacting production\\n- **Experimentation**: Add telemetry fields for new insights\\n- **Personalization**: Evolve customer data models based on behavior patterns\\n\\n---\\n\\n## The Operational Evolution Advantage\\n\\nTraditional databases force trade-offs between schema stability and operational agility. Apache Ignite eliminates this trade-off through flexible schema evolution that supports both operational stability and rapid functional expansion.\\n\\n**The principle**: Your schema should evolve as fast as your business requirements.\\n\\nWhen market demands shift daily but schema changes occur only during monthly maintenance windows, the architecture becomes the bottleneck to feature delivery. Flexible schema evolution ensures the data model advances with business needs rather than restricting them.\\n\\nFast-paced applications can\'t afford architectural constraints that slow adaptation. Schema flexibility becomes a strategic advantage when your system must evolve faster than competitors can deploy.\\n\\n---\\n\\n_Next: Part 4 explores how integrated platform performance maintains consistency across all workload types. This ensures that schema flexibility and business agility don\'t compromise the performance characteristics your application requires._"},{"id":"/2025/12/02/ignite3-architecture-p2","metadata":{"permalink":"/suggested-site/blog/2025/12/02/ignite3-architecture-p2","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-12-02-ignite3-architecture-p2.md","source":"@site/blog/2025-12-02-ignite3-architecture-p2.md","title":"Memory-First Architecture: The Foundation for High-Velocity Event Processing","description":"Traditional databases force a choice: fast memory access or durable storage. High-velocity applications processing 10,000+ events per second hit a wall when disk I/O adds 5-15ms to every transaction.","date":"2025-12-03T00:00:00.000Z","tags":[{"inline":true,"label":"architecture","permalink":"/suggested-site/blog/tags/architecture"},{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":6.65,"hasTruncateMarker":true,"authors":[{"name":"Michael Aglietti","title":"Apache Ignite Contributor and DevRel","url":"https://github.com/maglietti","imageURL":"/suggested-site/img/authors/maglietti.png","key":"maglietti","page":null}],"frontMatter":{"title":"Memory-First Architecture: The Foundation for High-Velocity Event Processing","authors":["maglietti"],"date":"2025-12-03T00:00:00.000Z","tags":["architecture","technical","ignite","ignite3"]},"unlisted":false,"prevItem":{"title":"Schema Evolution Under Operational Pressure: When Downtime Isn\'t an Option","permalink":"/suggested-site/blog/2025/12/09/ignite3-architecture-p3"},"nextItem":{"title":"When Multi-System Complexity Compounds at Scale","permalink":"/suggested-site/blog/2025/11/25/ignite3-architecture-p1"}},"content":"Traditional databases force a choice: fast memory access or durable storage. High-velocity applications processing 10,000+ events per second hit a wall when disk I/O adds 5-15ms to every transaction.\\n\\n\x3c!-- truncate --\x3e\\n\\n**Apache Ignite eliminates this trade-off with memory-first architecture that delivers microsecond response times while maintaining full durability.**\\n\\nEvent data lives in memory for immediate access. Persistence happens asynchronously in the background. By moving operations into memory, typical 7\u201325 ms disk operations drop into the sub-millisecond range while retaining ACID guarantees.\\n\\n**Performance transformation: significant speed improvements with enterprise durability.**\\n\\n---\\n\\n_Part 2 of 8 in the Apache Ignite 3 Architecture Series_\\n\\n---\\n\\n## The Event Processing Performance Challenge\\n\\n**Traditional Database Performance Under Load**\\n\\nWhen applications process high event volumes, disk-based databases create predictable performance degradation:\\n\\n**Single Event Processing (Traditional Database):**\\n\\n```java\\n// Event processing with traditional disk-based database\\nlong startTime = System.nanoTime();\\n// 1. Check event cache (memory hit ~50\u03bcs, miss ~2ms disk fetch)\\nEventData event = cache.get(eventId);\\nif (event == null) {\\n    event = database.query(\\"SELECT * FROM events WHERE id = ?\\", eventId);  // Disk I/O: 2-10ms\\n    cache.put(eventId, event, 300);  // Cache update: ~100\u03bcs\\n}\\n// 2. Transaction processing (requires disk durability)\\ndatabase.executeTransaction(tx -> {  // WAL write + fsync: 5-15ms\\n    tx.execute(\\"INSERT INTO event_log VALUES (?, ?)\\", eventId, timestamp);\\n    tx.execute(\\"UPDATE event_counters SET count = count + 1\\");\\n});\\nlong totalTime = System.nanoTime() - startTime;\\n// Result: 7-25ms per event (dominated by disk I/O)\\n```\\n\\n**Compound Effect at Scale:**\\n\\n**Mathematical impossibility at scale:**\\n\\n- 1,000 events/sec \xd7 15ms avg = 15 seconds processing time needed per second\\n- 5,000 events/sec \xd7 15ms avg = 75 seconds processing time needed per second\\n- 10,000 events/sec \xd7 15ms avg = 150 seconds processing time needed per second\\n\\n**The constraint**: Disk I/O creates a performance ceiling on throughput regardless of CPU or memory.\\n\\n---\\n\\n## Memory-First Performance Results\\n\\n**Concrete performance improvement with Apache Ignite memory-first architecture:**\\n\\n```java\\n// Event processing with memory-first architecture\\nlong startTime = System.nanoTime();\\n// All operations happen in memory with microsecond access times\\ntry (IgniteClient client = IgniteClient.builder().addresses(\\"cluster:10800\\").build()) {\\n    client.transactions().runInTransaction(tx -> {\\n        // 1. Event data access (memory-based operations)\\n        EventData event = eventsTable.get(tx, eventId);\\n        // 2. Transaction processing (memory-based with async durability)\\n        client.sql().execute(tx, \\"INSERT INTO event_log VALUES (?, ?)\\", eventId, timestamp);\\n        client.sql().execute(tx, \\"UPDATE event_counters SET count = count + 1\\");\\n        // Transaction commits immediately to memory\\n        // Disk persistence happens asynchronously in background\\n    });\\n}\\nlong totalTime = System.nanoTime() - startTime;\\n// Result: ~200-500 microseconds per event (20x+ faster than disk-based)\\n```\\n\\n**Real-world performance characteristics:**\\n\\n- **10,000 events/sec processing**: 0.5 seconds total vs 150 seconds with disk I/O\\n- **Peak throughput**: 50,000+ events per sec achievable vs 1,000 events per sec disk limit\\n- **Consistent performance**: Sub-millisecond response times even during traffic spikes\\n- **Resource utilization**: Memory bandwidth becomes the scaling factor, not disk I/O waits\\n\\n---\\n\\n## Architecture Comparison: Disk-First vs Memory-First\\n\\n```mermaid\\nflowchart LR\\n    subgraph \\"Disk-First Architecture\\"\\n        App1[Application]\\n        Cache1[Memory Cache<br/>Limited Size]\\n        DB1[(Disk Database<br/>Primary Storage)]\\n        App1 --\x3e|1 - Check Cache| Cache1\\n        Cache1 --\x3e|2 - Cache Miss<br/>2-10ms| DB1\\n        DB1 --\x3e|3 - Disk Read<br/>5-15ms| Cache1\\n        Cache1 --\x3e|4 - Return Data| App1\\n        App1 --\x3e|5 - Write Operation| DB1\\n        DB1 --\x3e|6 - WAL + fsync<br/>5-15ms| Storage1[Disk Storage]\\n        DB1 --\x3e|7 - Invalidate| Cache1\\n    end\\n```\\n\\n```mermaid\\nflowchart LR\\n    subgraph \\"Memory-First Architecture\\"\\n        App2[Application]\\n        Memory2[Memory Storage<br/>Primary Tier]\\n        Async2[Async Persistence<br/>Background Process]\\n        App2 --\x3e|1 - All Operations<br/>Memory Speed| Memory2\\n        Memory2 --\x3e|2 - Immediate Response<br/>sub-1ms| App2\\n        Memory2 -.->|3 - Background<br/>Async Write| Async2\\n        Async2 -.->|4 - Durability<br/>No Blocking| Storage2[Disk Storage]\\n    end\\n```\\n\\n**The Fundamental Difference:**\\n\\n- **Traditional**: Memory serves disk (cache-aside pattern with cache misses)\\n- **Memory-First**: Disk serves memory (async persistence without blocking)\\n- **Performance Impact**: 5-15ms disk waits become sub-millisecond memory operations\\n- **Scalability**: Memory bandwidth scales linearly vs disk I/O bottlenecks\\n\\n---\\n\\n## Memory-First Architecture Principles\\n\\n### Off-Heap Memory Management\\n\\nApache Ignite manages memory regions directly outside the JVM heap to eliminate garbage collection interference.\\n\\n**Performance Benefits:**\\n\\n- **Predictable Access Times**: No Java GC pauses during event processing bursts\\n- **Large Memory Utilization**: Event data can consume large amounts of RAM without heap issues\\n- **Direct Memory Operations**: Reduced serialization/deserialization overhead\\n\\n### Dual Engine Strategy for Event Requirements\\n\\nApache Ignite provides two storage engines optimized for different performance requirements:\\n\\n#### Memory-Only Storage (aimem)\\n\\n- **Purpose**: Session data, real-time analytics, temporary processing results\\n- **Performance**: Memory-speed operations without disk I/O overhead\\n- **Trade-off**: Maximum speed in exchange for volatility\\n\\n#### Memory-First Persistence (aipersist)\\n\\n- **Purpose**: Financial transactions, audit logs, business-critical events\\n- **Performance**: Memory-speed access with asynchronous persistence\\n- **Trade-off**: Near-memory speed with full durability protection\\n\\n**The Evolution Solution**: Instead of choosing between fast caches and durable databases, you get both performance characteristics in the same platform based on your specific data requirements.\\n\\n---\\n\\n## Event Processing Performance Characteristics\\n\\n### Memory-First Operations\\n\\nEvent processing benefits from memory-first operations that reduce traditional I/O bottlenecks:\\n\\n**Architecture Benefits**:\\n\\n- Events stored in off-heap memory regions for fast access\\n- Multi-version storage enables concurrent read/write operations\\n- Asynchronous checkpointing maintains durability without blocking processing\\n- B+ tree structures optimize both sequential and random access patterns\\n\\n**Performance Advantage**: Event data processing operates on memory-resident data with minimal serialization overhead.\\n\\n### Asynchronous Persistence for Event Durability\\n\\nThe checkpoint manager ensures event durability without blocking event processing.\\n\\n#### Background Checkpoint Process\\n\\n- **Collection Phase**: Identify modified pages during low-activity periods\\n- **Write Phase**: Persist changes to storage without blocking ongoing operations\\n- **Coordination**: Manage recovery markers for failure scenarios\\n\\n**Key Advantage**: Event processing continues at memory speeds while persistence happens in background threads.\\n\\n---\\n\\n## B+ Tree Organization for Event Data\\n\\nEvent-Optimized Data Structures\\n\\nApache Ignite organizes event data through specialized B+ tree variations optimized for time-series and event-driven access patterns:\\n\\n**Event Processing Optimizations**:\\n\\n- Time-based ordering for streaming access patterns\\n- Range scan optimization for time window queries\\n- Cache-friendly layout for sequential event processing\\n- Multi-version support for consistent read operations\\n\\n### MVCC Integration for Event Consistency\\n\\nEvent processing maintains consistency through multi-version concurrency control:\\n\\n**Event Processing Benefits**:\\n\\n- **Consistent Analytics**: Read events at specific points in time without blocking new events\\n- **High-Frequency Writes**: Events process concurrently with analytical queries\\n- **Recovery Guarantees**: Event ordering maintained across failures\\n\\n---\\n\\n## Performance Characteristics at Event Scale\\n\\n### Memory-First Performance Profile\\n\\n**Event Processing Characteristics**:\\n\\n- **Write Operations**: Events commit to memory efficiently\\n- **Read Operations**: Event queries complete quickly from memory\\n- **Range Scans**: Time-window analytics benefit from memory-resident data\\n- **Concurrent Processing**: Memory-first design supports mixed read/write loads\\n\\n**Scaling Characteristics**:\\n\\n- **Linear Memory Scaling**: Performance grows with available memory\\n- **CPU Utilization**: Event processing can saturate multiple cores\\n- **Network Optimization**: Collocated processing eliminates network bottlenecks\\n\\n### Real-World Event Processing Examples\\n\\n**Real-World Performance Impact:**\\n\\n**Financial Trading Platforms**: High-frequency trades process at memory speeds instead of waiting for disk writes. Portfolio updates, risk calculations, and compliance checks happen concurrently without I/O bottlenecks.\\n\\n**IoT Event Processing**: Sensor data ingestion scales to device-native rates without sampling or queuing delays. Anomaly detection runs on live data streams rather than batch-processed snapshots.\\n\\n**Gaming Backends**: Player actions process immediately while leaderboards, achievements, and session state update concurrently. No delays between action and world state changes.\\n\\n---\\n\\n## Foundation for High-Velocity Applications\\n\\nMemory-first architecture creates the performance foundation that makes high-velocity event processing practical:\\n\\n**Eliminates Traditional Bottlenecks**:\\n\\n- Disk I/O wait times removed from event processing path\\n- Garbage collection interference eliminated through off-heap design\\n- Network serialization overhead reduced through efficient memory management\\n\\n**Enables New Application Patterns**:\\n\\n- Real-time analytics on live transactional event streams\\n- Sub-millisecond response capabilities for high-frequency processing\\n- IoT processing at sensor data rates without data sampling\\n\\n**Maintains Enterprise Requirements**:\\n\\n- ACID transaction guarantees for critical events\\n- Durability through asynchronous checkpointing\\n- Recovery capabilities for event stream continuity\\n\\nThe memory-first foundation transforms what\'s possible for high-velocity applications. Instead of architecting around disk I/O constraints, you can design for the performance characteristics your business requirements actually need.\\n\\n---\\n\\n_Next: Part 3 explores how flexible schema management lets systems evolve without downtime or complex coordination, and why these capabilities are essential for high-velocity applications that cannot afford processing interruptions._"},{"id":"/2025/11/25/ignite3-architecture-p1","metadata":{"permalink":"/suggested-site/blog/2025/11/25/ignite3-architecture-p1","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-11-25-ignite3-architecture-p1.md","source":"@site/blog/2025-11-25-ignite3-architecture-p1.md","title":"When Multi-System Complexity Compounds at Scale","description":"Your high-velocity application began with smart architectural choices: PostgreSQL for reliable transactions, Redis for fast cache access, and custom processing for domain-specific logic. These decisions powered early success and growth.","date":"2025-11-26T00:00:00.000Z","tags":[{"inline":true,"label":"architecture","permalink":"/suggested-site/blog/tags/architecture"},{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":7.45,"hasTruncateMarker":true,"authors":[{"name":"Michael Aglietti","title":"Apache Ignite Contributor and DevRel","url":"https://github.com/maglietti","imageURL":"/suggested-site/img/authors/maglietti.png","key":"maglietti","page":null}],"frontMatter":{"title":"When Multi-System Complexity Compounds at Scale","authors":["maglietti"],"date":"2025-11-26T00:00:00.000Z","tags":["architecture","technical","ignite","ignite3"]},"unlisted":false,"prevItem":{"title":"Memory-First Architecture: The Foundation for High-Velocity Event Processing","permalink":"/suggested-site/blog/2025/12/02/ignite3-architecture-p2"},"nextItem":{"title":"Schema Design for Distributed Systems: Why Data Placement Matters","permalink":"/suggested-site/blog/2025/11/18/schema-design-for-distributed-systems-ai3"}},"content":"Your high-velocity application began with smart architectural choices: PostgreSQL for reliable transactions, Redis for fast cache access, and custom processing for domain-specific logic. These decisions powered early success and growth.\\n\\n\x3c!-- truncate --\x3e\\n\\nBut success changes the game. Your system now handles thousands of events per second, and customers expect microsecond-level response times. The same architectural choices that enabled growth now create performance bottlenecks that compound with every additional event.\\n\\n**At high event volumes, data movement between systems becomes the primary performance constraint.**\\n\\n_Part 1 of 8 in the Apache Ignite 3 Architecture Series_\\n\\n---\\n\\n## The Scale Reality for High-Velocity Applications\\n\\nAs event volume grows, architectural compromises that once seemed reasonable at lower scale become critical bottlenecks. Consider a financial trading platform, gaming backend, or IoT processor handling tens of thousands of operations per second.\\n\\n### Event Processing Under Pressure\\n\\n**High-frequency event characteristics**\\n\\n- Events arrive faster than traditional batch processing can handle  \\n- Each event requires immediate consistency checks against live data  \\n- Results must update multiple downstream systems simultaneously  \\n- Network delays compound into user-visible lag  \\n- **Traffic spikes create systemic pressure** \u2014 traditional stacks drop connections or crash when overwhelmed  \\n\\n**The compounding effect**\\n\\n- At 100 events per second, network latency of 2ms adds minimal overhead. \\n- At 10,000 events per second, that same 2ms latency creates a 20-second processing backlog within system boundaries. \\n- During traffic spikes (50,000+ events/second), traditional systems collapse entirely, dropping connections and losing data when they\'re needed most. \\n\\nThe math scales against you.\\n\\n---\\n\\n### When Smart Choices Become Scaling Limits\\n\\n**Initial Architecture, works great at lower scale:**\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"Event Processing\\"\\n        Events[High-Volume Events<br/>10,000/sec]\\n    end\\n    \\n    subgraph \\"Multi-System Architecture\\"\\n        Redis[(Cache<br/>Session Data<br/>2ms latency)]\\n        PostgreSQL[(Relational DB<br/>Transaction Data<br/>5ms latency)]\\n        Processing[Custom Processing<br/>Business Logic<br/>3ms latency]\\n    end\\n    \\n    Events --\x3e Redis\\n    Events --\x3e PostgreSQL\\n    Events --\x3e Processing\\n    \\n    Redis <--\x3e|Sync Overhead| PostgreSQL\\n    PostgreSQL <--\x3e|Data Movement| Processing\\n    Processing <--\x3e|Cache Updates| Redis\\n```\\n\\n**What happens at scale**\\n\\n- **Network Latency Tax**: Every system hop adds milliseconds that compound\\n- **Synchronization Delays**: Keeping systems consistent creates processing queues\\n- **Memory Overhead**: Each system caches the same data in different formats\\n- **Consistency Windows**: Brief periods where systems show different data states \\n\\n---\\n\\n### The Hidden Cost of Multi-System Success\\n\\n**Data Movement Overhead:**\\n\\nYour events don\'t just need processing, they need processing that maintains consistency across all systems. \\n\\nEach event triggers:\\n\\n1. **Cache lookup** (cache): \u2248 0.5 ms network + processing\\n2. **Transaction validation** (relational db): \u2248 2 ms network + disk I/O  \\n3. **Business-logic execution** (custom logic): \u2248 1ms processing + data marshalling \\n4. **Result synchronization** (across systems): \u2248 3 ms coordination overhead\\n\\n**Minimum per-event cost \u2248 7 ms before business logic.** \\n\\nAt 10,000 events/s, you\u2019d need 70 seconds of processing capacity just *for data movement* per real-time second!\\n\\n---\\n\\n## The Performance Gap That Grows With Success\\n\\n### Why Traditional Options Fail\\n\\n**Option 1: Scale Out Each System**\\n\\n- **Strategy**: Add cache clusters, database replicas, processing nodes\\n- **Result**: More systems to coordinate, exponentially more complexity\\n- **Reality**: Network overhead grows faster than processing capacity\\n\\n**Option 2: Custom Optimization**  \\n\\n- **Strategy**: Build application-layer caching, custom consistency protocols\\n- **Result**: Engineering team maintains complex, system-specific optimizations\\n- **Reality**: Solutions don\'t generalize; each optimization creates technical debt\\n\\n**Option 3: Accept Performance Compromises**\\n\\n- **Strategy**: Use async processing, eventual consistency,  and accept delayed insights\\n- **Result**: Business requirements compromised to fit architectural limitations\\n- **Reality**: Competitive disadvantage as customer expectations grow\\n\\n---\\n\\n### The Critical Performance Gap\\n\\n| Component | Optimized for | Typical Latency |\\n|------------|---------------|----------------|\\n| Database | ACID transactions | Milliseconds |\\n| Cache | Access speed | Microseconds |\\n| Compute | Throughput | Minutes \u2013 hours |\\n\\nApplications needing *microsecond insights* on *millisecond transactions* have no good options at scale in traditional architectures.\\n\\nDuring traffic spikes, traditional architectures either drop connections (data loss) or degrade performance (missed SLAs). High-velocity applications need intelligent flow control that guarantees stability under pressure while preserving data integrity.\\n\\n---\\n\\n## Event Processing at Scale\\n\\n**Here\'s what traditional multi-system event processing costs:**\\n\\n```java\\n// Traditional multi-system event processing\\nlong startTime = System.nanoTime();\\n\\n// 1. Cache lookup for session data\\nString sessionData = redisClient.get(\\"session:\\" + eventId);  // ~500\u03bcs network\\nif (sessionData == null) {\\n    sessionData = postgresDB.query(\\"SELECT * FROM sessions WHERE id = ?\\", eventId);  // ~2ms fallback\\n    redisClient.setex(\\"session:\\" + eventId, 300, sessionData);  // ~300\u03bcs cache update\\n}\\n\\n// 2. Transaction processing\\npostgresDB.executeTransaction(tx -> {  // ~2-5ms transaction\\n    tx.execute(\\"INSERT INTO events VALUES (?, ?, ?)\\", eventId, userId, eventData);\\n    tx.execute(\\"UPDATE user_stats SET event_count = event_count + 1 WHERE user_id = ?\\", userId);\\n});\\n\\n// 3. Custom processing with consistency coordination\\nProcessingResult result = customProcessor.process(eventData, sessionData);  // ~1ms processing\\nredisClient.setex(\\"result:\\" + eventId, 600, result);  // ~300\u03bcs result caching\\n\\n// 4. Synchronization across systems\\nensureConsistency(eventId, sessionData, result);  // ~2-3ms coordination\\n\\nlong totalTime = System.nanoTime() - startTime;\\n// Total: 6-12ms per event (not including queuing delays)\\n```\\n\\n**Compound Effect at Scale:**\\n\\n| Rate            | Required processing time/s |\\n| --------------- | -------------------------- |\\n| 1,000 events/s  | 6\u201312 seconds               |\\n| 5,000 events/s  | 30\u201360 seconds              |\\n| 10,000 events/s | 60\u2013120 seconds             |\\n\\n**The math doesn\u2019t work:** parallelism helps, but coordination overhead grows exponentially with system count.\\n\\n---\\n\\n### Real-World Breaking Points\\n\\n- **Financial Services**: Trading platforms hitting 10,000+ trades/second discover that compliance reporting delays impact trading decisions.\\n\\n- **Gaming Platforms**: Multiplayer backends processing user actions find that leaderboard updates lag behind gameplay events.\\n\\n- **IoT Analytics**: Manufacturing systems processing sensor data realize that anomaly detection arrives too late for preventive action.\\n\\n---\\n\\n## The Apache Ignite Alternative\\n\\n### Eliminating Multi-System Overhead\\n\\n```mermaid\\nflowchart TB\\n    subgraph \\"Event Processing\\"\\n        Events[High-Volume Events<br/>10,000/sec]\\n    end\\n    \\n    subgraph \\"Apache Ignite Platform\\"\\n        subgraph \\"Collocated Processing\\"\\n            Memory[Memory-First Storage<br/>Optimized Access Times]\\n            Transactions[MVCC Transactions<br/>ACID Guarantees]\\n            Compute[Event Processing<br/>Where Data Lives]\\n        end\\n    end\\n    \\n    Events --\x3e Memory\\n    Memory --\x3e Transactions\\n    Transactions --\x3e Compute\\n    \\n    Memory <--\x3e|Minimal Copying| Transactions\\n    Transactions <--\x3e|Collocated| Compute\\n    Compute <--\x3e|Direct Access| Memory\\n```\\n\\n**Key difference:** events process *where the data lives*, eliminating inter-system network latency.\\n\\n---\\n\\n### Apache Ignite 3 Performance Reality Check\\n\\n**Here\'s the same event processing with integrated architecture:**\\n\\n```java\\n// Apache Ignite 3 integrated event processing\\ntry (IgniteClient client = IgniteClient.builder().addresses(\\"cluster:10800\\").build()) {\\n    // Single integrated transaction spanning cache, database, and compute\\n    client.transactions().runInTransaction(tx -> {\\n        // 1. Access session data (in memory, no network overhead)\\n        Session session = client.tables().table(\\"sessions\\")\\n            .keyValueView().get(tx, Tuple.create().set(\\"id\\", eventId));\\n        \\n        // 2. Process event with ACID guarantees (same memory space)\\n        client.sql().execute(tx, \\"INSERT INTO events VALUES (?, ?, ?)\\", \\n                           eventId, userId, eventData);\\n        \\n        // 3. Execute processing collocated with data\\n        ProcessingResult result = client.compute().execute(\\n            JobTarget.colocated(\\"events\\", Tuple.create().set(\\"id\\", eventId)), \\n            EventProcessor.class, eventData);\\n        \\n        // 4. Update derived data (same transaction, guaranteed consistency)\\n        client.sql().execute(tx, \\"UPDATE user_stats SET event_count = event_count + 1 WHERE user_id = ?\\", userId);\\n        \\n        return result;\\n    });\\n}\\n// Result: microsecond-range event processing through integrated architecture\\n```\\n\\n**Result**: Processing 10,000 events/s is achievable with integrated architecture eliminating network overhead.\\n\\n---\\n\\n### The Unified Data-Access Advantage\\n\\n**Here\'s what eliminates the need for separate systems:**\\n\\n```java\\n// The SAME data, THREE access paradigms, ONE system\\nTable customerTable = client.tables().table(\\"customers\\");\\n\\n// 1. Key-value access for cache-like performance\\nCustomer customer = customerTable.keyValueView()\\n    .get(tx, Tuple.create().set(\\"customer_id\\", customerId));\\n\\n// 2. SQL access for complex analytics\\nResultSet<SqlRow> analytics = client.sql().execute(tx, \\n    \\"SELECT segment, AVG(order_value) FROM customers WHERE region = ?\\", region);\\n\\n// 3. Record access for type-safe operations\\nCustomerRecord record = customerTable.recordView()\\n    .get(tx, new CustomerRecord(customerId));\\n\\n// All three: same schema, same data, same transaction model\\n```\\n\\n**Eliminates:**\\n\\n- **Cache API** for cache operations\\n- Data movement during **distributed joins** for analytical queries  \\n- **Custom mapping logic** for type-safe access\\n- **Data synchronization** between cache and database\\n- **Schema drift risks** across different systems \\n\\n**Unified advantage:** one schema, one transaction model, multiple access paths.\\n\\n---\\n\\n## Apache Ignite Architecture Preview\\n\\nThe ability to handle high-velocity events without multi-system overhead requires specific technical innovations:\\n\\n- **Memory-First Storage**: Event data lives in memory with optimized access times typically under 10 microseconds for cache-resident data\\n- **Collocated Compute**: Processing happens where data already exists, eliminating movement\\n- **Integrated Transactions**: ACID guarantees span cache, database, and compute operations  \\n- **Minimal Data Copying**: Events process against live data through collocated processing and direct memory access\\n\\nThese innovations address the compound effects that make multi-system architectures unsuitable for high-velocity applications.\\n\\n---\\n\\n## Business Impact of Architectural Evolution\\n\\n### Cost Efficiency\\n\\n- **Reduced infrastructure:** one platform instead of several  \\n- **Lower network costs:** eliminate inter-system bandwidth overhead  \\n- **Simplified operations:** fewer platforms to monitor, backup, and scale  \\n\\n### Performance Gains\\n\\n- **Millisecond latency:** eliminates network overhead  \\n- **Higher throughput:** more events on existing hardware  \\n- **Predictable scaling:** consistent performance under load  \\n\\n### Developer Experience\\n\\n- **Single API:** one model for all data operations  \\n- **Consistent behavior:** no synchronization anomalies  \\n- **Faster delivery:** one integrated system to test and debug  \\n\\n---\\n\\n## The Architectural Evolution Decision\\n\\nEvery successful application reaches this point: the architecture that once fueled growth now constrains it.  \\n\\n**The question isn\'t whether you\'ll hit multi-system scaling limits. It\'s how you\'ll evolve past them.**\\n\\n**Apache Ignite** consolidates transactions, caching, and compute into a single, memory-first platform designed for high-velocity workloads. Instead of managing the compound complexity of coordinating multiple systems at scale, you consolidate core operations into a platform designed for high-velocity applications.\\n\\nYour winning architecture doesn\'t have to become your scaling limit. It can evolve into the foundation for your next phase of growth.\\n\\n---\\n\\n_Next: Part 2 explores how Apache Ignite\u2019s memory-first architecture enables optimized event processing without compromising durability. This foundation makes true high-velocity performance possible._"},{"id":"/2025/11/18/schema-design-for-distributed-systems-ai3","metadata":{"permalink":"/suggested-site/blog/2025/11/18/schema-design-for-distributed-systems-ai3","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-11-18-schema-design-for-distributed-systems-ai3.md","source":"@site/blog/2025-11-18-schema-design-for-distributed-systems-ai3.md","title":"Schema Design for Distributed Systems: Why Data Placement Matters","description":"You can scale out your database, add more nodes, and tune every index, but if your data isn\u2019t in the right place, performance still hits a wall. Every distributed system eventually runs into this: joins that cross the network, caches that can\u2019t keep up, and queries that feel slower the larger your cluster gets.","date":"2025-11-19T00:00:00.000Z","tags":[{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":7.94,"hasTruncateMarker":true,"authors":[{"name":"Michael Aglietti","title":"Apache Ignite Contributor and DevRel","url":"https://github.com/maglietti","imageURL":"/suggested-site/img/authors/maglietti.png","key":"maglietti","page":null}],"frontMatter":{"title":"Schema Design for Distributed Systems: Why Data Placement Matters","authors":["maglietti"],"date":"2025-11-19T00:00:00.000Z","tags":["technical","ignite","ignite3"]},"unlisted":false,"prevItem":{"title":"When Multi-System Complexity Compounds at Scale","permalink":"/suggested-site/blog/2025/11/25/ignite3-architecture-p1"},"nextItem":{"title":"Getting to Know Apache Ignite 3","permalink":"/suggested-site/blog/2025/11/11/getting-to-know-ignite3"}},"content":"You can scale out your database, add more nodes, and tune every index, but if your data isn\u2019t in the right place, performance still hits a wall. Every distributed system eventually runs into this: joins that cross the network, caches that can\u2019t keep up, and queries that feel slower the larger your cluster gets.\\n\\n\x3c!-- truncate --\x3e\\n\\nMost distributed SQL databases claim to solve scalability. They partition data evenly, replicate it across nodes, and promise linear performance. But *how* data is distributed and *which* records end up together matters more than most people realize. If related data lands on different nodes, every query has to travel the network to fetch it, and each millisecond adds up.\\n\\nThat\u2019s where **data placement** becomes the real scaling strategy.  \\n\\nApache Ignite 3 takes a different path with **schema-driven colocation** \u2014 a way to keep related data physically together. Instead of spreading rows randomly across nodes, Ignite uses your schema relationships to decide where data lives. The result: a 200 ms cross-node query becomes a 5 ms local read.\\n\\n---\\n\\n## How Ignite 3 Differs from Other Distributed Databases\\n\\n**Traditional Distributed SQL Databases:**\\n\\n- Hash-based partitioning ignores data relationships  \\n- Related data scattered across nodes by default  \\n- Cross-node joins create network bottlenecks  \\n- Millisecond latencies due to disk-first architecture  \\n\\n**Ignite 3 Schema-Driven Approach:**\\n\\n- Colocation configuration in schema definitions  \\n- Related data automatically placed together  \\n- Local queries eliminate network overhead  \\n- Microsecond latencies through memory-first storage  \\n\\n---\\n\\n## The Distributed Data Placement Problem\\n\\nYou\u2019ve tuned indexes, optimized queries, and scaled your cluster\u2014but latency still creeps in.  \\nThe problem isn\u2019t your SQL -- it\u2019s where your data lives.  \\n\\nTraditional hash-based partitioning distributes records randomly across nodes based on primary key values.  \\nWhile this ensures even data distribution, it scatters related records that applications frequently access together.  \\nIt\u2019s a clever approach\u2014until you need to join data that doesn\u2019t share the same key.  \\nThen every query turns into a distributed operation, and your network becomes the bottleneck.\\n\\nIgnite 3 provides automatic colocation based on schema relationships.  \\nYou define relationships directly in your schema, and Ignite automatically places related data on the same nodes using the specified colocation keys.\\n\\nUsing a [music catalog example](https://github.com/lerocha/chinook-database/tree/master),  \\nwe\u2019ll demonstrate how schema-driven data placement reduces query latency from 200 ms to 5 ms.\\n\\n> This post assumes you have a basic understanding of how to get an Ignite 3 cluster running and have worked with the Ignite 3 Java API.  \\n> If you\'re new to Ignite 3, start with the [Java API quick start guide](/docs/ignite3/3.1.0/getting-started/key-value-api) to set up your development environment.\\n\\n---\\n\\n## How Ignite 3 Places Data Differently\\n\\nTables are distributed across multiple nodes using consistent hashing, but with a key difference:  \\nyour schema definitions control data placement.  \\nInstead of accepting random distribution of related records, you declare relationships in your schema and let Ignite handle placement automatically.\\n\\n**Partitioning Fundamentals:**\\n\\n- Each table is divided into partitions (typically 64\u20131024 per table)  \\n- Primary key hash determines which partition data goes into  \\n- Partitions are distributed evenly across available nodes  \\n- Each partition has configurable replicas for fault tolerance  \\n\\n**Data Placement Concepts:**\\n\\n- **Affinity** \u2013 the algorithm that determines which nodes store which partitions  \\n- **Colocation** \u2013 ensuring related data from different tables gets placed on the same nodes  \\n\\nThe diagram below shows how colocation works in practice.  \\nArtist and Album tables use different primary keys, but colocation strategy ensures albums are partitioned by `ArtistId` rather than `AlbumId`:\\n\\n```mermaid\\ngraph TB\\n    subgraph \\"3-Node Cluster\\"\\n        N1[\\"Node 1 Partitions: 0,3,6,9\\"]\\n        N2[\\"Node 2 Partitions: 1,4,7,10\\"] \\n        N3[\\"Node 3 Partitions: 2,5,8,11\\"]\\n    end\\n    \\n    subgraph \\"Album Table - colocateBy ArtistId\\"\\n        B1[\\"AlbumId=101, ArtistId=1 hash(1)\u2192P1 Node 2\\"]\\n        B2[\\"AlbumId=102, ArtistId=1 hash(1)\u2192P1 Node 2\\"]\\n        B3[\\"AlbumId=201, ArtistId=22 hash(22)\u2192P11 Node 3\\"]\\n        B4[\\"AlbumId=301, ArtistId=42 hash(42)\u2192P6 Node 1\\"]\\n    end\\n    \\n    subgraph \\"Artist Table\\"\\n        A1[\\"ArtistId=1 hash\u2192P1 Node 2\\"]\\n        A2[\\"ArtistId=22 hash\u2192P11 Node 3\\"] \\n        A3[\\"ArtistId=42 hash\u2192P6 Node 1\\"]\\n    end\\n    \\n    A1 -.->|\\"Same partition\\"| B1 -.-> N2\\n    A1 -.->|\\"Same partition\\"| B2 -.-> N2 \\n    A2 -.->|\\"Same partition\\"| B3 -.-> N3\\n    A3 -.->|\\"Same partition\\"| B4 -.-> N1\\n```\\n\\nColocation configuration in your schema ensures that Album records use the `ArtistId` value (not `AlbumId`) for partition assignment.  \\nThis guarantees that Artist 1 and all albums with `ArtistId = 1` hash to the same partition and therefore live on the same nodes.\\n\\n---\\n\\n## Distribution Zones and Data Placement\\n\\nDistribution zones are cluster-level configurations that define how data is distributed and replicated.\\n\\n> **Zone Creation Options:** Ignite 3 supports multiple approaches:  \\n> 1. **SQL DDL** \u2013 `CREATE ZONE` statements  \\n> 2. **Java Builder API** \u2013 programmatic `ZoneDefinition.builder()`  \\n>  \\n> We use the Java Builder API here for consistency with our programmatic schema examples.\\n\\nA distribution zone specifies:\\n\\n- **Partition count** \u2013 how many partitions your data is divided into (typically 64\u20131024 per table)  \\n- **Replica count** \u2013 how many copies of each partition exist for fault tolerance  \\n- **Node filters** \u2013 which nodes can store data for this zone  \\n\\nFirst, create the distribution zones:\\n\\n```java\\n// Create the standard zone for frequently updated data\\nignite.catalog().create(ZoneDefinition.builder(\\"MusicStore\\")\\n    .replicas(2)\\n    .storageProfiles(\\"default\\")\\n    .build()).execute();\\n\\n// Create the replicated zone for reference data (replicas = cluster size)\\nignite.catalog().create(ZoneDefinition.builder(\\"MusicStoreReplicated\\")\\n    .replicas(clusterNodes().size()) \\n    .storageProfiles(\\"default\\")\\n    .build()).execute();\\n```\\n\\n---\\n\\n## Building Your Music Platform Schema\\n\\n> **Schema Creation:** Ignite 3 supports three approaches:  \\n> 1. **SQL DDL** \u2013 traditional `CREATE TABLE` statements  \\n> 2. **Java Annotations API** \u2013 POJO markup with `@Table`, `@Column`, etc.  \\n> 3. **Java Builder API** \u2013 programmatic `TableDefinition.builder()`  \\n>  \\n> We use the Annotations API here for its clarity and type safety.\\n\\nThe Artist table establishes the partitioning strategy that dependent tables will follow through colocation:\\n\\n```java\\n@Table(zone = @Zone(value = \\"MusicStore\\", storageProfiles = \\"default\\"))\\npublic class Artist {\\n    @Id\\n    @Column(value = \\"ArtistId\\", nullable = false)\\n    private Integer ArtistId;\\n    \\n    @Column(value = \\"Name\\", nullable = false, length = 120)\\n    private String Name;\\n    \\n    public Artist() {}\\n    \\n    public Artist(Integer artistId, String name) {\\n        this.ArtistId = artistId;\\n        this.Name = name;\\n    }\\n    \\n    public Integer getArtistId() { return ArtistId; }\\n    public void setArtistId(Integer artistId) { this.ArtistId = artistId; }\\n    public String getName() { return Name; }\\n    public void setName(String name) { this.Name = name; }\\n}\\n```\\n\\n---\\n\\n## Parent\u2013Child Colocation Implementation\\n\\nWhen users search for \\"The Beatles\\", they expect both artist details and album listings in the same query.  \\nWithout colocation, this requires cross-node joins that can take 40\u2013200 ms.\\n\\nWe solve this by setting `colocateBy` in the `@Table` annotation:\\n\\n```java\\n@Table(\\n    zone = @Zone(value = \\"MusicStore\\", storageProfiles = \\"default\\"),\\n    colocateBy = @ColumnRef(\\"ArtistId\\")\\n)\\npublic class Album {\\n    @Id\\n    @Column(value = \\"AlbumId\\", nullable = false)\\n    private Integer AlbumId;\\n    \\n    @Id\\n    @Column(value = \\"ArtistId\\", nullable = false)\\n    private Integer ArtistId;\\n    \\n    @Column(value = \\"Title\\", nullable = false, length = 160)\\n    private String Title;\\n    \\n    @Column(value = \\"ReleaseDate\\", nullable = true)\\n    private LocalDate ReleaseDate;\\n    \\n    // Constructors and getters/setters...\\n}\\n```\\n\\nThe colocation field (`ArtistId`) must be part of the composite primary key.  \\nIgnite uses the `ArtistId` value to ensure albums with the same artist live on the same nodes as their corresponding artist record.\\n\\n---\\n\\n## Performance Impact: Memory-First + Colocation\\n\\nLet\u2019s quantify the effect of combining memory-first storage with schema-driven colocation.\\n\\n**Without Colocation \u2013 Data Scattered:**\\n\\n```java\\nArtist artist = artistView.get(null, artistKey);              // Node 2\\nCollection<Album> albums = albumView.getAll(null, albumKeys); // Nodes 1,2,3\\n// Result: 3 network operations for related data\\n// Query time: 40\u2013200 ms (network latency \xd7 nodes involved)\\n```\\n\\n**With Memory-First + Colocation \u2013 Data Local:**\\n\\n```java\\nArtist artist = artistView.get(null, artistKey);              // Node 2\\nCollection<Album> albums = albumView.getAll(null, albumKeys); // Node 2\\n// Result: 1 node involved, local memory access\\n// Query time: 1\u20135 ms (memory access + no network hops)\\n```\\n\\nThe performance difference combines **memory-first storage** with **schema-driven colocation**:\\n\\n- **Query latency reduction:** 200 ms \u2192 5 ms (memory access + no network hops)  \\n- **Network traffic elimination:** related data queries become local operations  \\n- **Resource efficiency:** CPU focuses on serving requests instead of moving data  \\n\\n---\\n\\n## Colocation Enables Compute-to-Data Processing\\n\\nSchema-driven colocation doesn\u2019t just optimize queries\u2014it enables processing where data lives:\\n\\n```java\\n// Process all albums for an artist locally\\nComputeJob<RecommendationResult> job = ComputeJob.colocated(\\"Artist\\", artistId,\\n    AlbumRecommendationJob.class);\\n    \\n// Runs on the same node where artist and album data live\\nCompletableFuture<RecommendationResult> result = ignite.compute()\\n    .submitAsync(job, preferences);\\n```\\n\\nInstead of moving gigabytes of album data to a compute cluster, you move kilobytes of logic to where the data already resides.\\n\\n---\\n\\n## Implementation Guide\\n\\nDeploy tables in dependency order to avoid colocation reference errors:\\n\\n```java\\ntry (IgniteClient client = IgniteClient.builder()\\n        .addresses(\\"127.0.0.1:10800\\")\\n        .build()) {\\n    \\n    // 1. Reference tables with no dependencies\\n    client.catalog().createTable(Genre.class);\\n    \\n    // 2. Root entities\\n    client.catalog().createTable(Artist.class);\\n    \\n    // 3. Dependent entities in hierarchy order\\n    client.catalog().createTable(Album.class);      // References Artist\\n    client.catalog().createTable(Track.class);      // References Album\\n}\\n```\\n\\n---\\n\\n## Accessing Your Distributed Data\\n\\nIgnite 3 provides multiple views of the same colocated data:\\n\\n```java\\n// RecordView for entity operations\\nRecordView<Artist> artists = client.tables()\\n    .table(\\"Artist\\")\\n    .recordView(Artist.class);\\n\\n// Operations with partition keys route to single nodes\\nArtist beatles = new Artist(1, \\"The Beatles\\");\\nartists.upsert(null, beatles);\\n\\nAlbum abbeyRoad = new Album(1, 1, \\"Abbey Road\\", LocalDate.of(1969, 9, 26));\\nalbums.upsert(null, abbeyRoad);  // Automatically colocated with artist\\n```\\n\\n---\\n\\n## Summary\\n\\nData placement is where distributed performance is won or lost.  \\nWith **schema-driven colocation**, Apache Ignite 3 keeps related data together on the same nodes, so your queries stay local, fast, and predictable.\\n\\nInstead of tuning around network latency, you design for it once at the schema level.  \\nYour joins stay local, your compute jobs run where the data lives, and scaling stops being a tradeoff between performance and size.\\n\\n**Why it works:**  \\n- **Memory-first + colocation** \u2192 microsecond access to related data  \\n- **Schema-driven placement** \u2192 predictable performance at scale  \\n- **Compute-to-data** \u2192 logic runs with data, not across the network  \\n- **Unified platform** \u2192 transactions, analytics, and compute together  \\n\\nWhen data lives together, your system scales naturally \u2014 without complexity creeping in.\\n\\nExplore the [Ignite 3 documentation](/docs/) for detailed examples and API references."},{"id":"/2025/11/11/getting-to-know-ignite3","metadata":{"permalink":"/suggested-site/blog/2025/11/11/getting-to-know-ignite3","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-11-11-getting-to-know-ignite3.md","source":"@site/blog/2025-11-11-getting-to-know-ignite3.md","title":"Getting to Know Apache Ignite 3","description":"Apache Ignite 3 is a memory-first distributed SQL database platform that consolidates transactions, analytics, and compute workloads previously requiring separate systems. Built from the ground up over four years of development, it represents a complete departure from traditional caching solutions toward a unified distributed computing platform with microsecond latencies and collocated processing capabilities.","date":"2025-11-12T00:00:00.000Z","tags":[{"inline":true,"label":"technical","permalink":"/suggested-site/blog/tags/technical"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"},{"inline":true,"label":"featured","permalink":"/suggested-site/blog/tags/featured"}],"readingTime":7.08,"hasTruncateMarker":true,"authors":[{"name":"Michael Aglietti","title":"Apache Ignite Contributor and DevRel","url":"https://github.com/maglietti","imageURL":"/suggested-site/img/authors/maglietti.png","key":"maglietti","page":null}],"frontMatter":{"title":"Getting to Know Apache Ignite 3","authors":["maglietti"],"date":"2025-11-12T00:00:00.000Z","tags":["technical","ignite","ignite3","featured"]},"unlisted":false,"prevItem":{"title":"Schema Design for Distributed Systems: Why Data Placement Matters","permalink":"/suggested-site/blog/2025/11/18/schema-design-for-distributed-systems-ai3"},"nextItem":{"title":"Apache Ignite 3.1: Performance, Multi-Language Client Support, and Production Hardening","permalink":"/suggested-site/blog/2025/11/03/apache-ignite-3-1"}},"content":"Apache Ignite 3 is a memory-first distributed SQL database platform that consolidates transactions, analytics, and compute workloads previously requiring separate systems. Built from the ground up over four years of development, it represents a complete departure from traditional caching solutions toward a unified distributed computing platform with microsecond latencies and collocated processing capabilities.\\n\\n\x3c!-- truncate --\x3e\\n\\n**Forget everything you knew about Apache Ignite.** Version 3.0 is a complete architectural rewrite that transforms Ignite from a caching platform into a memory-first distributed computing platform with microsecond latencies and collocated processing.\\n\\n## Architectural Foundation: Schema-Driven Design\\n\\nThe core architectural shift in Ignite 3 is that your schema becomes the foundation for data placement, query optimization, and compute job scheduling. Instead of managing separate systems with different data models, you define your schema once and it drives everything.\\n\\n```java\\n// Unified platform connection\\nIgniteClient ignite = IgniteClient.builder()\\n    .addresses(\\"node1:10800\\", \\"node2:10800\\", \\"node3:10800\\")\\n    .build();\\n```\\n\\nSchema definitions use Java annotations to specify colocation strategies, storage profiles, and indexing:\\n\\n> **Schema Creation**: Ignite 3 supports three approaches for schema creation:\\n> \\n> 1. **SQL DDL** - Traditional `CREATE TABLE` statements\\n> 2. **Java Annotations API** - POJO markup with `@Table`, `@Column`, etc.\\n> 3. **Java Builder API** - Programmatic `TableDefinition.builder()` approach\\n>\\n> We use the Java Annotations API in this blog for their compile-time type safety and clear colocation syntax.\\n\\n```java\\n@Table(zone = @Zone(value = \\"MusicStore\\", storageProfiles = \\"default\\"))\\npublic class Artist {\\n    @Id\\n    private Integer ArtistId;\\n\\n    @Column(value = \\"Name\\", length = 120, nullable = false)\\n    private String Name;\\n\\n    // Constructors, getters, setters...\\n}\\n\\n@Table(\\n    zone = @Zone(value = \\"MusicStore\\", storageProfiles = \\"default\\"),\\n    colocateBy = @ColumnRef(\\"ArtistId\\"),\\n    indexes = @Index(value = \\"IFK_AlbumArtistId\\", columns = { \\n        @ColumnRef(\\"ArtistId\\") })\\n)\\npublic class Album {\\n    @Id\\n    private Integer AlbumId;\\n\\n    @Id\\n    private Integer ArtistId;\\n\\n    @Column(value = \\"Title\\", length = 160, nullable = false)\\n    private String Title;\\n\\n    // Constructors, getters, setters...\\n}\\n```\\n\\nThe `colocateBy` annotation ensures that albums are stored on the same nodes as their corresponding artists, eliminating distributed join overhead and enabling local processing.\\n\\n## Multiple APIs, Single Schema\\n\\nIgnite 3 provides different API views into the same schema, eliminating impedance mismatch between operational and analytical workloads:\\n\\n```java\\n// RecordView for structured operations\\nRecordView<Artist> artists = ignite.tables()\\n    .table(\\"Artist\\")\\n    .recordView(Artist.class);\\n\\n// KeyValueView for high-performance access patterns\\nKeyValueView<Long, Album> albums = ignite.tables()\\n    .table(\\"Album\\")\\n    .keyValueView(Long.class, Album.class);\\n\\n// SQL for analytics using Apache Calcite engine\\nSqlStatement analytics = ignite.sql()\\n    .statementBuilder()\\n    .query(\\"SELECT a.Name, COUNT(al.AlbumId) as AlbumCount \\" +\\n            \\"FROM Artist a JOIN Album al ON a.ArtistId = al.ArtistId \\" +\\n            \\"GROUP BY a.Name\\");\\n\\n// Collocated compute jobs\\nComputeJob<String> job = ComputeJob.colocated(\\"Artist\\", 42,\\n    RecommendationJob.class);\\nJobExecution<String> recommendation = ignite.compute()\\n    .submit(ignite.clusterNodes(), job, \\"rock\\");\\n```\\n\\nThis approach eliminates the typical data serialization and movement overhead between different systems while maintaining type safety and schema evolution capabilities.\\n\\n> This represents a fundamental architectural shift from Ignite 2.x, which treated everything as key-value operations in a cache. Ignite 3 puts an evolvable schema first and uses memory-centric storage to deliver microsecond latencies for all operations, not just cache hits.\\n\\n## Memory-First Storage Architecture\\n\\nUnlike disk-first distributed databases, Ignite 3 uses a memory-first storage model with configurable persistence options:\\n\\n- **`aimem`**: Pure in-memory storage for maximum performance\\n- **`aipersist`**: Memory-first with persistence for durability\\n- **`RocksDB`**: Disk-based storage for write-heavy workloads\\n\\nThe memory-first approach delivers microsecond response times for hot data while providing flexible cost-performance trade-offs through configurable memory-to-disk ratios.\\n\\n### Storage Engine Characteristics\\n\\n|Engine|Primary Use Case|Latency Profile|Durability|\\n|---|---|---|---|\\n|aimem|Ultra-low latency|Microseconds|Volatile|\\n|aipersist|Balanced performance|Microseconds (memory)|Persistent|\\n|RocksDB|Write-heavy workloads|Variable|Persistent|\\n\\n## Consistency and Concurrency Model\\n\\nIgnite 3 implements Raft consensus for strong consistency and MVCC (Multi-Version Concurrency Control) for transaction isolation:\\n\\n- **Raft consensus**: Ensures data consistency across replicas without split-brain scenarios\\n- **MVCC transactions**: Provides snapshot isolation and deadlock-free concurrency\\n- **ACID compliance**: Full transactional guarantees across distributed operations\\n\\nThis consistency model applies uniformly across all APIs, whether you\'re using RecordView operations, SQL queries, or compute jobs.\\n\\n## Collocated Processing: Compute-to-Data Architecture\\n\\nOne of Ignite 3\'s key architectural advantages is collocated processing, which brings computation to where data is stored rather than moving data to compute resources:\\n\\n```java\\n// Traditional approach: data movement overhead\\n// 1. Query data from database\\n// 2. Move data to compute cluster  \\n// 3. Process data remotely\\n// 4. Return results\\n\\n// Ignite 3 approach: compute colocation\\nComputeJob<Result> job = ComputeJob.colocated(\\"Customer\\", customerId,\\n    RiskAnalysisJob.class);\\nCompletableFuture<Result> result = ignite.compute()\\n    .submitAsync(job, parameters);\\n```\\n\\nThis compute-to-data pattern eliminates network serialization overhead and enables processing of large datasets without data movement. Instead of moving terabytes of data to processing nodes, you move kilobytes of code to where the data lives.\\n\\n## System Consolidation Benefits\\n\\nTraditional distributed architectures typically require separate systems for different workloads:\\n\\n**Traditional Multi-System Architecture:**\\n\\n- Transactional database (PostgreSQL, MySQL) - millisecond latencies\\n- Analytics database (ClickHouse, Snowflake) - batch processing\\n- Caching layer (Redis, Hazelcast) - separate consistency model\\n- Compute cluster (Spark, Flink) - data movement overhead\\n- Message queue (Kafka, RabbitMQ) - separate operational model\\n- Stream processing (Kafka Streams, Pulsar) - additional complexity\\n\\n**Ignite 3 Unified Platform:**\\n\\n- Schema-driven storage with multiple storage engines - microsecond latencies\\n- SQL analytics through Apache Calcite - real-time processing\\n- Collocated compute processing - zero data movement\\n- Built-in streaming with flow control - integrated backpressure\\n- ACID transactions across all operations - single consistency model\\n- One operational model and consistency guarantee\\n\\n### Operational Advantages\\n\\n1. **Unified Schema Evolution**: Schema changes propagate automatically across all access patterns\\n2. **Single Consistency Model**: ACID guarantees across transactions, analytics, and compute\\n3. **Reduced Operational Complexity**: One system to monitor, tune, and scale\\n4. **Eliminated Data Movement**: Processing happens where data lives\\n5. **Cost-Elastic Scaling**: Adjust memory-to-disk ratios based on performance requirements\\n\\n## Streaming and Flow Control\\n\\nIgnite 3 includes built-in streaming capabilities with configurable backpressure mechanisms:\\n\\n```java\\n// Publisher with flow control configuration\\nStreamingOptions options = StreamingOptions.builder()\\n    .pageSize(1000)\\n    .autoFlushFrequency(Duration.ofMillis(100))\\n    .retryLimit(3)\\n    .build();\\n\\n// Handle millions of events with automatic backpressure\\nCompletableFuture<Void> streaming = ignite.sql()\\n    .streamAsync(\\"INSERT INTO events VALUES (?, ?, ?)\\", \\n                 eventStream, \\n                 options);\\n```\\n\\nThe streaming API provides automatic flow control through configurable page sizes, flush intervals, and retry policies, preventing system overload without data loss.\\n\\n## Performance Characteristics\\n\\nIgnite 3\'s memory-first architecture delivers significantly different performance characteristics compared to disk-based distributed databases:\\n\\n- **Latency**: Microsecond response times for memory-resident data vs. millisecond latencies for disk-based systems\\n- **Throughput**: Handles millions of operations per second per node\\n- **Scalability**: Linear scaling through data partitioning and colocation\\n- **Consistency**: ACID transactions with minimal overhead due to memory speeds\\n\\nThe 10-1000x performance improvement comes from eliminating disk I/O bottlenecks and data movement overhead through collocated processing.\\n\\n## Migration and Adoption Strategy\\n\\nFor technical teams considering Ignite 3:\\n\\n### Assessment Phase\\n\\n1. **Workload Analysis**: Identify performance-critical paths requiring microsecond latencies\\n2. **Data Model Mapping**: Design colocation strategies for your entities\\n3. **Integration Points**: Plan API migration from current multi-system architecture\\n4. **Performance Benchmarking**: Compare memory-first vs. disk-first performance for your workloads\\n\\n### Implementation Approach\\n\\n1. **Start with New Features**: Use Ignite 3 for new development requiring low latency\\n2. **Gradual Migration**: Move performance-critical workloads first\\n3. **Schema Design**: Leverage colocation for optimal data locality\\n4. **Operational Integration**: Integrate monitoring and deployment pipelines\\n\\n## Technical Considerations\\n\\n### Schema Design Best Practices\\n\\n- Use `colocateBy` annotations to ensure related data stays together\\n- Design partition keys to distribute load evenly across nodes\\n- Consider query patterns when defining indexes and colocation strategies\\n- Plan for schema evolution with backward-compatible changes\\n\\n### Performance Optimization\\n\\n- Size memory regions appropriately for your working set\\n- Use collocated compute jobs to minimize data movement\\n- Leverage appropriate storage engines for different workload patterns\\n- Monitor memory usage and adjust disk ratios as needed\\n\\n### Operational Requirements\\n\\n- Plan for Raft consensus network requirements (low-latency, reliable connectivity)\\n- Design backup and recovery procedures for persistent storage engines\\n- Implement monitoring for memory usage, query performance, and compute job execution\\n- Establish capacity planning procedures for memory-first architecture\\n\\n## Summary\\n\\nApache Ignite 3 represents a schema-driven distributed computing platform that consolidates transaction processing, analytics, and compute workloads into a single memory-first architecture. Key architectural elements include:\\n\\n- **Schema-driven design**: Single schema definition drives data placement, query optimization, and compute colocation\\n- **Memory-first storage**: Multiple storage engines with microsecond latency characteristics\\n- **Collocated processing**: Compute-to-data architecture that eliminates data movement overhead\\n- **Unified APIs**: Multiple access patterns (RecordView, KeyValueView, SQL, Compute) for the same schema\\n- **ACID consistency**: Raft consensus and MVCC transactions across all operations\\n- **Built-in streaming**: Flow control and backpressure mechanisms for high-velocity data ingestion\\n\\nThe platform addresses scenarios where traditional multi-system architectures create operational complexity and performance bottlenecks through data movement between separate databases, compute clusters, and analytics systems.\\n\\nExplore the [Ignite 3 documentation](/docs/) for detailed implementation guides and API references."},{"id":"/2025/11/03/apache-ignite-3-1","metadata":{"permalink":"/suggested-site/blog/2025/11/03/apache-ignite-3-1","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-11-03-apache-ignite-3-1.mdx","source":"@site/blog/2025-11-03-apache-ignite-3-1.mdx","title":"Apache Ignite 3.1: Performance, Multi-Language Client Support, and Production Hardening","description":"Apache Ignite 3.1 improves the three areas that matter most when running distributed systems: performance at scale, language flexibility, and operational visibility. The release also fixes hundreds of bugs related to data corruption, race conditions, and edge cases discovered since 3.0.","date":"2025-11-03T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"release","permalink":"/suggested-site/blog/tags/release"}],"readingTime":8.34,"hasTruncateMarker":true,"authors":[{"name":"Evgeniy Stanilovskiy","title":"Apache Ignite Committer","url":"https://github.com/zstan","imageURL":"/suggested-site/img/authors/zstan.png","key":"evgeniy","page":null}],"frontMatter":{"title":"Apache Ignite 3.1: Performance, Multi-Language Client Support, and Production Hardening","authors":["evgeniy"],"date":"2025-11-03T00:00:00.000Z","tags":["apache","ignite","release"]},"unlisted":false,"prevItem":{"title":"Getting to Know Apache Ignite 3","permalink":"/suggested-site/blog/2025/11/11/getting-to-know-ignite3"},"nextItem":{"title":"What\'s New in Apache Ignite 3.0","permalink":"/suggested-site/blog/2025/02/24/whats-new-in-apache-ignite-3-0"}},"content":"Apache Ignite 3.1 improves the three areas that matter most when running distributed systems: performance at scale, language flexibility, and operational visibility. The release also fixes hundreds of bugs related to data corruption, race conditions, and edge cases discovered since 3.0.\\n\\n\x3c!--truncate--\x3e\\n\\n**Before you upgrade**: This release introduces zone-based replication, which changes how RAFT groups are allocated. Persistent storage in upgraded 3.0 clusters will continue to use table-based replication. Read the [Zone-Based Replication Migration](#zone-based-replication-migration) section to understand your options.\\n\\n### What This Release Delivers\\n\\n- **Improved replication performance and reliability**: Zone-based replication improves data colocation, reduces required thread count and memory overhead while naturally improving SQL performance.\\n- **Extended clients support**: Python DB API driver, .NET distributed computing with ADO.NET integration, and enhanced C++ client support expand your options for working with data.\\n- **Better cluster observability**: 50+ new metrics covering checkpoints, SQL, transactions, and storage. New system views for cluster introspection.\\n- **Improved APIs**: Multiple schema support, improved query planning controls, and streamlined Java APIs.\\n\\n### Performance: Built for Scale\\n\\n#### Zone-Based Replication Reduces Overhead\\n\\nApache Ignite 3.1 replaces the table-based replication model from 3.0 with zone-based replication.\\n\\n**Updated defaults**: Zone-based replication is enabled by default for new 3.1 clusters.\\n\\n**Existing clusters**: Clusters with persistent storage upgraded from 3.0 will continue to use table-based replication. To adopt zone-based replication and gain the performance benefits, you must migrate to a new 3.1 cluster. Table-based replication support will be discontinued in Ignite 3.2 release.\\n\\n#### DDL Operations Now Batch Automatically\\n\\nCreating multiple tables no longer requires multiple round-trips. DDL operations batch automatically when possible, reducing setup time during schema initialization and testing.\\n\\n#### Partition Pruning and Partition Awareness\\n\\nApache Ignite 3.1 introduces two other major SQL optimizations that dramatically improve query performance:\\n\\n**Partition Pruning**: The query optimizer automatically eliminates unnecessary partition scans based on predicates. Queries with key-based filters only scan relevant partitions instead of the entire dataset.\\n\\n**Partition Awareness**: Client queries route directly to nodes owning the data, eliminating coordinator hops. The client determines the exact target node for single-partition queries.\\n\\nUse the new `EXPLAIN MAPPING` statement to verify query routing:\\n\\n```sql\\nEXPLAIN MAPPING FOR SELECT * FROM orders WHERE order_id = 12345;\\n```\\n\\nCombined impact: Queries with partition key predicates can see 10x+ performance improvements.\\n\\n### Multi-Language Support\\n\\n#### Python: PEP 249-Compliant Database Driver\\n\\nConnect to Apache Ignite from Python with a standard DB API 2.0 compliant driver. SSL support and macOS compatibility are built in.\\n\\nYou can install the driver from pip:\\n\\n```shell\\npip install pyignite-dbapi\\n```\\n\\nThen, you can import it into your application and initialize the connection:\\n\\n```python\\n# Import and use the driver\\nimport pyignite_dbapi\\n\\nconn = pyignite_dbapi.connect(address=\'localhost:10800\', use_ssl=True)\\ncursor = conn.cursor()\\ncursor.execute(\'SELECT * FROM orders WHERE customer_id = ?\', (customer_id,))\\n```\\n\\n#### .NET: Distributed Computing and ADO.NET\\n\\nWrite compute jobs in C#, F#, or any .NET language and distribute them across your cluster:\\n\\n```csharp\\npublic class HelloJob : IComputeJob<string, string>\\n{\\n    public ValueTask<string> ExecuteAsync(IJobExecutionContext context, string arg, CancellationToken cancellationToken) =>\\n        ValueTask.FromResult(\\"Hello \\" + arg);\\n}\\n\\nvar jobDesc = new JobDescriptor<string, string>(\\n    JobClassName: typeof(HelloJob).AssemblyQualifiedName!,\\n    DeploymentUnits: [new DeploymentUnit(\\"unit1\\")]);\\nvar jobTarget = JobTarget.AnyNode(await client.GetClusterNodesAsync());\\nvar jobExec = await client.Compute.SubmitAsync(jobTarget, jobDesc, \\"world\\");\\n```\\n\\nADO.NET integration brings familiar patterns to .NET developers:\\n\\n```csharp\\nvar connStr = \\"Endpoints=localhost:10800\\";\\nawait using var conn = new IgniteDbConnection(connStr);\\nawait conn.OpenAsync();\\nDbCommand cmd = conn.CreateCommand();\\ncmd.CommandText = \\"DROP TABLE IF EXISTS Person\\";\\nawait cmd.ExecuteNonQueryAsync();\\n```\\n\\nAdditional .NET features in 3.1:\\n\\n- **Platform Streamer Receiver**: Custom data processing during streaming operations\\n- **Batch SQL Execution**: `ISql.ExecuteBatchAsync` for efficient multi-statement execution\\n- **CancellationToken Support**: Integrated cancellation for SQL and Compute APIs\\n\\nSee the [extended blog post about .NET compute in Ignite 3.1](https://ptupitsyn.github.io/NET-Compute-In-Apache-Ignite-3-1/) for a more in-depth explanation.\\n\\n#### C++ Client\\n\\nUse the improved C++ client to improve you application with several new production-ready features:\\n\\n- **Heartbeat Support**: Connection health monitoring prevents timeout disconnects\\n- **Transaction Timeouts**: Configurable timeout settings for transaction operations\\n- **Query Cancellation**: An option to cancel long-running queries\\n\\n#### ODBC Driver SSL Support\\n\\nUse the newly added support for **SSL/TLS** to enable secure connections to the cluster from your ODBC applications.\\n\\n### SQL Capabilities\\n\\n#### Multiple Schemas\\n\\nOrganize tables across multiple schemas instead of using only PUBLIC:\\n\\n```sql\\nCREATE SCHEMA analytics;\\nCREATE TABLE analytics.events (id int primary key, timestamp timestamp, data varchar);\\n```\\n\\n#### Query Plan Recalculation\\n\\nConfigure when query plans are recalculated based on data changes:\\n\\n```sql\\nCREATE TABLE Person (\\n  id INT PRIMARY KEY,\\n  name VARCHAR,\\n  age INT\\n) WITH (MIN STALE ROWS 1000, STALE ROWS FRACTION 0.15);\\n```\\n\\nIgnite will recalculate plans automatically when the application exceeds the thresholds.\\n\\nAlternatively, manually invalidate plans to ensure they reflect current data:\\n\\n```shell\\nsql planner invalidate-cache --tables=PUBLIC.Person\\n```\\n\\n#### EXPLAIN Output Improvements\\n\\nUse the improved EXPLAIN command to track which nodes execute queries and what data they access, making query execution plans clearer. The command now also supports the `EXPLAIN MAPPING FOR` option for tracking data distribution.\\n\\n#### New Functions\\n\\n- `GROUPING`: Aggregate function for advanced grouping operations\\n- `CURRENT_USER`: Access current user for auditing and access control\\n\\n### Code Deployment\\n\\nAccess deployment unit information directly from your compute jobs to better diagnose issues and validate your code:\\n\\n```java\\npublic class DiagnosticJob implements ComputeJob<Void, String> {\\n    @Override\\n    public CompletableFuture<String> executeAsync(JobExecutionContext context, Void input) {\\n        String deploymentInfo = context.deploymentUnits().stream()\\n            .map(unit -> String.format(\\"%s:%s at %s\\", unit.name(), unit.version(), unit.path()))\\n            .collect(Collectors.joining(\\", \\"));\\n        return CompletableFuture.completedFuture(deploymentInfo);\\n    }\\n}\\n```\\n\\nDeployment improvements in 3.1:\\n\\n- ZIP archive support preserves folder structure for complex applications\\n- Files over 10 MB now supported\\n- Automatic unit loading at node startup\\n\\n### Production Operations\\n\\n#### Metrics and Observability\\n\\nApache Ignite 3.1 adds comprehensive metrics across all major subsystems:\\n\\n- **Storage Metrics**: Checkpoint operations, data regions, and storage I/O for aipersist storage engine\\n- **Table Metrics**: Per-table operation statistics including read/write throughput\\n- **Rebalance Metrics**: Track rebalancing progress and performance\\n- **SQL Query Metrics**: Execution time, row counts, and query cache hit rates\\n- **Transaction Metrics**: Transaction lifecycle and duration tracking\\n- **Topology Metrics**: Node join/leave events and cluster state changes\\n- **Throttling Metrics**: Backpressure and flow control statistics\\n- **Clock Drift Metrics**: Monitor time synchronization across cluster nodes\\n\\n**Metric Log Exporter**: Exports metrics to files. The exporter is used by default for all new clusters, providing guaranteed access to basic cluster metrics.\\n\\n#### System Views for Cluster Introspection\\n\\nNew views expose internal cluster state:\\n\\n- `SYSTEM.SQL_CACHED_QUERY_PLANS`: View cached query plans\\n- `SYSTEM.INDEX_COLUMNS`: Access index column information\\n- `SYSTEM.SCHEMAS`: List all schemas in the cluster\\n\\nAll system views now use standardized column naming. Old column naming is still supported for compatibility purposes.\\n\\n#### Compute Job Lifecycle Events\\n\\nNew lifecycle events help you track compute jobs through submission, execution, completion, and failure. MapReduce task events provide visibility into distributed computations.\\n\\n### Cluster Management\\n\\n#### Automatic Metastorage Node Selection\\n\\nIgnite now automatically selects metastorage and cluster management group nodes based on cluster size on cluster initialization:\\n\\n- \u22643 nodes: all nodes participate\\n- 4 nodes: 3 nodes (maintains odd number for consensus)\\n- \u22655 nodes: 5 nodes (balances fault tolerance with overhead)\\n\\n#### Multicast Discovery for Dynamic Environments\\n\\nNodes discover each other automatically using multicast, removing the need for static node lists in containerized deployments:\\n\\n```shell\\nnode config update ignite.network.nodeFinder.multicast.group=239.5.0.0\\nnode config update ignite.network.nodeFinder.type=MULTICAST\\n```\\n\\n#### Docker Enhancements\\n\\nThe default docker environment comes with multiple improvements:\\n\\n- `BOOTSTRAP_NODE_CONFIG` environment variable for configuration management\\n- ARM64 images for ARM-based systems\\n- Non-root default user improves security\\n- Java 17 and 21 images available\\n\\n#### Distribution Zone Quorum Control\\n\\nYou can explicitly set quorum requirements in distribution zones:\\n\\n```sql\\nCREATE ZONE exampleZone (REPLICAS 3, QUORUM SIZE 3) STORAGE PROFILES[\'default\'];\\n```\\n\\n### Transaction Improvements\\n\\nNew transaction timeout options can set different timeouts for read-only and read-write transactions:\\n\\n- `readOnlyTimeoutMillis`: Shorter timeout for read-only transactions\\n- `readWriteTimeoutMillis`: Longer timeout for complex write operations\\n\\nThis prevents read-only queries from timing out unnecessarily while protecting against long-running writes.\\n\\n### Java API Updates\\n\\nMultiple new Java methods simplify the work with your cluster and provide more direct access to data:\\n\\n- `deleteAll()`: Bulk delete operations\\n- `ignite.cluster().nodes()`: Returns nodes in logical topology\\n- `ignite.cluster().localNode()`: Quick access to local node in embedded mode\\n- `CancelHandle` API: Stop queries, transactions, and compute jobs\\n- Batched execution cancellation support\\n\\n### Disaster Recovery and Operational Tools\\n\\nNew CLI and REST APIs enable partition-level data cleanup and restart, letting you recover from corrupted partitions without restarting the whole cluster. The system properly destroys tables during node recovery and cleans abandoned transaction write intents during index builds.\\n\\n### Migration from Apache Ignite 2\\n\\nApache Ignite 3.1 includes a complete migration toolkit with DDL generator for automatic schema conversion, persistent data migration with progress tracking, and automatic type conversion for legacy Java time APIs. The toolkit supports authenticated operations and complex field mappings for key and value replication.\\n\\n### Breaking Changes and Deprecations\\n\\nAll breaking changes include backward compatibility support. Recreate your cluster, update your code and configuration before 3.2, when deprecated approaches will be removed.\\n\\n#### Zone-Based Replication Migration {#zone-based-replication-migration}\\n\\nZone-based replication changes how RAFT groups are allocated across tables. Clusters upgraded from 3.0 continue using table-based replication to preserve stability. To adopt zone-based replication and gain the performance benefits, create a new 3.1 cluster and migrate data using SQL `COPY INTO`/`COPY FROM` commands. See the [3.0 to 3.1 Migration Guide](https://ignite.apache.org/docs/ignite3/latest/installation/migration-from-ai3-1) for detailed workflow.\\n\\n#### Configuration and API Changes\\n\\n**Configuration**: Property names now include units (i.e. `timeoutMillis` instead of `timeout`). System properties were consolidated under `ignite.system`. Old formats work temporarily.\\n\\n**SQL Syntax**: `CREATE ZONE` syntax modernized to align with SQL standards. Old `WITH` clause syntax is deprecated but functional.\\n\\n**Java API**: `ignite.clusterNodes()` deprecated in favor of `ignite.cluster().nodes()`. System view columns standardized with old names temporarily available.\\n\\n**Data Types**: `BINARY` and `CHAR` removed. Use `VARBINARY` and `VARCHAR` instead. Maximum precision for `VARCHAR`/`VARBINARY` increased to 2GB.\\n\\n### Get Started\\n\\n**Download**: [Apache Ignite 3.1](/download)\\n\\n**Migration Guide**: [Upgrading from 3.0](https://ignite.apache.org/docs/ignite3/latest/installation/migration-from-ai3-1)\\n\\n**Community**: Join the [Apache Ignite mailing list](https://ignite.apache.org/community/resources.html) or [Slack channel](https://ignite.apache.org/community/resources.html)\\n\\nQuestions about upgrading? Ask on the [dev list](mailto:dev@ignite.apache.org) or [user list](mailto:user@ignite.apache.org)."},{"id":"/2025/02/24/whats-new-in-apache-ignite-3-0","metadata":{"permalink":"/suggested-site/blog/2025/02/24/whats-new-in-apache-ignite-3-0","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-02-24-whats-new-in-apache-ignite-3-0.mdx","source":"@site/blog/2025-02-24-whats-new-in-apache-ignite-3-0.mdx","title":"What\'s New in Apache Ignite 3.0","description":"Apache Ignite 3.0 is the latest milestone in Apache Ignite evolution that enhances developer experience, platform resilience, and efficiency. In this article, we\'ll explore the key new features and improvements in Apache Ignite 3.0.","date":"2025-02-24T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"release","permalink":"/suggested-site/blog/tags/release"},{"inline":true,"label":"ignite3","permalink":"/suggested-site/blog/tags/ignite-3"}],"readingTime":3.85,"hasTruncateMarker":true,"authors":[{"name":"Stanislav Lukyanov","title":"Apache Ignite Committer","url":"https://github.com/slukyanov","imageURL":"/suggested-site/img/authors/slukyanov.png","key":"stanislav","page":null}],"frontMatter":{"title":"What\'s New in Apache Ignite 3.0","authors":["stanislav"],"date":"2025-02-24T00:00:00.000Z","tags":["apache","ignite","release","ignite3"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 3.1: Performance, Multi-Language Client Support, and Production Hardening","permalink":"/suggested-site/blog/2025/11/03/apache-ignite-3-1"},"nextItem":{"title":"Apache Ignite 2.17 Release: What\'s New","permalink":"/suggested-site/blog/2025/02/13/apache-ignite-2-17-0"}},"content":"Apache Ignite 3.0 is the latest milestone in Apache Ignite evolution that enhances developer experience, platform resilience, and efficiency. In this article, we\'ll explore the key new features and improvements in Apache Ignite 3.0.\\n\\n\x3c!--truncate--\x3e\\n\\n## Installation and Management of Ignite 3\\n\\nApache Ignite 3.0 simplifies the installation and management process, making it more accessible for developers and administrators.\\n\\n### New Ignite 3 Installation Packages\\n\\nApache Ignite 3.0 introduces modern Linux installation options:\\n\\n- **Native Linux Packages**: Ignite now offers native RPM and DEB installation packages for Linux distributions, making it easier to install, upgrade, and manage through standard package managers.\\n\\n### Interactive Unified Ignite CLI\\n\\nThe new Ignite 3.0 Command Line Interface (CLI) provides a streamlined experience for managing and interacting with the cluster.\\n\\n- **Single CLI Tool**: The new CLI unifies cluster configuration, management, and SQL access in one tool, reducing complexity.\\n- **Scripting and Interactive Mode**: Users can execute scripts for automation or use an interactive shell (REPL) for direct interaction with the cluster.\\n- **Syntax Highlighting and Autocomplete**: The interactive mode includes enhanced usability features such as syntax highlighting and command autocompletion, making cluster management and troubleshooting easier.\\n\\n### Dynamic Configuration Framework\\n\\nApache Ignite 3.0 replaces the legacy Spring XML-based configuration with a modern dynamic configuration system:\\n\\n- **HOCON-based Configuration**: Human-Optimized Config Object Notation (HOCON) is now the standard configuration language, offering a more readable and flexible alternative to XML.\\n- **Dynamic Changes**: Configuration parameters can now be modified dynamically via the CLI or REST API without requiring a node restart, improving operational flexibility.\\n\\n## Clients and APIs for Ignite 3\\n\\nApache Ignite 3.0 streamlines application development by unifying access across different clients and data models while ensuring flexibility and improving developer experience.\\n\\n### Unified Ignite Clients\\n\\nPreviously, Ignite had a distinction between thin and thick clients, requiring developers to choose between lightweight connections or full-featured clients. In Ignite 3.0:\\n\\n- **Lightweight Yet Fully Featured Ignite Clients**: New clients now support the full range of API features without the need to choose between thin or thick clients.\\n- **Consistent Experience Across Languages**: The new client model ensures consistency in API behavior across different programming languages, reducing learning curves and integration complexities.\\n\\n### Table API and Unified Schema for Ignite 3\\n\\nIgnite 3.0 enhances the way the data schema is managed and accessed across SQL and NoSQL interfaces.\\n\\n- **Consistent Data Schema**: All data representations\u2014SQL schema, key-value data types, and internal data storage\u2014are now unified, eliminating the need for error-prone mappings between different models.\\n- **Table Views**: Table Views is a new API that allows applications to independently define how they access the database tables, increasing flexibility in choosing the client-side data model.\\n- **Record API**: Record API is a new alternative to Key-Value API that allows you to use a single data object without separating it into keys and values, which further increases flexibility of development.\\n\\n### Asynchronous and Reactive Programming for Ignite 3\\n\\nIgnite 3.0 enhances support for concurrent programming paradigms:\\n\\n- **Language Features Support**: Client APIs support asynchronous and reactive programming with standard language constructs such as CompletableFuture and Flow API in Java or Task and IAsyncEnumerable in .NET.\\n\\n### SQL Transactions in Ignite 3\\n\\nIgnite 3.0 includes full transaction support in SQL:\\n\\n- **Transactional SQL Support**: Full support for transactions allows you to bring more SQL applications to Ignite without compromising on consistency.\\n- **Combining SQL and NoSQL**: Both SQL and NoSQL-style APIs can be used together in a single transaction, enabling developers to choose the API they want to use at any given time.\\n\\n## Architecture and Performance in Ignite 3\\n\\nApache Ignite 3.0 includes major architectural upgrades to improve cluster reliability and efficiency.\\n\\n### Raft-based Consensus\\n\\nApache Ignite 3.0 now utilizes a Raft-based consensus mechanism for enhanced resiliency:\\n\\n- **Improved Fault Tolerance**: Distributed consensus ensures stronger cluster stability and prevents split-brain scenarios.\\n- **Simplified Cluster Changes**: The new protocol reduces impact of node restarts and topology changes on the overall cluster, minimizing downtime and simplifying maintenance.\\n\\n## Apache Ignite 3.0\\n\\nApache Ignite 3.0 is a major step forward, simplifying installation, improving configuration management, enhancing API usability, and significantly boosting performance.\\n\\n- **For New Ignite Users**: Apache Ignite 3.0 provides an easier learning curve, streamlining first-time installation and configuration.\\n- **For Experienced Ignite Users**: Enhanced developer experience simplifies continuous improvement and maintenance.\\n\\nTo learn more:\\n\\n- Register for and attend [Ignite Summit 2025](https://events.ringcentral.com/events/ignite-summit-2025) on Feb 25 online!\\n- Don\'t miss the session, \\"[What\'s New and Cool in Ignite 3](https://ignite-summit.org/2025/sessions/844189)\\" by Pavel Tupitsyn, a core Ignite committer.\\n\\nTo try the new Apache Ignite 3.0 capabilities yourself:\\n\\n- [Download Apache Ignite 3.0](https://ignite.apache.org/download) from the official website."},{"id":"/2025/02/13/apache-ignite-2-17-0","metadata":{"permalink":"/suggested-site/blog/2025/02/13/apache-ignite-2-17-0","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2025-02-13-apache-ignite-2-17-0.mdx","source":"@site/blog/2025-02-13-apache-ignite-2-17-0.mdx","title":"Apache Ignite 2.17 Release: What\'s New","description":"We are happy to announce the release of Apache Ignite 2.17.0! In this latest version, the Ignite community has introduced a range of new features and improvements to deliver a more efficient, flexible, and future-proof platform. Below, we\'ll cover the key highlights that you can look forward to when upgrading to the new release.","date":"2025-02-13T00:00:00.000Z","tags":[{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"release","permalink":"/suggested-site/blog/tags/release"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":4.05,"hasTruncateMarker":true,"authors":[{"name":"Nikita Amelchev","title":"Apache Ignite Committer","url":"https://github.com/NSAmelchev","imageURL":"/suggested-site/img/authors/nsamelchev.png","key":"nikita","page":null}],"frontMatter":{"title":"Apache Ignite 2.17 Release: What\'s New","authors":["nikita"],"date":"2025-02-13T00:00:00.000Z","tags":["database","ignite","release","archived"]},"unlisted":false,"prevItem":{"title":"What\'s New in Apache Ignite 3.0","permalink":"/suggested-site/blog/2025/02/24/whats-new-in-apache-ignite-3-0"},"nextItem":{"title":"Ignite on .NET 9 and Intel CET","permalink":"/suggested-site/blog/2024/11/22/apache-ignite-net-intel-cet-fix"}},"content":"We are happy to announce the release of [Apache Ignite](https://ignite.apache.org/) 2.17.0! In this latest version, the Ignite community has introduced a range of new features and improvements to deliver a more efficient, flexible, and future-proof platform. Below, we\'ll cover the key highlights that you can look forward to when upgrading to the new release.\\n\\n\x3c!--truncate--\x3e\\n\\n### Migrating to Java 11 {#migrate-to-java-11}\\n\\nIgnite 2.17 has officially moved its codebase to Java 11. With Java 8 approaching end-of-life (for public updates) and many organizations already moving to newer Java versions, this change enables Ignite to take advantage of modern language features, improved performance, and security enhancements. By upgrading Ignite to this latest release, you can ensure your cluster runs on a more up-to-date and secure Java environment.\\n\\n### Transaction-Aware SQL and Scan Queries {#transaction-aware-sql}\\n\\nApache Ignite provides multiple interfaces to query data, including:\\n\\n- Key-Value API.\\n- SQL.\\n- Scan query.\\n- Index scan query.\\n\\nWhile Ignite has long supported ACID transactions for the Key-Value API, there has been a known limitation when it comes to data queries: changes made within an ongoing transaction were not visible to SQL or Scan queries. This could lead to inconsistencies when querying data inside a transaction.\\n\\nWith the release of Apache Ignite 2.17, we are excited to introduce transaction-aware SQL and Scan queries, a significant enhancement that improves consistency and isolation when querying data within transactions.\\n\\n**NOTE**: This feature is currently supported only for the Calcite SQL engine and at the `READ_COMMITTED` isolation level.\\n\\nTo enable this feature, set the `txAwareQueriesEnabled` property in the `TransactionConfiguration`. Once enabled, your SQL and Scan queries will reflect transaction-related changes in real-time.\\n\\nHere\'s an example of how it works:\\n\\n```java\\ntry (Transaction tx = srv.transactions().txStart(PESSIMISTIC, READ_COMMITTED)) {\\n    cache.put(1, 2);\\n\\n    List<List<?>> sqlData = executeSql(srv, \\"SELECT COUNT(*) FROM TBL.TBL\\");\\n    List<Entry<Integer, Integer>> scanData = cache.query(new ScanQuery<Integer, Integer>()).getAll();\\n\\n    assertEquals(\\"Must see transaction related data\\", 1L, sqlData.get(0).get(0));\\n    assertEquals(\\"Must see transaction related data\\", 1, scanData.size());\\n\\n    tx.commit();\\n}\\n```\\n\\n### Control Utility Switches to Thin Client Protocol {#control-utility-migrates-to-thin-client}\\n\\nStarting from Apache Ignite version 2.17, the utility by default uses a connection through the thin client protocol (configured on a node via `ClientConnectorConfiguration`).\\n\\nWith the default configuration of Ignite, no migration actions will be required. Additional configuration of the connector is no longer necessary.\\n\\nIn some cases, a few actions may be required to migrate user scripts using the utility. See more in [documentation](https://ignite.apache.org/docs/latest/tools/control-script#migration-to-the-thin-client-protocol). It\'s important to note that connection via Binary-REST protocol (configured via `ConnectorConfiguration`) has been deprecated and is planned to be removed in future releases.\\n\\n### Custom User-Defined Metrics {#custom-user-defined-metrics}\\n\\nYou can now create custom user-defined metrics in Ignite 2.17. This feature allows you to gather and expose metric data specific to the needs of your application. You can perform in-depth monitoring and troubleshooting by tracking those metrics that matter most to you. Here is example of usage:\\n\\n```java\\nIgnite ignite = Ignition.ignite();\\n\\nignite.metrics()\\n    .getOrCreate(\\"app-metric-registry\\")\\n    .register(\\"Status\\", () -> appStatus.get(), \\"Application status.\\");\\n```\\n\\n### Calcite SQL Engine {#calcite-sql-engine}\\n\\nThe Apache Ignite community continues to enhance the SQL experience, and the latest updates to the [Calcite SQL engine](https://ignite.apache.org/docs/latest/SQL/sql-calcite) bring several powerful features and improvements!\\n\\nUpdates include support for bitwise operations and aggregates (e.g., `BIT_AND`, `BIT_OR`, `BIT_XOR`), arithmetic overflow handling, date formatting/parsing with custom formats, and join type hints for better query optimization. Additionally, the Calcite engine now supports grouping by aliases and ordinal values, providing more flexibility in query design.\\n\\nLooking ahead, the Ignite community has big plans for Calcite. The goal is to make Calcite the default SQL engine in future releases, deprecating the legacy H2-based engine.\\n\\n### Java Thin Client Enhancements: `invoke()` and `invokeAll()` {#java-thin-client-enhancements}\\n\\nIn this release, the Java thin client adds entry processor methods that will simplify how you interact with Ignite clusters: `invoke()` and `invokeAll()`.\\n\\nAn entry processor is used to process cache entries on the nodes where they are stored. An entry processor does not require the entry to be transferred to the client in order to perform an operation on it. The operation is performed remotely, and only the results are transmitted to the client.\\n\\nHere is example of an operation:\\n\\n```java\\nClientCache<Integer, Integer> cache = client.getOrCreateCache(\\"myCache\\");\\n\\ncache.invoke(0, new IncrementProcessor());\\n```\\n\\n**NOTE**: The classes of the entry processors must be available on the server nodes of the cluster.\\n\\n### Anything else? {#anything-else}\\n\\nSee the [release notes](https://ignite.apache.org/releases/ignite2/2.17.0/release_notes.html) to learn about all of the new features and improvements.\\n\\nReach out to us on the community user list for more questions, details, and feedback.\\n\\nSincerely yours, Ignite contributors and committers"},{"id":"/2024/11/22/apache-ignite-net-intel-cet-fix","metadata":{"permalink":"/suggested-site/blog/2024/11/22/apache-ignite-net-intel-cet-fix","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2024-11-22-apache-ignite-net-intel-cet-fix.mdx","source":"@site/blog/2024-11-22-apache-ignite-net-intel-cet-fix.mdx","title":"Ignite on .NET 9 and Intel CET","description":"Old JDK code meets new Intel security feature, JVM + CLR in one process, and a mysterious crash.","date":"2024-11-22T00:00:00.000Z","tags":[{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":".NET","permalink":"/suggested-site/blog/tags/net"},{"inline":true,"label":"security","permalink":"/suggested-site/blog/tags/security"},{"inline":true,"label":"Intel","permalink":"/suggested-site/blog/tags/intel"},{"inline":true,"label":"CET","permalink":"/suggested-site/blog/tags/cet"}],"readingTime":0.13,"hasTruncateMarker":true,"authors":[{"name":"Pavel Tupitsyn","title":"Apache Ignite Committer","url":"https://github.com/ptupitsyn","imageURL":"/suggested-site/img/authors/ptupitsyn.png","key":"pavel","page":null}],"frontMatter":{"title":"Ignite on .NET 9 and Intel CET","authors":["pavel"],"date":"2024-11-22T00:00:00.000Z","tags":["database","ignite",".NET","security","Intel","CET"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.17 Release: What\'s New","permalink":"/suggested-site/blog/2025/02/13/apache-ignite-2-17-0"},"nextItem":{"title":"Apache Ignite 2.16.0: Cache dumps, Calcite engine stabilization, JDK 14+ bug fixes","permalink":"/suggested-site/blog/2023/12/25/apache-ignite-2-16-0"}},"content":"Old JDK code meets new Intel security feature, JVM + CLR in one process, and a mysterious crash.\\n\\n\x3c!--truncate--\x3e\\n\\n[Read More...](https://ptupitsyn.github.io/Ignite-on-NET-9/)"},{"id":"/2023/12/25/apache-ignite-2-16-0","metadata":{"permalink":"/suggested-site/blog/2023/12/25/apache-ignite-2-16-0","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2023-12-25-apache-ignite-2-16-0.mdx","source":"@site/blog/2023-12-25-apache-ignite-2-16-0.mdx","title":"Apache Ignite 2.16.0: Cache dumps, Calcite engine stabilization, JDK 14+ bug fixes","description":"As of December 25, 2023, Apache Ignite 2.16 has been released. You can directly check the full list of resolved Important JIRA tasks but let\'s briefly overview some valuable improvements.","date":"2023-12-25T00:00:00.000Z","tags":[{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"in","permalink":"/suggested-site/blog/tags/in"},{"inline":true,"label":"memory","permalink":"/suggested-site/blog/tags/memory"},{"inline":true,"label":"open","permalink":"/suggested-site/blog/tags/open"},{"inline":true,"label":"source","permalink":"/suggested-site/blog/tags/source"},{"inline":true,"label":"release","permalink":"/suggested-site/blog/tags/release"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":4.01,"hasTruncateMarker":true,"authors":[{"name":"Nikita Amelchev","title":"Apache Ignite Committer","url":"https://github.com/NSAmelchev","imageURL":"/suggested-site/img/authors/nsamelchev.png","key":"nikita","page":null}],"frontMatter":{"title":"Apache Ignite 2.16.0: Cache dumps, Calcite engine stabilization, JDK 14+ bug fixes","authors":["nikita"],"date":"2023-12-25T00:00:00.000Z","tags":["database","ignite","in","memory","open","source","release","archived"]},"unlisted":false,"prevItem":{"title":"Ignite on .NET 9 and Intel CET","permalink":"/suggested-site/blog/2024/11/22/apache-ignite-net-intel-cet-fix"},"nextItem":{"title":"Dynamic LINQ performance and usability with Ignite.NET and System.Linq.Dynamic","permalink":"/suggested-site/blog/2023/05/22/apache-ignite-net-dynamic-linq"}},"content":"As of December 25, 2023, [Apache Ignite](https://ignite.apache.org/) 2.16 has been released. You can directly check the full list of resolved [Important JIRA tasks](https://s.apache.org/j3brc) but let\'s briefly overview some valuable improvements.\\n\\n### Cache dumps\\n\\nIgnite has persistent cache [snapshots](https://ignite.apache.org/docs/latest/snapshots/snapshots) and this feature is highly appreciated by Ignite users. This release introduces another way to make a copy of user data - a cache dump.\\n\\nThe cache dump is essentially a file that contains all entries of a cache group at the time of dump creation. Dump is consistent like a snapshot, which means all entries that existed in the cluster at the moment of dump creation will be included in the dump file. Meta information of dumped caches and binary meta are also included in the dump.\\n\\nMain differences from cache snapshots:\\n\\n- Supports in-memory caches that a snapshot feature does not support.\\n- Takes up less disk space. The dump contains only the cache entries as-is.\\n- Can be used for offline data processing.\\n\\n\x3c!--truncate--\x3e\\n\\nThe [IgniteSnapshot](https://github.com/apache/ignite/blob/ignite-2.16/modules/core/src/main/java/org/apache/ignite/IgniteSnapshot.java#L75) interface provides a method to create a dump:\\n\\n```java\\nignite.snapshot().createDump(\\"dump-name\\", Arrays.asList(\\"cacheGrp1\\", \\"cacheGrp2\\")).get();\\n```\\n\\nYou can read a dump using the [Dump Reader](https://github.com/apache/ignite/blob/ignite-2.16/modules/core/src/main/java/org/apache/ignite/dump/DumpReader.java) application:\\n\\n```java\\n// Consumer should be implemented to receive all data stored in cache dump.\\nDumpConsumer cnsmr = new DumpConsumerImpl(..);\\n\\nDumpReaderConfiguration cfg = new DumpReaderConfiguration(new File(\\"path-to-dump\\"), cnsmr);\\n\\nnew DumpReader(cfg, log).run();\\n```\\n\\n### Calcite engine: added hints and diagnostic tools, became more stable\\n\\nThe [Calcite engine](https://ignite.apache.org/docs/2.16.0/SQL/sql-calcite) is now fully equipped with metrics, events and the performance statistics tool. Moreover, query plans and other useful properties have been added to the [performance statistics report](https://ignite.apache.org/docs/2.16.0/extensions-and-integrations/performance-statistics).\\n\\nThe following hints have been introduced to help the optimizer make optimizations more rational or build an execution plan faster:\\n\\n- [FORCE_INDEX / NO_INDEX](https://ignite.apache.org/docs/2.16.0/SQL/sql-calcite#force_index-no_index) - Forces or disables index scan.\\n- [ENFORCE_JOIN_ORDER](https://ignite.apache.org/docs/2.16.0/SQL/sql-calcite#enforce_join_order) - Forces join order as appears in a query. Fastens building of joins plan.\\n\\nThere are 10+ bugs and performance fixes in the release.\\n\\n### Java thin client: Service Awareness feature\\n\\nIn previous versions of Ignite, the thin client invoked the service grid on a random server node. A service may not be deployed on each node. If a node misses the invoked service, the invocation request is redirected to a proper node. This additional network hop adds overhead and can now be avoided.\\n\\nWith Service Awareness, the thin client knows where service instances are deployed and sends the request to the correct node.\\n\\nNote that the service topology is updated asynchronously starting with the first service invocation. Thus, some invocation redirects are still possible.\\n\\nThe feature is activated if:\\n\\n- The [Partition Awareness](https://github.com/apache/ignite/blob/ignite-2.16/modules/core/src/main/java/org/apache/ignite/configuration/ClientConfiguration.java#L577) property is enabled in a client configuration (it is enabled by default).\\n- Cluster and thin client versions are 2.16.0 and higher.\\n\\n### CDC: Realtime mode\\n\\nSince version 2.12 Ignite supports the [CDC functionality](https://ignite.apache.org/docs/2.16.0/persistence/change-data-capture). It\'s implemented with a background process that processes archived WAL segments. The delay between the creation of an event and it\'s processing by this process contains the time WAL segments were archived. Some users are intolerant of this lag. For such users, Ignite now provides functionality that makes it possible to process events almost instantly after they are created. Please refer to the [CdcManager](https://github.com/apache/ignite/blob/ignite-2.16/modules/core/src/main/java/org/apache/ignite/internal/cdc/CdcManager.java) interface javadocs for more details.\\n\\n### Other improvements and changes\\n\\n- Fixed JDK 14-21 support issues.\\n- [Cluster Management API](https://cwiki.apache.org/confluence/display/IGNITE/IEP-81+Cluster+Management+API) has been implemented to unify management command invokers via various protocols - CLI, JMX, REST. The Ignite now provides JMX command invoker.\\n- Operations on atomic caches within transactions are finally forbidden. The system property `IGNITE_ALLOW_ATOMIC_OPS_IN_TX` has been removed.\\n- The community agreed to remove MVCC. The `CacheAtomicityMode#TRANSACTIONAL_SNAPSHOT` cache mode has been removed.\\n- Mixed mode cache groups are now forbidden, but `IGNITE_ALLOW_MIXED_CACHE_GROUPS` system option may temporarily allow them.\\n- The ignite-ml and cassandra modules have been migrated to the [Ignite extensions](https://github.com/apache/ignite-extensions).\\n- 100+ small improvements and bug fixes.\\n\\n### Anything else?\\n\\nSee the [release notes](https://ignite.apache.org/releases/ignite2/2.16.0/release_notes.html) to learn about all of the new features and improvements.\\n\\nReach out to us on the community user list for more questions, details, and feedback.\\n\\nSincerely yours, Ignite contributors and committers"},{"id":"/2023/05/22/apache-ignite-net-dynamic-linq","metadata":{"permalink":"/suggested-site/blog/2023/05/22/apache-ignite-net-dynamic-linq","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2023-05-22-apache-ignite-net-dynamic-linq.mdx","source":"@site/blog/2023-05-22-apache-ignite-net-dynamic-linq.mdx","title":"Dynamic LINQ performance and usability with Ignite.NET and System.Linq.Dynamic","description":"Dynamically building database queries can be necessary for some use cases, such as UI-defined filtering. This can get challenging with LINQ frameworks like EF Core and Ignite.NET.","date":"2023-05-22T00:00:00.000Z","tags":[{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"linq","permalink":"/suggested-site/blog/tags/linq"},{"inline":true,"label":".NET","permalink":"/suggested-site/blog/tags/net"},{"inline":true,"label":"SQL","permalink":"/suggested-site/blog/tags/sql"},{"inline":true,"label":"performance","permalink":"/suggested-site/blog/tags/performance"},{"inline":true,"label":"csharp","permalink":"/suggested-site/blog/tags/csharp"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":0.19,"hasTruncateMarker":true,"authors":[{"name":"Pavel Tupitsyn","title":"Apache Ignite Committer","url":"https://github.com/ptupitsyn","imageURL":"/suggested-site/img/authors/ptupitsyn.png","key":"pavel","page":null}],"frontMatter":{"title":"Dynamic LINQ performance and usability with Ignite.NET and System.Linq.Dynamic","authors":["pavel"],"date":"2023-05-22T00:00:00.000Z","tags":["database","ignite","linq",".NET","SQL","performance","csharp","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.16.0: Cache dumps, Calcite engine stabilization, JDK 14+ bug fixes","permalink":"/suggested-site/blog/2023/12/25/apache-ignite-2-16-0"},"nextItem":{"title":"Apache Ignite 2.13.0: new Apache Calcite-based SQL engine","permalink":"/suggested-site/blog/2022/04/28/apache-ignite-2-13-0"}},"content":"Dynamically building database queries can be necessary for some use cases, such as UI-defined filtering. This can get challenging with LINQ frameworks like EF Core and Ignite.NET.\\n\\n\x3c!--truncate--\x3e\\n\\n[Read More...](https://ptupitsyn.github.io/Dynamic-LINQ-With-Ignite/)"},{"id":"/2022/04/28/apache-ignite-2-13-0","metadata":{"permalink":"/suggested-site/blog/2022/04/28/apache-ignite-2-13-0","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2022-04-28-apache-ignite-2-13-0.mdx","source":"@site/blog/2022-04-28-apache-ignite-2-13-0.mdx","title":"Apache Ignite 2.13.0: new Apache Calcite-based SQL engine","description":"As of April 26, 2022, Apache Ignite 2.13 has been released. You can directly check the full list of resolved Important JIRA tasks but here let\'s briefly overview some valuable improvements.","date":"2022-04-28T00:00:00.000Z","tags":[{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"in","permalink":"/suggested-site/blog/tags/in"},{"inline":true,"label":"memory","permalink":"/suggested-site/blog/tags/memory"},{"inline":true,"label":"open","permalink":"/suggested-site/blog/tags/open"},{"inline":true,"label":"source","permalink":"/suggested-site/blog/tags/source"},{"inline":true,"label":"release","permalink":"/suggested-site/blog/tags/release"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":3.02,"hasTruncateMarker":true,"authors":[{"name":"Nikita Amelchev","title":"Apache Ignite Committer","url":"https://github.com/NSAmelchev","imageURL":"/suggested-site/img/authors/nsamelchev.png","key":"nikita","page":null}],"frontMatter":{"title":"Apache Ignite 2.13.0: new Apache Calcite-based SQL engine","authors":["nikita"],"date":"2022-04-28T00:00:00.000Z","tags":["database","ignite","in","memory","open","source","release","archived"]},"unlisted":false,"prevItem":{"title":"Dynamic LINQ performance and usability with Ignite.NET and System.Linq.Dynamic","permalink":"/suggested-site/blog/2023/05/22/apache-ignite-net-dynamic-linq"},"nextItem":{"title":"Apache Ignite 2.12.0: CDC, Index Query API, Vulnerabilities Fixes","permalink":"/suggested-site/blog/2022/01/14/apache-ignite-2-12-0"}},"content":"As of April 26, 2022, [Apache Ignite](https://ignite.apache.org/) 2.13 has been released. You can directly check the full list of resolved [Important JIRA tasks](https://s.apache.org/x8u49) but here let\'s briefly overview some valuable improvements.\\n\\n#### This is a breaking change release: The legacy service grid implementation was removed.\\n\\n### New Apache Calcite-based SQL engine\\n\\nWe\'ve implemented a new experimental SQL engine based on Apache Calcite. Now it\'s possible to:\\n\\n- Get rid of some [H2 limitations](https://cwiki.apache.org/confluence/display/IGNITE/IEP-37%3A+New+query+execution+engine#IEP37:Newqueryexecutionengine-Motivation);\\n- [Optimize](https://cwiki.apache.org/confluence/display/IGNITE/IEP-37%3A+New+query+execution+engine#IEP37:Newqueryexecutionengine-Implementationdetails) some query execution.\\n\\nThe current H2-based engine has fundamental limitations. For example:\\n\\n- some queries should be splitted into 2 phases (map subquery and reduce subquery), but some of them cannot be effectively executed in 2 phases.\\n- H2 is a third-party database product with not-ASF license.\\n- The optimizer and other internal things are not supposed to work in a distributed environment.\\n- It\'s hard to make Ignite-specific changes to the H2 code, patches are often declined.\\n\\n\x3c!--truncate--\x3e\\n\\nThe Apache Calcite is a SQL engine with customizable modules. Requests can be splitted into more than 2 phases.\\n\\nA query engine can be set before query execution. Here is an example for SQL:\\n\\n```java\\nSELECT /*+ QUERY_ENGINE(\'h2\') */ fld FROM table;\\n```\\n\\nor\\n\\n```java\\nSELECT /*+ QUERY_ENGINE(\'calcite\') */ fld FROM table;\\n```\\n\\nSee JDBC and ODBC examples [here](https://github.com/apache/ignite/blob/master/modules/calcite/README.txt).\\n\\nThe new engine requires the ignite-indexing module (which depends on H2) to be included to the classpath to support queries infrastructure.\\n\\nSee more technical details about the new engine in the [IEP-37](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=130028084).\\n\\n### Read Repair strategies\\n\\n[\\"Read Repair\\"](https://ignite.apache.org/docs/latest/read-repair) refers to a technique of repairing inconsistencies between primary and backup copies of data during normal read operations. When a specific key (or keys) is read by a user operation, Ignite checks the values for the given key in all backup copies.\\n\\nWe\'ve implemented the new Read Repair strategies as follows:\\n\\n- LWW (Last Write Wins) - Last write (the newest entry) wins.\\n- PRIMARY - Value from the primary node wins.\\n- RELATIVE_MAJORITY - The relative majority: a value found more often than any other wins.\\n- REMOVE - Inconsistent entries will be removed.\\n- CHECK_ONLY - Only check will be performed.\\n\\n### Array type in Binary Object\\n\\nIn previous versions Ignite did not save information about array type. Now it can be stored in a binary object:\\n\\n```\\ncache.put(1, new Person[] {new Person(1), new Person(2)});\\n\\nPerson[] obj = cache.get(1);\\n\\nassertEquals(Person[].class, obj.getClass());\\n```\\n\\nThe feature is disabled by default due to compatibility issues. Set the IGNITE_USE_BINARY_ARRAYS system property to true to enable it.\\n\\n### CDC for in-memory caches\\n\\nThe [Change Data Capture](https://ignite.apache.org/docs/latest/persistence/change-data-capture) now can be configured for in-memory caches. From now on, only CDC needed records for such caches will be logged to WAL.\\n\\n### Other improvements and changes\\n\\n- The C++ thin client implemented continuous queries and asynchronous network events handling. See the updated thin clients features list [here](https://cwiki.apache.org/confluence/display/IGNITE/Thin+clients+features);\\n- Implemented NUMA-aware [allocator](https://github.com/apache/ignite/tree/master/modules/numa-allocator) for data regions;\\n- Ignite maven BOM;\\n- Removed the legacy service grid implementation;\\n- 100+ small improvements and bug fixes.\\n\\n### Anything else?\\n\\nSee the [release notes](https://ignite.apache.org/releases/ignite2/2.13.0/release_notes.html) to learn about all of the new features and improvements.\\n\\nReach out to us on the community user list for more questions, details, and feedback.\\n\\nSincerely yours,\\nIgnite contributors and committers"},{"id":"/2022/01/14/apache-ignite-2-12-0","metadata":{"permalink":"/suggested-site/blog/2022/01/14/apache-ignite-2-12-0","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2022-01-14-apache-ignite-2-12-0.mdx","source":"@site/blog/2022-01-14-apache-ignite-2-12-0.mdx","title":"Apache Ignite 2.12.0: CDC, Index Query API, Vulnerabilities Fixes","description":"As of January 14, 2022, Apache Ignite 2.12 has been released. You can directly check the full list of resolved Important JIRA tasks but here let\'s briefly overview some valuable improvements.","date":"2022-01-14T00:00:00.000Z","tags":[{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"in-memory","permalink":"/suggested-site/blog/tags/in-memory"},{"inline":true,"label":"open-source","permalink":"/suggested-site/blog/tags/open-source"},{"inline":true,"label":"release","permalink":"/suggested-site/blog/tags/release"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":3.86,"hasTruncateMarker":true,"authors":[{"name":"Nikita Amelchev","title":"Apache Ignite Committer","url":"https://github.com/NSAmelchev","imageURL":"/suggested-site/img/authors/nsamelchev.png","key":"nikita","page":null}],"frontMatter":{"title":"Apache Ignite 2.12.0: CDC, Index Query API, Vulnerabilities Fixes","authors":["nikita"],"date":"2022-01-14T00:00:00.000Z","tags":["database","ignite","in-memory","open-source","release","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.13.0: new Apache Calcite-based SQL engine","permalink":"/suggested-site/blog/2022/04/28/apache-ignite-2-13-0"},"nextItem":{"title":"Apache Ignite 2.11.1: Emergency Log4j2 Update","permalink":"/suggested-site/blog/2021/12/21/apache-ignite-2-11-1"}},"content":"As of January 14, 2022, [Apache Ignite](https://ignite.apache.org/) 2.12 has been released. You can directly check the full list of resolved [Important JIRA tasks](https://s.apache.org/0zyi2) but here let\'s briefly overview some valuable improvements.\\n\\n### Vulnerability Updates\\n\\nThe Apache Ignite versions lower than 2.11.1 are vulnerable to [CVE-2021-44832](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44832) which is related to the `ignite-log4j2` module usage.\\n\\nThe release also fixes 10+ CVE\'s of various modules. See [release notes](https://ignite.apache.org/releases/ignite2/2.12.0/release_notes.html) for more details.\\n\\n### Change Data Capture\\n\\nChange Data Capture ([CDC](https://en.wikipedia.org/wiki/Change_data_capture)) is a data processing pattern used to asynchronously receive entries that have been changed on the local node so that action can be taken using the changed entry.\\n\\n\x3c!--truncate--\x3e\\n\\nBelow are some of the CDC use cases:\\n\\n- Streaming changes in Warehouse;\\n- Updating search index;\\n- Calculating statistics (streaming queries);\\n- Auditing logs;\\n- Async interaction with external systems: Moderation, business process invocation, etc.\\n\\nIgnite implements CDC with the `ignite-cdc.sh` application and [Java API](https://github.com/apache/ignite/blob/master/modules/core/src/main/java/org/apache/ignite/cdc/CdcConsumer.java#L56).\\n\\nBelow are the CDC application and the Ignite node integrated via [WAL archive segments](https://ignite.apache.org/docs/2.12.0/persistence/native-persistence#write-ahead-log):\\n\\n![Apache Ignite CDC design](/img/blog/c5574c7d-2a24-4def-b1c4-b2343ed98397.svg)\\n\\nWhen CDC is enabled, the Ignite server node creates a hard link to each WAL archive segment in the special `db/cdc/{consistency_id}` directory. The `ignite-cdc.sh` application can be runruns on a different JVM and processes newly archived WAL segments. When the segment is fully processed by `ignite-cdc.sh`, it is removed. The actual disk space is free when both links (archive and CDC) are removed.\\n\\nState of consumption is a pointer to the last processed event. A consumer can tell `ignite-cdc.sh` to save the consumption state. On startup event processing will be continued from the last saved state.\\n\\nSee implementation details [here](https://ignite.apache.org/docs/2.12.0/persistence/change-data-capture).\\n\\n### Index Query API\\n\\nThe Apache Ignite now provides Index Query API for existing indexes. Index queries work over distributed indexes and retrieve cache entries that match the specified query.\\n\\nIt will help in some cases, where:\\n\\n- SQL is not applicable by the design of user application;\\n- `IndexScan` is preferable to `ScanQuery` for performance reasons.\\n\\nExample of query:\\n\\n```java\\n// Find the persons who work in Organization 1 and have salary more than 1,000.\\nQueryCursor<Cache.Entry<Integer, Person>> cursor = cache.query(\\n  new IndexQuery<Integer, Person>(Person.class, \\"ORG_SALARY_IDX\\")\\n  .setCriteria(eq(\\"orgId\\", 1), gt(\\"salary\\", 1000))\\n);\\n```\\n\\nSee more details [here](https://ignite.apache.org/docs/latest/key-value-api/using-cache-queries#executing-index-queries).\\n\\n### Snapshots\\n\\nPrevious versions can perform snapshot restore in the same cluster topology only. The new version provides the ability to restore snapshots on different cluster topologies. Moreover, added support of encrypted caches.\\n\\n### Distributed Environment Tests\\n\\nThe [new testing framework](https://cwiki.apache.org/confluence/display/IGNITE/IEP-56%3A+Distributed+environment+tests) was implemented. The main goal is to have a large enough set of integration tests that cover most of the typical cluster usage scenarios.\\n\\nFeatures:\\n\\n- Ignite nodes can be started/stopped on a Docker or a real cluster with any custom configuration;\\n- Any Apache Ignite version is supported (released or compiled from sources);\\n- Apache Ignite forks are also supported out of the box;\\n- Any other application execution is also possible, e.g. we implemented starters for Spark and Zookeeper;\\n- The cluster can be managed using the `control.sh`, we made this a part of the test API;\\n- Custom Java applications can be executed remotely with/without a built-in Ignite node or a Thin client;\\n- Any ssh command can be executed remotely, and the result will be available locally (at the python test);\\n- A network can be broken by editing `iptables` to check communication issues;\\n- Tests can be executed in parallel when the cluster size is bigger than tests requirements.\\n\\nFramework based on [Ducktape](https://ducktape-docs.readthedocs.io/en/latest/index.html) library from Kafka team, that\'s why we called it Ducktests.\\n\\n### Migration modules to the Apache Ignite Extensions\\n\\nThere is activity on the migration of frameworks to extensions:\\n\\n- GCE, AWS, Azure modules were migrated to `gce`, `aws`, `azure` extensions.\\n- CacheSpringStoreSessionListener was migrated to the `spring-tx` extension.\\n- TcpDiscoveryZookeeperIpFinder was migrated to the `zookeeper-ip-finder` extension.\\n\\nThe binaries archive now weighs 10 percent less.\\n\\n### Anything else?\\n\\nSee the [release notes](https://ignite.apache.org/releases/ignite2/2.12.0/release_notes.html) to learn about all of the new features and bug fixes.\\n\\nReach out to us on the community user list for more questions, details, and feedback.\\n\\nSincerely yours,\\n\\nIgnite contributors and committers"},{"id":"/2021/12/21/apache-ignite-2-11-1","metadata":{"permalink":"/suggested-site/blog/2021/12/21/apache-ignite-2-11-1","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2021-12-21-apache-ignite-2-11-1.mdx","source":"@site/blog/2021-12-21-apache-ignite-2-11-1.mdx","title":"Apache Ignite 2.11.1: Emergency Log4j2 Update","description":"The new Apache Ignite 2.11.1 is an emergency release that fixes CVE-2021-44228, CVE-2021-45046, CVE-2021-45105 related to the ignite-log4j2 module usage.","date":"2021-12-21T00:00:00.000Z","tags":[{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"in-memory","permalink":"/suggested-site/blog/tags/in-memory"},{"inline":true,"label":"log4j2","permalink":"/suggested-site/blog/tags/log-4-j-2"},{"inline":true,"label":"open-source","permalink":"/suggested-site/blog/tags/open-source"},{"inline":true,"label":"release","permalink":"/suggested-site/blog/tags/release"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":2.14,"hasTruncateMarker":true,"authors":[{"name":"Maxim Muzafarov","title":"Apache Ignite Committer","url":"https://github.com/mmuzaf","imageURL":"/suggested-site/img/authors/mmuzaf.png","key":"maxim","page":null}],"frontMatter":{"title":"Apache Ignite 2.11.1: Emergency Log4j2 Update","authors":["maxim"],"date":"2021-12-21T00:00:00.000Z","tags":["database","ignite","in-memory","log4j2","open-source","release","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.12.0: CDC, Index Query API, Vulnerabilities Fixes","permalink":"/suggested-site/blog/2022/01/14/apache-ignite-2-12-0"},"nextItem":{"title":"Apache Ignite 2.11: Stabilization First","permalink":"/suggested-site/blog/2021/09/20/apache-ignite-2-11-stabilization"}},"content":"The new [Apache Ignite](https://ignite.apache.org/) 2.11.1 is an emergency release that fixes [CVE-2021-44228](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-44228), [CVE-2021-45046](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-45046), [CVE-2021-45105](https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-45105) related to the ignite-log4j2 module usage.\\n\\n### Apache Ignite with Log4j Vulnerability\\n\\nAll the following conditions must be met:\\n\\n- The Apache Ignite version lower than 2.11.0 is used (since these vulnerabilities are already fixed in 2.11.1, 2.12, and upper versions);\\n- The `ignite-logj42` is used by Apache Ignite and located in the `libs` directory (by default it is located in the `libs/optional` directory, so these deployments are not affected);\\n- The Java version in use is older than the following versions: `8u191`, `11.0.1`. This is due to the fact that later versions set the JVM property `com.sun.jndi.ldap.object.trustURLCodebase` to `false` by default, which disables JNDI loading of classes from arbitrary URL code bases.\\n\\nNOTE: Relying only on the Java version as a protection against these vulnerabilities is very risky and has not been tested.\\n\\n\x3c!--truncate--\x3e\\n\\n### Risk Mitigation Without Upgrading\\n\\nPlease note that all of these cases require a cluster downtime, but we still recommend to upgrade the Apache Ignite.\\n\\n#### Method 1: Removing the Vulnerable Classes\\n\\nWhen using an older Apache Ignite version, it is possible to remove the JndiLookup class from any Java application by executing this command:\\n\\n```\\nfind $IGNITE_HOME/ -type f -name \\"*log4j-core-*.jar\\" -exec zip -q -d \\"{}\\" org/apache/logging/log4j/core/lookup/JndiLookup.class \\\\;\\n```\\n\\nThis will recursively find all log4j-core JAR files, starting from the `IGNITE_HOME` directory, and remove the vulnerable JndiLookup class from them.\\n\\n#### Method 2: Disabling Message Lookups\\n\\nThis method can be used as an additional protection layer in case you suspect not all log4j dependencies have been properly updated. If you are using the Apache Ignite of an older version, we recommend to disable message lookups globally by setting the environment variable `LOG4J_FORMAT_MSG_NO_LOOKUPS` to `true` or, alternatively, run the Apache Ignite with the `-Dlog4j2.formatMsgNoLookups=true` command-line option.\\n\\n#### Method 3: Replace log4j2 Dependency Manually\\n\\nIt is still possible to manually replace the Log4j of 2.x version in the Apache Ignite binary distribution to the 2.17.0 Log4j version if your log configuration does not imply to use the RoutingAppender. In case the RoutingAppender is used it may produce some error messages in a log file at the startup or empty lines during the execution, which are considered as a minor flow, however, we do not recommend this mitigation method in this case."},{"id":"/2021/09/20/apache-ignite-2-11-stabilization","metadata":{"permalink":"/suggested-site/blog/2021/09/20/apache-ignite-2-11-stabilization","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2021-09-20-apache-ignite-2-11-stabilization.mdx","source":"@site/blog/2021-09-20-apache-ignite-2-11-stabilization.mdx","title":"Apache Ignite 2.11: Stabilization First","description":"The new Apache Ignite 2.11 was released on September 17, 2021. It can be considered to be a greater extent as a stabilization release that closed a number of technical debts of the internal architecture and bugs. Out of more than 200 completed tasks, 120 are bug fixes. However, some valuable improvements still exist, so let\'s take a quick look at them together.","date":"2021-09-20T00:00:00.000Z","tags":[{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"in-memory","permalink":"/suggested-site/blog/tags/in-memory"},{"inline":true,"label":"open-source","permalink":"/suggested-site/blog/tags/open-source"},{"inline":true,"label":"release","permalink":"/suggested-site/blog/tags/release"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":4.22,"hasTruncateMarker":true,"authors":[{"name":"Maxim Muzafarov","title":"Apache Ignite Committer","url":"https://github.com/mmuzaf","imageURL":"/suggested-site/img/authors/mmuzaf.png","key":"maxim","page":null}],"frontMatter":{"title":"Apache Ignite 2.11: Stabilization First","authors":["maxim"],"date":"2021-09-20T00:00:00.000Z","tags":["database","ignite","in-memory","open-source","release","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.11.1: Emergency Log4j2 Update","permalink":"/suggested-site/blog/2021/12/21/apache-ignite-2-11-1"},"nextItem":{"title":"Apache Ignite Momentum: Highlights from 2020-2021","permalink":"/suggested-site/blog/2021/09/14/apache-ignite-momentum-highlights-from"}},"content":"The new [Apache Ignite](https://ignite.apache.org/) 2.11 was released on September 17, 2021. It can be considered to be a greater extent as a stabilization release that closed a number of technical debts of the internal architecture and bugs. Out of more than 200 completed tasks, 120 are bug fixes. However, some valuable improvements still exist, so let\'s take a quick look at them together.\\n\\n### Thin Clients\\n\\nPartition awareness is enabled by default in the 2.11 release and allows thin clients to send query requests directly to the node that owns the queried data. Without partition awareness, an application executes all queries and operations via a single server node that acts as a proxy for the incoming requests.\\n\\nThe support of [Continuous Queries](https://ignite.apache.org/docs/latest/thin-clients/java-thin-client#cache-entry-listening) added to the java thin client. For the other supported features, you can check the [List of Thin Client Features](https://cwiki.apache.org/confluence/display/IGNITE/Thin+clients+features).\\n\\n\x3c!--truncate--\x3e\\n\\n### Cellular-clusters Deployment\\n\\nThe Apache Ignite internals has the so-called *switch* (a part of Partition Map Exchange) process that is used to perform atomic execution of cluster-wide operations and move a cluster from one consistent state to another, for example, a cache creation/destroy, a node JOIN/LEFT/FAIL operations, snapshot creation, etc. During the switching process, all user transactions are parked for a small period of time which in turn increases the average latency and decreases throughput of the overall cluster.\\n\\nSplitting the cluster into virtual cells containing 4-8 nodes may increase the total cluster performance and minimize the influence of one cell on another in case of node fail events. Such a technique also significantly increases the recovery speed of transactions on cells not affected by failing nodes. The time when transactions are parked also decreases on non-affected cells which in turn decreases the worst latency for the cluster operations overall.\\n\\nFrom now on, you can use the *RendezvousAffinityFunction* affinity function with *ClusterNodeAttributeColocatedBackupFilter* to group nodes into virtual cells. Since the node baseline attributes are used as cell markers the corresponding [BASELINE_NODE_ATTRIBUTES](https://ignite.apache.org/docs/latest/monitoring-metrics/system-views#baseline_node_attributes) system view was added.\\n\\nSee benchmarks below that represent the worst (max) latency, which happens in case of node left/failure/timeout events on broken and alive cells.\\n\\n[![Cellular clusters latency comparison](/img/blog/ec8a7800-01e9-4910-aaa9-0e27ea2d4303.png)](/img/blog/ec8a7800-01e9-4910-aaa9-0e27ea2d4303.png)\\n\\n### New Page Replacement Policies\\n\\nWhen Native Persistence is on and the amount of data, which Ignite stores on the disk, is bigger than the off-heap memory amount allocated for the data region, another page should be evicted from the off-heap to the disk to preload a page from the disk to the completely full off-heap memory. This process is called page replacement. Previously, Apache Ignite used the Random-LRU page replacement algorithm which has a low maintenance cost, but it has many disadvantages and greatly affects the performance when the page replacement is started. On some deployments, administrators even force a cluster restart periodically to avoid page replacement. There are a few new algorithms available from now on:\\n\\n- Segmented-LRU Algorithm\\n- CLOCK Algorithm\\n\\nPage replacement algorithm can be configured by the *PageReplacementMode* property of *DataRegionConfiguration*. By default, the CLOCK algorithm is now used. You can check the [Replacement Policies](https://ignite.apache.org/docs/latest/memory-configuration/replacement-policies) in the documentation for more details.\\n\\n### Snapshot Restore And Check Commands\\n\\n#### Check\\n\\nAll snapshots are fully consistent in terms of concurrent cluster-wide operations as well as ongoing changes with Ignite. However, in some cases and for your own peace of mind, it may be necessary to check the snapshot for completeness and for data consistency. The Apache Ignite is now delivered with a built-in snapshot consistency check commands that enable you to verify internal data consistency, calculate data partitions hashes and pages checksums, and print out the result if a problem is found. The check command also compares hashes calculated by containing keys of primary partitions with corresponding backup partitions and reports any differences.\\n\\n```\\n# This procedure does not require the cluster to be in the idle state.\\ncontrol.(sh|bat) --snapshot check snapshot_name\\n```\\n\\n#### Restore\\n\\nPreviously, only the manual snapshot restore procedure was available by fully copying persistence data files from the snapshot directory to the Apache Ignite *work* directory. The automatic restore procedure allows you to restore cache groups from a snapshot on an active cluster by using the Java API or command line script (using CLI is recommended). Currently, the restore procedure has several limitations, so please check the documentation pages for details.\\n\\n```\\nStart restoring all user-created cache groups from the snapshot \\"snapshot_09062021\\".\\ncontrol.(sh|bat) --snapshot restore snapshot_09062021 --start\\n\\n# Start restoring only \\"cache-group1\\" and \\"cache-group2\\" from the snapshot \\"snapshot_09062021\\".\\ncontrol.(sh|bat) --snapshot restore snapshot_09062021 --start cache-group1,cache-group2\\n\\n# Get the status of the restore operation for \\"snapshot_09062021\\".\\ncontrol.(sh|bat) --snapshot restore snapshot_09062021 --status\\n\\n# Cancel the restore operation for \\"snapshot_09062021\\".\\ncontrol.(sh|bat) --snapshot restore snapshot_09062021 --cancel\\n```"},{"id":"/2021/09/14/apache-ignite-momentum-highlights-from","metadata":{"permalink":"/suggested-site/blog/2021/09/14/apache-ignite-momentum-highlights-from","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2021-09-14-apache-ignite-momentum-highlights-from.mdx","source":"@site/blog/2021-09-14-apache-ignite-momentum-highlights-from.mdx","title":"Apache Ignite Momentum: Highlights from 2020-2021","description":"When Apache Ignite entered the Apache Software Foundation (ASF) Incubator in 2014, it took less than a year for the project and its community to graduate from the Incubator and become a top-level project for the ASF. Since then, Ignite has experienced a significant and steady growth in popularity, and it has been used by thousands of application developers and architects to create high-performance and scalable applications used by millions of people daily. In this article, we\'ll recap the achievements of Ignite in 2020-2021.","date":"2021-09-14T00:00:00.000Z","tags":[{"inline":true,"label":"bigdata","permalink":"/suggested-site/blog/tags/bigdata"},{"inline":true,"label":"computing","permalink":"/suggested-site/blog/tags/computing"},{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"in-memory","permalink":"/suggested-site/blog/tags/in-memory"},{"inline":true,"label":"open","permalink":"/suggested-site/blog/tags/open"},{"inline":true,"label":"source","permalink":"/suggested-site/blog/tags/source"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":3.92,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite Momentum: Highlights from 2020-2021","authors":["denis"],"date":"2021-09-14T00:00:00.000Z","tags":["bigdata","computing","database","ignite","in-memory","open","source","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.11: Stabilization First","permalink":"/suggested-site/blog/2021/09/20/apache-ignite-2-11-stabilization"},"nextItem":{"title":"Apache Ignite 2.10: Thin Client Expansion","permalink":"/suggested-site/blog/2021/03/18/apache-ignite-2-10-thin"}},"content":"When Apache Ignite entered the Apache Software Foundation (ASF) Incubator in 2014, it took less than a year for the project and its community to graduate from the Incubator and become a top-level project for the ASF. Since then, Ignite has experienced a significant and steady growth in popularity, and it has been used by thousands of application developers and architects to create high-performance and scalable applications used by millions of people daily. In this article, we\'ll recap the achievements of Ignite in 2020-2021.\\n\\n### Ignite is Ranked as a Top 5 Project\\n\\nThe ASF has ranked Apache ignite as a Top 5 project in various categories since 2017. [That year](https://blogs.apache.org/foundation/entry/apache-in-2017-by-the), Ignite was in the Top 5 of Apache Project Repositories by Commits and most active Apache mailing lists. [Today](https://www.apache.org/foundation/docs/FY2021AnnualReport.pdf), the momentum continues, and Ignite continues to be ranked as a Top 5 project in multiple categories: second on the Top 5 big data user lists, third on the Top 5 big data dev lists, second on the Top 5 of all user lists, third on the Top 5 repos by size.\\n\\n[![Apache Ignite Rankings](/img/blog/463ae37d-866d-4c15-8ebf-6b88499daa81.jpeg)](/img/blog/463ae37d-866d-4c15-8ebf-6b88499daa81.jpeg)\\n\\n\x3c!--truncate--\x3e\\n\\nOf greatest significance, the continued Top 5 ranking on the \\"dev list\\" reflects an active community of contributors who are committed to keeping the code base growing, while the Top 5 ranking on the \\"user list\\" means that more and more Ignite application developers come to the community to ask questions, indicating continued growth in adoption.\\n\\n### The Worldwide Ignite community is Engaged\\n\\nThis broad and growing interest in Apache Ignite has continued over the last year and a half. However, faced with the pandemic and shelter-in-place orders around the world, the community sought ways to stay in touch and continue sharing experiences. The community naturally turned to a virtual format and established two new successful programs.\\n\\nThe first was a series of [Ignite Virtual Meetups](https://www.meetup.com/Apache-Ignite-Virtual-Meetup/events/past/), where Apache Ignite users, developers, committers, contributors and architects worldwide could share experiences on a wide range of topics, ask questions, and help drive the project forward. Since these virtual meetups began, the community has already held 17 events, which were attended by hundreds of community members and developers.\\n\\nThe second new program was launched this May with the virtual [Ignite Summit](https://ignite-summit.org/2021/), the first global conference designed for the entire Ignite community. [Twenty-five speakers from industry-leading companies](https://www.youtube.com/playlist?list=PLMc7NR20hA-KF8c_hVICKpzKnWkjzfC2V) including finance, biotech, health & fitness, construction and cloud computing led 15 hours of discussion about how Apache Ignite delivers the performance and scale required to address the world\'s most challenging computational and hybrid transactional/analytical processing requirements. The Summit had attendees from North America, Latin America, EMEA and APAC. Remarkably, attendees spent an average of nearly 5 hours at the event!\\n\\n### Innovation Continues at a Rapid Pace\\n\\nOver the last year and a half, the community has released [five new versions of Ignite 2.x](https://ignite.apache.org/download). The releases introduce numerous improvements and optimizations, including major features, such as new monitoring and profiling frameworks, cluster snapshots, encoding keys rotation for transparent data encryption, and more.\\n\\nThe community also put significant effort into contributing and releasing [new documentation](https://ignite.apache.org/docs/), which is now hosted on the Ignite website. Since the new documentation was posted, it has become the most visited resource on the website, a clear indication that it is helping Ignite developers make faster, easier progress on their Ignite development and optimization tasks.\\n\\nFurther, Igniters have begun working on the next major release, Ignite 3.0, which introduces significant usability improvements, a new SQL engine based on Apache Calcite, a Raft-based consistency protocol, and many other improvements. Users can already try the first two Alpha versions:\\n\\n- [Alpha 1 Overview](https://www.gridgain.com/resources/blog/ignite-3-alpha-sneak-peek-future-apache-ignite)\\n- [Alpha 2 Overview](https://www.gridgain.com/resources/blog/just-released-apache-ignite-3-alpha-2)\\n\\n### The payoff: Ignite Downloads Continue to Soar\\n\\nThe inherent benefits of Apache Ignite, combined with all the effort of a dedicated community, has resulted in a popular project that continues to see increasing adoption. Ignite Maven monthly downloads are skyrocketing, and we have seen a 65% year-over-year growth in downloads so far in 2021, resulting in hundreds of thousands of downloads each month.\\n\\nWe eagerly look forward to the full release of Apache ignite 3.0 and fully expect downloads, adoption and community enthusiasm to continue to soar. Good luck to the Ignite community!"},{"id":"/2021/03/18/apache-ignite-2-10-thin","metadata":{"permalink":"/suggested-site/blog/2021/03/18/apache-ignite-2-10-thin","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2021-03-18-apache-ignite-2-10-thin.mdx","source":"@site/blog/2021-03-18-apache-ignite-2-10-thin.mdx","title":"Apache Ignite 2.10: Thin Client Expansion","description":"As of March 15, 2021, Apache Ignite 2.10 has been released. You can directly check the full list of resolved Important JIRA\'s but here let\'s briefly overview some valuable improvements.","date":"2021-03-18T00:00:00.000Z","tags":[{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":3.11,"hasTruncateMarker":true,"authors":[{"name":"Maxim Muzafarov","title":"Apache Ignite Committer","url":"https://github.com/mmuzaf","imageURL":"/suggested-site/img/authors/mmuzaf.png","key":"maxim","page":null}],"frontMatter":{"title":"Apache Ignite 2.10: Thin Client Expansion","authors":["maxim"],"date":"2021-03-18T00:00:00.000Z","tags":["ignite","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite Momentum: Highlights from 2020-2021","permalink":"/suggested-site/blog/2021/09/14/apache-ignite-momentum-highlights-from"},"nextItem":{"title":"Apache Ignite 2.9 Released: Cluster snapshots and tracing","permalink":"/suggested-site/blog/2020/11/05/apache-ignite-2-9-released"}},"content":"As of March 15, 2021, [Apache Ignite](https://ignite.apache.org/) 2.10 has been released. You can directly check the full list of resolved [Important JIRA\'s](https://s.apache.org/i3ny6) but here let\'s briefly overview some valuable improvements.\\n\\n### Thin Clients\\n\\nThin clients now support several important features which, previously were available only on the thick clients. Thin clients are always backward and forward compatible with the server nodes of the cluster, so the cluster upgrade process will be more convenient if the lack of these features prevented you from doing that.\\n\\nSee the list of what is changed for thin clients below:\\n\\n- Transactions\\n- Service invocations\\n- Continuous Queries\\n- SQL API\\n- Cluster API\\n- Cache Async API\\n- Kubernetes Discovery (*ThinClientKubernetesAddressFinder*)\\n\\nYou may check the [List of Thin Client Features](https://cwiki.apache.org/confluence/display/IGNITE/Thin+clients+features) that supported by platforms you are interested in or see the [What\'s new in Apache Ignite.NET 2.10](https://ptupitsyn.github.io/Whats-New-In-Ignite-Net-2.10/).\\n\\n\x3c!--truncate--\x3e\\n\\n### Cluster Monitoring\\n\\nApache Ignite self-monitoring and cluster health check subsystems are also extended by additional SQL-views and command line scripts.\\n\\n#### New *control-script* Commands\\n\\nQuery any of the available system views.\\n\\n```\\ncontrol.sh --system-view views\\nCommand [SYSTEM-VIEW] started\\n--------------------------------------------------------------------------------\\nname                           schema    description\\nSQL_QUERIES_HISTORY            SYS       SQL queries history.\\nINDEXES                        SYS       SQL indexes\\nBASELINE_NODES                 SYS       Baseline topology nodes\\nSTRIPED_THREADPOOL_QUEUE       SYS       Striped thread pool task queue\\nSCAN_QUERIES                   SYS       Scan queries\\nPARTITION_STATES               SYS       Distribution of cache group partitions across cluster nodes\\n\\nCommand [SYSTEM-VIEW] finished with code: 0\\n--------------------------------------------------------------------------------\\n```\\n\\nQuery any of the available system metrics.\\n\\n```\\ncontrol.sh --metric sysCurrentThreadCpuTime\\nCommand [METRIC] started\\n--------------------------------------------------------------------------------\\nmetric                          value\\nsys.CurrentThreadCpuTime        17270000\\nCommand [METRIC] finished with code: 0\\n--------------------------------------------------------------------------------\\n```\\n\\n[Read More](https://ignite.apache.org/docs/latest/tools/control-script)\\n\\n#### Managing Ignite System Properties\\n\\nIn addition to basic cluster configuration settings, you can perform some low-level cluster configuration and tuning via Ignite system properties. Run the command below to see the list of all available system properties for configuration:\\n\\n```\\n$./ ignite.sh -systemProps\\n\\n--------------------------------------------------------------------------------\\nIGNITE_AFFINITY_HISTORY_SIZE           - [Integer] Maximum size for affinity assignment history. Default is 25.\\nIGNITE_ALLOW_ATOMIC_OPS_IN_TX          - [Boolean] Allows atomic operations inside transactions. Default is true.\\nIGNITE_ALLOW_START_CACHES_IN_PARALLEL  - [Boolean] Allows to start multiple caches in parallel. Default is true.\\n```\\n\\n[Read more](https://ignite.apache.org/docs/latest/setup#setting-ignite-system-properties)\\n\\n### Cluster Profiling\\n\\nFrom now on, Apache Ignite is delivered with the cluster profiling tool. This tool collects and processes all cluster internal information about Queries, Compute Tasks, Cache operations, Checkpoint and WAL statistics, and so on for problem detection and cluster self-tuning purposes. Each cluster node collects performance statistics into a special binary file that is placed under the `[IGINTE_WORK_DIR]/perf_stat/` directory with the template filename as `node-[nodeId]-[index].prf`. All these files are consumed by offline-tool that builds the report in a human-readable format.\\n\\n[Read More](https://ignite.apache.org/docs/latest/monitoring-metrics/performance-statistics)\\n\\n![transactions statistics](/img/blog/d445a88f-98d1-4a6e-b4d8-037e819ca91f.png)\\n\\n### Transparent Data Encryption - Cache Key Rotation\\n\\nPayment card industry data security standard (PCI DSS) requires that key-management procedures include a predefined crypto period for each key in use and define a process for key changes at the end of the defined crypto period. An expired key should not be used to encrypt new data, but it can be used for archived data, such keys should be strongly protected (section 3.5 - 3.6 of PCI DSS Requirements and Security Assessment Procedures).\\n\\nApache Ignite now supports full PCI DSS requirements:\\n\\n- *Transparent Data Encryption* available since the 2.7 release.\\n- *Master Key Rotation* procedure available since the 2.9 release.\\n- *Cache Key Rotation* procedure available since the 2.10 release.\\n\\nYou may use the CLI tools that provide the ability to change the re-encryption rate as well as suspend and resume background re-encryption at runtime.\\n\\n[Read More](https://ignite.apache.org/docs/latest/security/cache-encryption-key-rotation)"},{"id":"/2020/11/05/apache-ignite-2-9-released","metadata":{"permalink":"/suggested-site/blog/2020/11/05/apache-ignite-2-9-released","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2020-11-05-apache-ignite-2-9-released.mdx","source":"@site/blog/2020-11-05-apache-ignite-2-9-released.mdx","title":"Apache Ignite 2.9 Released: Cluster snapshots and tracing","description":"As of October 23, 2020, Apache Ignite 2.9 is available. Like every other Ignite release, release 2.9 includes many changes. Let\'s take a look at the major features of release 2.9.","date":"2020-11-05T00:00:00.000Z","tags":[{"inline":true,"label":"two","permalink":"/suggested-site/blog/tags/two"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":1.54,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite 2.9 Released: Cluster snapshots and tracing","authors":["denis"],"date":"2020-11-05T00:00:00.000Z","tags":["two","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.10: Thin Client Expansion","permalink":"/suggested-site/blog/2021/03/18/apache-ignite-2-10-thin"},"nextItem":{"title":"Ignite 2.8 Released: Less Stress in Production and Advances in Machine Learning","permalink":"/suggested-site/blog/2020/03/11/ignite-2-8-released-less"}},"content":"As of October 23, 2020, Apache Ignite 2.9 is available. Like every other Ignite release, release 2.9 includes many changes. Let\'s take a look at the major features of release 2.9.\\n\\n### Cluster Snapshots\\n\\nIgnite 2.9 provides the ability to create full cluster snapshots for deployments that use Ignite Persistence. Snapshots can be taken online, when the cluster is active and accessible to users. An Ignite snapshot includes a cluster-wide copy of all data records that exist at the moment the snapshot is started. All snapshots are consistent, in terms of concurrent, cluster-wide operations as well as in terms of ongoing changes in Ignite Persistence data, index, schema, binary metadata, marshaller, and other files on nodes. See [Ignite documentation](https://ignite.apache.org/docs/latest/persistence/snapshots) to learn about this feature.\\n\\n\x3c!--truncate--\x3e\\n\\n### Tracing\\n\\nThe Ignite monitoring system continues to improve. In Ignite 2.9, a new tracing subsystem became available. Tracing provides information that is useful for debugging, that helps with both regular, daily, basic system monitoring and with incident analysis. You can collect distributed traces of tasks that are executed in your cluster and use this information to diagnose latency problems. In the 2.9 release, the following Ignite components are instrumented for tracing:\\n\\n- Discovery\\n- Communication\\n- Exchange\\n- Transactions\\n\\nSee [the documentation](https://ignite.apache.org/docs/latest/monitoring-metrics/tracing) for more information.\\n\\n### Other Changes\\n\\nIn addition to snapshots and tracing, Ignite 2.9 provides the following new features:\\n\\n- Cluster discovery, cluster API, compute API, and service invocation support for thin clients (Java and .Net)\\n- Cluster-wide, read-only mode\\n- Ability to run user-defined code inside the Ignite sandbox\\n- Transparent data encryption: master key rotation\\n- Management tools to cancel user tasks and queries\\n- Platform cache (.Net)\\n\\nSee the [release notes](https://ignite.apache.org/releases/ignite2/2.9.0/release_notes.html) to learn about all of the new features.\\n\\nSincerely yours,\\nIgnite contributors and committers"},{"id":"/2020/03/11/ignite-2-8-released-less","metadata":{"permalink":"/suggested-site/blog/2020/03/11/ignite-2-8-released-less","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2020-03-11-ignite-2-8-released-less.mdx","source":"@site/blog/2020-03-11-ignite-2-8-released-less.mdx","title":"Ignite 2.8 Released: Less Stress in Production and Advances in Machine Learning","description":"With thousands of changes contributed to Apache Ignite 2.8 that enhanced almost all the components of the platform, it\'s possible to overlook some of the improvements that can convince you to upgrade to this version sooner than later. While a quick check of the release notes will help to discover anticipated bug fixes, this article aims to guide through enhancements every Ignite developer should be aware of.","date":"2020-03-11T00:00:00.000Z","tags":[{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"in-memory","permalink":"/suggested-site/blog/tags/in-memory"},{"inline":true,"label":"learning","permalink":"/suggested-site/blog/tags/learning"},{"inline":true,"label":"machine","permalink":"/suggested-site/blog/tags/machine"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":5.04,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Ignite 2.8 Released: Less Stress in Production and Advances in Machine Learning","authors":["denis"],"date":"2020-03-11T00:00:00.000Z","tags":["database","ignite","in-memory","learning","machine","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.9 Released: Cluster snapshots and tracing","permalink":"/suggested-site/blog/2020/11/05/apache-ignite-2-9-released"},"nextItem":{"title":"Apache Ignite 2.7: Deep Learning and Extended Languages Support","permalink":"/suggested-site/blog/2018/12/13/apache-ignite-2-7-deep"}},"content":"With thousands of changes contributed to Apache Ignite 2.8 that enhanced almost all the components of the platform, it\'s possible to overlook some of the improvements that can convince you to upgrade to this version sooner than later. While a quick check of the [release notes](https://ignite.apache.org/releases/ignite2/2.8.0/release_notes.html) will help to discover anticipated bug fixes, this article aims to guide through enhancements every Ignite developer should be aware of.\\n\\n### New Subsystem for Production Monitoring and Tracing\\n\\nSeveral months of constant work on [IEP-35: Monitoring & Profiling](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=112820392) has resulted in the creation of a robust and elastic subsystem for production monitoring and diagnostic (aka. profiling). This was influenced by the needs of many developers who deployed Ignite in critical environments and were asking for a foundation that can be integrated with many external monitoring tools and be expanded easily.\\n\\nThe [new subsystem](https://apacheignite.readme.io/docs/new-metrics#section-exporters) consists of several registries that group individual metrics related to a specific Ignite component. For instance, you will find registries for cache, compute, or service grid APIs. Since the registries are designed to be generic, specific exporters can observe the state of Ignite via a myriad of tools supporting various protocols. By default, Ignite 2.8 introduces exporters for monitoring interfaces such as log files, JMX and SQL views, and contemporary ones such as OpenCensus.\\n\\n\x3c!--truncate--\x3e\\n\\nPresently, this new subsystem is released in an experimental mode only to give Ignite users some time to check the new API and suggest any improvements. Since the developer community is already impatient to remove the experimental flag, don\'t delay!\\n\\n### Advances in Ignite Machine Learning\\n\\nMachine Learning (ML) capabilities of Ignite 2.8 are so drastically different from previous versions that if you\'ve been waiting for the best moment to use the API, then the time has come. Let\'s scratch the surface here and learn more details from the updated documentation pages.\\n\\nA model training is usually a multi-step process that goes with preprocessing, training, and evaluation/valuation phases. A new [pipelining API](https://apacheignite.readme.io/docs/evaluation) puts things in order by combining all the phases in a single workflow.\\n\\nIn addition to the pipelining APIs, Ignite 2.8 introduced [ensemble methods](https://apacheignite.readme.io/docs/ensemble-methods), which allow combining several machine learning techniques into one predictive model to decrease variance (bagging) and bias (boosting), or improve predictions (stacking).\\n\\nFurthermore, now you can import [Apache Spark or XGBoost models](https://apacheignite.readme.io/docs/model-importing) to Ignite for further inference, pipelining other tasks. Feel free to keep training a model with your favorite framework and convert it to Ignite representation once the model needs to be deployed in production and executed at scale.\\n\\n### Beyond Java: Partition-Awareness and Other Changes\\n\\nEven though Ignite is a Java middleware, it functions as a cross-platform database and compute platform that is used for applications developed in C#, C++, Python, and other programming languages.\\n\\nThin client protocol is a real enabler for other programming languages support, and with Ignite 2.8, it got a significant performance optimization by supporting [partition-awareness](https://apacheignite-net.readme.io/docs/thin-client#section-partition-awareness). The latter allows thin clients to send query requests directly to nodes that own the queried data. Without partition awareness, an application that is connected to the cluster via a thin client executes all queries and operations via a single server node that acts as a proxy for the incoming requests.\\n\\nCheck the [detailed blog](https://ptupitsyn.github.io/Whats-New-In-Ignite-Net-2.8/) post by Pavel Tupitsyn, Ignite committer and PMC, who elaborates on the partition-awareness feature and introduces other .NET-specific enhancements.\\n\\n### Less Stress in Production\\n\\nThis section lists top improvements that might not have striking or catchy names but can bring relief by automating and optimizing things, and by avoiding data inconsistencies when you are already in production.\\n\\nThe stop-the-world pauses triggered by Java garbage collectors impact performance, responsiveness, and throughput of our Java applications. Apache Ignite has a partition-map-exchange (PME) process that, as Java garbage collectors, has some phases that put on hold all running operations for the sake of cluster-wide consistency. For most of the Ignite usage scenarios, these phases complete promptly and are unnoticed. However, some low-latency or high-throughput use cases can detect a decline that might impact some business operations for a moment in time. [This wiki page](https://cwiki.apache.org/confluence/display/IGNITE/%28Partition+Map%29+Exchange+-+under+the+hood) lists all the conditions that can trigger a distributed PME, and with Ignite 2.8, some of them were taken off the list -- the blocking PME no longer happens if a node belonging to the current baseline topology leaves the cluster or a thick client connects to it.\\n\\nNext, we all know that things break, and what really matters is how a system handles failures. With Ignite 2.8, we revisited the way the cluster handles crash recoveries on restarts while replaying write-ahead-logs (check [IGNITE-7196](https://issues.apache.org/jira/browse/IGNITE-7196) and [IGNITE-9420](https://issues.apache.org/jira/browse/IGNITE-9420)). Also, the [read-repair feature](https://apacheignite.readme.io/docs/read-repair) was added to manage data inconsistencies between primary and backups copies of the cluster on-the-fly.\\n\\nFurthermore, it\'s worth mentioning that Ignite 2.8 became more prudent about disk space consumption by supporting the compaction of data files and write-ahead-logs of the native persistence. By sacrificing a bit more CPU cycles for the needs of [compaction algorithms](https://ignite.apache.org/releases/ignite2/2.8.0/javadoc/org/apache/ignite/configuration/DiskPageCompression.html), you can save a lot on the storage end.\\n\\nLast but not least, is an [auto-baseline](https://apacheignite.readme.io/docs/baseline-topology#section-baseline-topology-autoadjustment) feature that changes a cluster topology for deployments with Ignite native persistence without the need for your intervention in many scenarios. Check this documentation page for more details.\\n\\nReach out to us on the community user list for more questions, details, and feedback.\\n\\nSincerely yours,\\nIgnite contributors and committers"},{"id":"/2018/12/13/apache-ignite-2-7-deep","metadata":{"permalink":"/suggested-site/blog/2018/12/13/apache-ignite-2-7-deep","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2018-12-13-apache-ignite-2-7-deep.mdx","source":"@site/blog/2018-12-13-apache-ignite-2-7-deep.mdx","title":"Apache Ignite 2.7: Deep Learning and Extended Languages Support","description":"Deep Learning With TensorFlow","date":"2018-12-13T00:00:00.000Z","tags":[{"inline":true,"label":"two","permalink":"/suggested-site/blog/tags/two"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":2.12,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite 2.7: Deep Learning and Extended Languages Support","authors":["denis"],"date":"2018-12-13T00:00:00.000Z","tags":["two","archived"]},"unlisted":false,"prevItem":{"title":"Ignite 2.8 Released: Less Stress in Production and Advances in Machine Learning","permalink":"/suggested-site/blog/2020/03/11/ignite-2-8-released-less"},"nextItem":{"title":"Apache Ignite 2.5: Scaling to 1000s Nodes Clusters","permalink":"/suggested-site/blog/2018/05/31/apache-ignite-2-5-scaling"}},"content":"### Deep Learning With TensorFlow\\n\\nEven though it was natural to provide machine learning algorithms in Ignite out of the box, another direction was taken for deep learning capabilities. Primarily because machine learning approaches have already been adopted in businesses from big to small, while deep learning is still being used for narrow and specific use cases.\\n\\nThus, Ignite 2.7 can boast about an [official integration](https://ignite.apache.org/features/tensorflow.html) with TensorFlow deep learning framework that gives a way to use Ignite as a distributed storage for TensorFlow calculations. With Ignite, data scientists can store unlimited data sets across a cluster, gain performance improvements and rely on fault-tolerance of both products if an algorithm fails in the middle of an execution.\\n\\n\x3c!--truncate--\x3e\\n\\n### Extended Languages Support - Node.JS, Python, PHP\\n\\nJava, .NET and C++ have been extensively supported by Ignite for a while now. But until now, when it came to other languages, developers had to fall back to REST, JDBC/ODBC calls. To address the limitation of missing native APIs for programming languages different from the three above, the community released a low-level binary protocol used to build thin clients. A thin client is a lightweight Ignite client that connects to the cluster via a standard socket connection.\\n\\nBased on this protocol, Ignite 2.7 adds support for [Node.JS, Python and PHP](https://apacheignite.readme.io/docs/thin-clients). As for Java, .NET and C++, you can leverage from thin clients, as well, if the regular clients are not suitable for some reason.\\n\\n### Transparent Data Encryption\\n\\nFor those of you who are using Ignite persistence in production, this functionality brings peace of mind. Whether you store any sensitive information or an entire data set has to be encrypted due to regulations, this feature is what you need. Check [this page](https://apacheignite.readme.io/docs/transparent-data-encryption) for more details.\\n\\n### Transactional SQL Beta\\n\\nLast, but probably the most anticipated addition to Ignite, is fully transactional SQL. You\'re no longer limited to key-value APIs if an application needs to run ACID-compliant distributed transactions. Prefer SQL? Use SQL! Yes, it\'s still in beta and might not yet be the best fit for mission-critical deployments, but definitely try it in your development cycles and share your feedback. It took us several years to reach this milestone and before GA release comes out, we want to hear what you think.\\n\\nFinally, I have no more paper left to cover other optimizations and improvements. So, go ahead and check out our [release notes](https://ignite.apache.org/releases/ignite2/2.7.0/release_notes.html)."},{"id":"/2018/05/31/apache-ignite-2-5-scaling","metadata":{"permalink":"/suggested-site/blog/2018/05/31/apache-ignite-2-5-scaling","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2018-05-31-apache-ignite-2-5-scaling.mdx","source":"@site/blog/2018-05-31-apache-ignite-2-5-scaling.mdx","title":"Apache Ignite 2.5: Scaling to 1000s Nodes Clusters","description":"Apache Ignite was always appreciated by its users for two primary things it delivers - scalability and performance. Throughout the lifetime many distributed systems tend to do performance optimizations from a release to release while making scalability related improvements just a couple of times. It\'s not because the scalability is of no interest. Usually, scalability requirements are set and solved once by a distributed system and don\'t require significant additional interventions by engineers.","date":"2018-05-31T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"spark","permalink":"/suggested-site/blog/tags/spark"},{"inline":true,"label":"sql","permalink":"/suggested-site/blog/tags/sql"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":5.29,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite 2.5: Scaling to 1000s Nodes Clusters","authors":["denis"],"date":"2018-05-31T00:00:00.000Z","tags":["apache","database","ignite","spark","sql","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.7: Deep Learning and Extended Languages Support","permalink":"/suggested-site/blog/2018/12/13/apache-ignite-2-7-deep"},"nextItem":{"title":"Apache Ignite 2.4 Brings Advanced Machine Learning and Spark DataFrames Capabilities","permalink":"/suggested-site/blog/2018/03/15/apache-ignite-2-4-brings"}},"content":"Apache Ignite was always appreciated by its users for two primary things it delivers - scalability and performance. Throughout the lifetime many distributed systems tend to do performance optimizations from a release to release while making scalability related improvements just a couple of times. It\'s not because the scalability is of no interest. Usually, scalability requirements are set and solved once by a distributed system and don\'t require significant additional interventions by engineers.\\n\\nHowever, Apache Ignite grew to the point when the community decided to revisit its discovery subsystem that influences how well and far Ignite scales out. The goal was pretty clear - Ignite has to scale to 1000s of nodes as good as it scales to 100s now.\\n\\nIt took many months to get the task implemented. So, please join me in welcoming Apache Ignite 2.5 that now can be scaled easily to 1000s of nodes and goes with other exciting capabilities. Let\'s check out the most prominent ones.\\n\\n\x3c!--truncate--\x3e\\n\\n### Massive Scalability\\n\\nThere are two components of Ignite that were modified in Ignite 2.5 to improve its scalability capabilities. The first one is related to 1000s nodes clusters while the other is related to the way we train machine learning (ML) models in Ignite. Let\'s start with the first.\\n\\n#### Marrying Apache Ignite and ZooKeeper\\n\\nRight, that 1000s nodes scalability goal was solved with the help of Apache ZooKeeper. Why did we turn to it?\\n\\nApache Ignite default TCP/IP Discovery organizes cluster nodes into a ring-topology form that has its advantages and disadvantages. For instance, on topologies with hundreds of cluster nodes, it can take many seconds why a system message traverse through all the nodes. As a result, necessary processing of events such as joining of new nodes or detecting of failed ones can take a while affecting overall cluster responsiveness and performance. That is a big deal if you\'d like to run 1000s nodes clusters.\\n\\nThe new ZooKeeper Discovery uses ZooKeeper as a single point of synchronization where Ignite nodes are exchanging discovery events through it. It solved the issue with long-to-be-processed discovery messages and, as a result, allowed Ignite scaling to large cluster topologies.\\n\\n[![ZooKeeper Discovery](/img/blog/4b80632d-232d-4e4f-bd5e-9d91f0bc550f.png)](https://apacheignite.readme.io/docs/zookeeper-discovery)\\n\\nAs a rule of thumb, keep using default [TCP/IP Discovery](https://apacheignite.readme.io/docs/tcpip-discovery) if it\'s unlikely that your Ignite cluster scales beyond 300s nodes and switch to [ZooKeeper Discovery](https://apacheignite.readme.io/docs/zookeeper-discovery) if that\'s the case.\\n\\n#### Machine Learning: Partition-Based Datasets\\n\\nThat\'s the second prominent feature of Ignite 2.5 that improves the way of how far you can scale your Ignite clusters to train ML models over terabytes or petabytes of data. The [partition-based datasets](https://apacheignite.readme.io/docs/ml-partition-based-dataset) moved us closer to the implementation of Zero-ETL concept which implies that Ignite can be used as a single storage where ML models and algorithms are being improved iteratively and online without ETLing data back and forth between Ignite and another storage.\\n\\nRead more about the datasets from [this](https://apacheignite.readme.io/docs/ml-partition-based-dataset) documentation page.\\n\\n### Genetic Algorithms\\n\\nIgnite\'s ML component is ramping up and in the version 2.5 it accepted a contribution of genetic algorithms (GAs) which help to solve optimization problems by simulating the process of biological evolution. GAs are excellent for searching through large and complex data sets for an optimal solution. Real world applications of GAs include automotive design, computer gaming, robotics, investments, traffic/shipment routing and more.\\n\\nRefer to excessive articles of my community-mates Turik Campbell and Akmal B. Chaudhri which cover main benefits of GAs:\\n\\n- [Travel Like MacGyver: Solve Knapsack Problem with GA Grid](https://www.linkedin.com/pulse/travel-like-macgyver-solve-knapsack-problem-ga-grid-turik-campbell/)\\n- [Genetic Algorithms with Apache Ignite](https://www.gridgain.com/resources/blog/genetic-algorithms-apacher-ignitetm)\\n\\n### Continuous Self-Healing and Consistency Checks\\n\\nIt\'s a known fact that many companies and businesses trusted Ignite its mission-critical deployments and solutions. As a result, sometimes Ignite doesn\'t even have a right to \\"misfire\\" and should be able to handle critical or unpredictable situations automatically or provide facilities to do deal with them manually.\\n\\nWith Ignite 2.5, we\'ve kicked off the realization of continuous self-healing concept that implies that no matter what happens with Ignite in production it should be able to tolerate unexpected failures and stay up and running. The following was done in 2.5:\\n\\n- [Critical Failures Handling](https://apacheignite.readme.io/docs/critical-failures-handling)\\n- [Long running transactions monitoring and termination](https://apacheignite.readme.io/docs/transactions#section-long-running-transactions-termination)\\n- [Data Consistency Check Facilities](https://apacheignite.readme.io/docs/consistency-check-facilities)\\n\\n### SQL: Security and Fast Data Loading\\n\\nThe community stays strong and determined in its goal of making Ignite SQL engine undistinguishable from SQL engines of famous and mature SQL database. What\'s the purpose? We want to make it easy for you to migrate from a relational database to Ignite, so that you can reuse all your skills gained before. Overall, this is what our SQL engine got in 2.5:\\n\\n- Fast data loading with [COPY](https://apacheignite-sql.readme.io/docs/copy) command and [streaming mode](https://apacheignite-sql.readme.io/docs/jdbc-driver#section-streaming) using SQL APIs.\\n- [Long running transactions monitoring and termination](https://apacheignite.readme.io/docs/transactions#section-long-running-transactions-termination)\\n- Secured Ignite cluster. Use [CREATE USER, DROP USER and ALTER USER](https://apacheignite-sql.readme.io/docs/ddl) commands to manage who is allowed to connect to your clusters.\\n\\n### In-place Execution of Spark DataFrame Queries\\n\\nApache Spark users can applaud because the [following ticket](https://issues.apache.org/jira/browse/IGNITE-7077) got merged in 2.5. In short, it means that from now on Ignite will be able to execute as many DataFrames SQL queries as it can in-place on Ignite servers side avoiding data movement from Ignite to Spark. The performance of your DataFrames queries should boost significantly. Enjoy!\\n\\n### DEB and RPM packages\\n\\nLast but not least, if you\'re a Linux user, now you can install the latest Ignite versions directly from DEB and RPM repositories. Refer to [how-to](https://apacheignite.readme.io/docs/getting-started#section-rpm-deb-packages-installation) and share your feedback with us.\\n\\nFinally, I have no more paper left to cover other optimizations and improvements. So, go ahead and check out our [release notes](https://ignite.apache.org/releases/ignite2/2.5.0/release_notes.html)."},{"id":"/2018/03/15/apache-ignite-2-4-brings","metadata":{"permalink":"/suggested-site/blog/2018/03/15/apache-ignite-2-4-brings","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2018-03-15-apache-ignite-2-4-brings.mdx","source":"@site/blog/2018-03-15-apache-ignite-2-4-brings.mdx","title":"Apache Ignite 2.4 Brings Advanced Machine Learning and Spark DataFrames Capabilities","description":"Usually, Ignite community rolls out a new version once in 3 months, but we had to make an exception for Apache Ignite 2.4 that consumed five months in total. We could easily blame Thanksgiving, Christmas and New Year holidays for the delay and would be forgiven, but, in fact, we were forging the release you can\'t simply pass by.","date":"2018-03-15T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"bigdata","permalink":"/suggested-site/blog/tags/bigdata"},{"inline":true,"label":"data","permalink":"/suggested-site/blog/tags/data"},{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"fast","permalink":"/suggested-site/blog/tags/fast"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"learning","permalink":"/suggested-site/blog/tags/learning"},{"inline":true,"label":"machine","permalink":"/suggested-site/blog/tags/machine"},{"inline":true,"label":"nodal","permalink":"/suggested-site/blog/tags/nodal"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":3.09,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite 2.4 Brings Advanced Machine Learning and Spark DataFrames Capabilities","authors":["denis"],"date":"2018-03-15T00:00:00.000Z","tags":["apache","bigdata","data","database","fast","ignite","learning","machine","nodal","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.5: Scaling to 1000s Nodes Clusters","permalink":"/suggested-site/blog/2018/05/31/apache-ignite-2-5-scaling"},"nextItem":{"title":"Meltdown and Spectre patches show negligible impact to Apache Ignite performance","permalink":"/suggested-site/blog/2018/01/30/meltdown-and-spectre-patches-show"}},"content":"Usually, Ignite community rolls out a new version once in 3 months, but we had to make an exception for Apache Ignite 2.4 that consumed five months in total. We could easily blame Thanksgiving, Christmas and New Year holidays for the delay and would be forgiven, but, in fact, we were forging the release you can\'t simply pass by.\\n\\nLet\'s dive in and search for a big fish.\\n\\n### Machine Learning General Availability\\n\\nEight months ago, at the time of Apache Ignite 2.0, we put out the first APIs that formed the foundation of the Ignite\'s machine learning component of today. Since that time, Ignite machine learning experts and enthusiasts have been moving the library to the general availability condition meticulously. And Ignite 2.4 became a milestone that let us consider the [ML Grid](https://apacheignite.readme.io/docs/machine-learning) to be production ready.\\n\\n\x3c!--truncate--\x3e\\n\\nThe component gained a variety of algorithms that can solve a myriad of regression and classification tasks, gave an ability to train models avoiding ETL from Ignite to other systems, paved a way to [deep learning](https://apacheignite.readme.io/docs/multilayer-perceptron) usage scenarios. All that now empowers Ignite users with the tools for dealing with fraud detection, predictive analytics, and for building recommendation systems...if you want. Note, ETL is optional, and the whole memory-centric cluster is at your service!\\n\\nMoreover, Machine Learning Grid welcomed a [software donation](http://incubator.apache.org/ip-clearance/ga-grid-ignite.html) by NetMillennium, Inc. in the form of genetic algorithms that solve optimization problems by simulating the process of biological evolution. The algorithms haven\'t got to Ignite 2.4 and waiting for their time for a release in the master branch. Once you get them, you can apply the biological evolution simulation for real-world applications including automotive design, computer gaming, robotics, investments, traffic/shipment routing and more.\\n\\n### Spark DataFrames\\n\\nIt\'s not a joke or misprint. Spark users, the DataFrames are now officially supported for you! Many of you have been anticipating them for years and, thanks to Nikolay Izhikov, who was \\"promoted\\" to an Ignite committer for the contribution, now you can leverage from them.\\n\\nNo need to be wordy here. Just go ahead and start with [DataFrames in Ignite](https://apacheignite-fs.readme.io/docs/ignite-data-frame).\\n\\n### Expanding Ignite ecosystem\\n\\nIt was unfair that only Java, C#, and C++ developers could utilize the breadth and depth of Ignite APIs in their applications. Ignite 2.4 solved the injustice with its new [low-level binary client protocol](https://apacheignite.readme.io/v2.4/docs/binary-client-protocol). The protocol communicates with an existing Ignite cluster without starting a full-fledged Ignite node. An application can connect to the cluster through a raw TCP socket from any programming language you like.\\n\\nThe beauty of the protocol is that you can develop a so-called Ignite thin client that is a lightweight client connected to the cluster and interacts with it using key-value, SQL, and other APIs. [.NET thin client](https://apacheignite-net.readme.io/docs/thin-client) is already at your service and Node.JS, Python, PHP, Java thin clients are in a forge and being developed for the next releases.\\n\\n### RPM repository and much more\\n\\nSo, now Apache Ignite can also be installed from the [official RPM repository](https://www.apache.org/dist/ignite/rpm). Debian users, the packages for your operating systems to be assembled [soon](https://cwiki.apache.org/confluence/display/IGNITE/IEP-11%3A+Introduce+Apache+Ignite+delivery+in+RPM+and+DEB+packages).\\n\\nOverall, if to list all the features and benefits Ignite 2.4 brings, only 2 people will read the article till the end - me and my dear mom :) Thus, I\'ll let you discover the rest from the [release notes](https://ignite.apache.org/releases/ignite2/2.4.0/release_notes.html)."},{"id":"/2018/01/30/meltdown-and-spectre-patches-show","metadata":{"permalink":"/suggested-site/blog/2018/01/30/meltdown-and-spectre-patches-show","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2018-01-30-meltdown-and-spectre-patches-show.mdx","source":"@site/blog/2018-01-30-meltdown-and-spectre-patches-show.mdx","title":"Meltdown and Spectre patches show negligible impact to Apache Ignite performance","description":"As promised in my initial blog post on this matter, Apache Ignite community applied security patches against the notorious Meltdown Spectre vulnerabilities and completed performance testing of general operations and workloads that are typical for Ignite deployments.","date":"2018-01-30T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"meltdown","permalink":"/suggested-site/blog/tags/meltdown"},{"inline":true,"label":"performance","permalink":"/suggested-site/blog/tags/performance"},{"inline":true,"label":"security","permalink":"/suggested-site/blog/tags/security"},{"inline":true,"label":"spectre","permalink":"/suggested-site/blog/tags/spectre"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":1.69,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Meltdown and Spectre patches show negligible impact to Apache Ignite performance","authors":["denis"],"date":"2018-01-30T00:00:00.000Z","tags":["apache","ignite","meltdown","performance","security","spectre","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.4 Brings Advanced Machine Learning and Spark DataFrames Capabilities","permalink":"/suggested-site/blog/2018/03/15/apache-ignite-2-4-brings"},"nextItem":{"title":"Protecting Apache Ignite from \'Meltdown\' and \'Spectre\' vulnerabilities","permalink":"/suggested-site/blog/2018/01/08/protecting-apache-ignite-from-meltdown"}},"content":"As promised in my [initial blog post](https://blogs.apache.org/ignite/entry/protecting-apache-ignite-from-meltdown) on this matter, Apache Ignite community applied security patches against the notorious Meltdown Spectre vulnerabilities and completed performance testing of general operations and workloads that are typical for Ignite deployments.\\n\\nThe security patches were applied only for [CVE-2017-5754](https://nvd.nist.gov/vuln/detail/CVE-2017-5754) (Meltdown) and [CVE-2017-5753](https://nvd.nist.gov/vuln/detail/CVE-2017-5753) (Spectre Variant 1) vulnerabilities. The patches for [CVE-2017-5715](https://nvd.nist.gov/vuln/detail/CVE-2017-5715) (Spectre Variant 2) for the hardware the community used for testing are not stable yet an can [cause system reboot issues or another unpredictable behavior](https://newsroom.intel.com/news/root-cause-of-reboot-issue-identified-updated-guidance-for-customers-and-partners/).\\n\\nThe applied patches have shown that the performance implications are negligible - the performance drop is just in the 0 - 7% range as the figure shows:\\n\\n\x3c!--truncate--\x3e\\n\\n![Spectre and Meltdown Benchmarks](https://www.gridgain.com/sites/default/files/inline-images/meltdown-benchmarks.png)\\n\\nThus, Apache Ignite community highly recommends its customers and partners to consider security patches for CVE-2017-5754 (Meltdown) and CVE-2017-5753 (Spectre Variant 1) in their deployment environments and contact us on the user list if you run into a larger performance drop in your use case.\\n\\nAt the same time, we\'re keeping an eye on Intel announcements and will validate the performance implications of Spectre Variant 2 once a solution is released by the hardware vendor.\\n\\nJust for your reference, the benchmarks were executed in the following environment and configuration.\\n\\n### Benchmarking Environment\\n\\nCluster Configuration:\\n\\n- 4 servers and 8 client nodes\\n- Apache Ignite version: 2.4.0\\n\\nHardware:\\n\\n- Huawei RH2288 V3, CPU - 2x Xeon E5-2609 v4, 1.7GHz, RAM - 96Gb, SSD - 3x800Gb RAID0 2.4Tb, Network - 10Gb/s\\n- DEll R610, CPU - 2x Xeon X5570, RAM - 96Gb, SSD - 512Gb, HDD - 2048GB, Network - 10Gb/s\\n\\nOperating System:\\n\\n- OS CentOS Linux release 7.4.1708 (Core)\\n- Kernel - Linux 3.10.0-693.11.6.el7.x86_64 #1 SMP Thu Jan 4 01:06:37 UTC 2018 x86_64"},{"id":"/2018/01/08/protecting-apache-ignite-from-meltdown","metadata":{"permalink":"/suggested-site/blog/2018/01/08/protecting-apache-ignite-from-meltdown","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2018-01-08-protecting-apache-ignite-from-meltdown.mdx","source":"@site/blog/2018-01-08-protecting-apache-ignite-from-meltdown.mdx","title":"Protecting Apache Ignite from \'Meltdown\' and \'Spectre\' vulnerabilities","description":"The world was rocked after the recent disclosure of the Meltdown and Spectre vulnerabilities that literally affect almost all software ever developed. Both issues are related to the way all modern CPUs are designed and this is why they have opened unprecedented security breaches, making the software, including Apache Ignite, vulnerable to hacker attacks.","date":"2018-01-08T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"meltdown","permalink":"/suggested-site/blog/tags/meltdown"},{"inline":true,"label":"security","permalink":"/suggested-site/blog/tags/security"},{"inline":true,"label":"spectre","permalink":"/suggested-site/blog/tags/spectre"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":2.58,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Protecting Apache Ignite from \'Meltdown\' and \'Spectre\' vulnerabilities","authors":["denis"],"date":"2018-01-08T00:00:00.000Z","tags":["apache","ignite","meltdown","security","spectre","archived"]},"unlisted":false,"prevItem":{"title":"Meltdown and Spectre patches show negligible impact to Apache Ignite performance","permalink":"/suggested-site/blog/2018/01/30/meltdown-and-spectre-patches-show"},"nextItem":{"title":"Apache Ignite Essentials: 2-part Webinar Series for Architects and Java Developers","permalink":"/suggested-site/blog/2017/11/17/apache-ignite-essentials-series-for"}},"content":"The world was rocked after the recent disclosure of the [Meltdown and Spectre](https://www.vox.com/business-and-finance/2018/1/4/16850004/meltdown-spectre-intel-security-flaw-update) vulnerabilities that literally affect almost all software ever developed. Both issues are related to the way all modern CPUs are designed and this is why they have opened unprecedented security breaches, making the software, including Apache Ignite, vulnerable to hacker attacks.\\n\\nThe vulnerabilities are registered in the National Vulnerability Database under the following CVEs:\\n\\n- [CVE-2017-5753](https://nvd.nist.gov/vuln/detail/CVE-2017-5753) - Spectre variant 1\\n- [CVE-2017-5715](https://nvd.nist.gov/vuln/detail/CVE-2017-5715) - Spectre variant 2\\n- [CVE-2017-5754](https://nvd.nist.gov/vuln/detail/CVE-2017-5754) - Meltdown\\n\\n\x3c!--truncate--\x3e\\n\\n## How to protect Apache Ignite deployments?\\n\\nFirst, the vulnerabilities can be fixed only on the operating system (OS) or hardware levels. All OS and hardware vendors are working on and releasing patches to fill-in the security breaches. Depending on the type of your Apache Ignite deployment, make sure to do the following:\\n\\n- **On-premise deployments** - apply the patches prepared by your OS and hardware vendors. Consult with them to find out additional steps to act on. [This page](https://www.us-cert.gov/ncas/alerts/TA18-004A) is a good place to start with.\\n- **Cloud deployments** - major cloud providers such as Amazon and Microsoft are in a process of patching their cloud computing services. Consider a cloud provider\'s security announcements and recommendations or follow up with a representative for suggestions.\\n\\nSecond, an Apache Ignite cluster becomes vulnerable to the attacks only if someone gets unauthorized access to cluster machines (both on-premise or cloud deployments) and executes a malicious shell script or connects to the cluster directly and executes a Java, .NET or C++ computation there.\\n\\nDo the following to prevent this from happening:\\n\\n- Make sure the cluster machines are secured with a hard-to-guess or hard-to-calculate password.\\n- Consider using 3rd party security components provided by enterprise vendors (such as [this one](https://docs.gridgain.com/docs/security-and-audit)) to strengthen a security shield of your deployments.\\n\\nFinally, researchers who discovered Meltdown and Spectre have said that the first issue can be fixed with software patches while the second can be fully addressed only with hardware upgrades/replacement. Luckily, it\'s much more difficult for hackers to exploit Spectre. Thus, if the two recommendations given above are taken seriously, the chances that you will be impacted from Spectre are low.\\n\\n## What is the performance impact of security patches?\\n\\nMany security patches are rolled out with a precaution that some of the applications can see up to a 30% performance degradation. Apache Ignite community is planning to measure the impact of general usage scenarios and will follow up with the results in a consequent post.\\n\\nThis general performance testing might not cover your use case. Therefore, it\'s highly recommended that you assess and test a possible performance drop of your Apache Ignite deployments before applying the patches in production. If the drop is significant, then contact us on the [dev list](https://ignite.apache.org/community/resources.html#mail-lists)."},{"id":"/2017/11/17/apache-ignite-essentials-series-for","metadata":{"permalink":"/suggested-site/blog/2017/11/17/apache-ignite-essentials-series-for","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2017-11-17-apache-ignite-essentials-series-for.mdx","source":"@site/blog/2017-11-17-apache-ignite-essentials-series-for.mdx","title":"Apache Ignite Essentials: 2-part Webinar Series for Architects and Java Developers","description":"We finally made this happen! I\'m happy to invite all of the software architects and engineers out there to a series of webinars that will introduce you to the fundamental capabilities of in-memory computing platforms such as Apache Ignite.","date":"2017-11-17T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":0.88,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite Essentials: 2-part Webinar Series for Architects and Java Developers","authors":["denis"],"date":"2017-11-17T00:00:00.000Z","tags":["apache","ignite","archived"]},"unlisted":false,"prevItem":{"title":"Protecting Apache Ignite from \'Meltdown\' and \'Spectre\' vulnerabilities","permalink":"/suggested-site/blog/2018/01/08/protecting-apache-ignite-from-meltdown"},"nextItem":{"title":"Apache Ignite 2.3 - More SQL and Persistence Capabilities","permalink":"/suggested-site/blog/2017/11/01/apache-ignite-2-3-more"}},"content":"We finally made this happen! I\'m happy to invite all of the software architects and engineers out there to a series of webinars that will introduce you to the fundamental capabilities of in-memory computing platforms such as Apache Ignite.\\n\\nThere will also be a mix of theory and practice. A lot of code examples are waiting to be shown so that you can apply the theory in practice right away.\\n\\nThe series consists of two parts.\\n\\n### [Part 1: Tuesday, November 21, 2017, 11:00am PT / 2:00pm ET](https://ignite.apache.org/events.html#in-memory-computing-essentials-architects-and-developers-part-1)\\n\\nTo be covered:\\n\\n- Cluster configuration and deployment.\\n- Distributed database internals (partitioning, replication).\\n- Data processing with key-value APIs.\\n- Affinity Collocation.\\n- Data processing with SQL.\\n\\n\x3c!--truncate--\x3e\\n\\n### [Part 2: Wednesday, December 13, 2017, 11:00am PT / 2:00pm ET](https://ignite.apache.org/events.html#in-memory-computing-essentials-architects-and-developers-part-2)\\n\\nTo be covered:\\n\\n- Collocated processing.\\n- Collocated processing for distributed computations.\\n- Collocated processing for SQL (distributed joins and more).\\n- Machine Learning.\\n- Memory Architecture.\\n\\nBook your seat!"},{"id":"/2017/11/01/apache-ignite-2-3-more","metadata":{"permalink":"/suggested-site/blog/2017/11/01/apache-ignite-2-3-more","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2017-11-01-apache-ignite-2-3-more.mdx","source":"@site/blog/2017-11-01-apache-ignite-2-3-more.mdx","title":"Apache Ignite 2.3 - More SQL and Persistence Capabilities","description":"Putting aside the regular bug fixes and performance optimizations, the Apache Ignite 2.3 release brings new SQL capabilities and Ignite persistence improvements that are worth mentioning.","date":"2017-11-01T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"bigdata","permalink":"/suggested-site/blog/tags/bigdata"},{"inline":true,"label":"database","permalink":"/suggested-site/blog/tags/database"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"key-value","permalink":"/suggested-site/blog/tags/key-value"},{"inline":true,"label":"sql","permalink":"/suggested-site/blog/tags/sql"},{"inline":true,"label":"store","permalink":"/suggested-site/blog/tags/store"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":2.53,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite 2.3 - More SQL and Persistence Capabilities","authors":["denis"],"date":"2017-11-01T00:00:00.000Z","tags":["apache","bigdata","database","ignite","key-value","sql","store","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite Essentials: 2-part Webinar Series for Architects and Java Developers","permalink":"/suggested-site/blog/2017/11/17/apache-ignite-essentials-series-for"},"nextItem":{"title":"Apache Ignite Community News (Issue 3)","permalink":"/suggested-site/blog/2017/09/15/apache-ignite-community-news-september"}},"content":"Putting aside the regular bug fixes and performance optimizations, the Apache Ignite 2.3 release brings new SQL capabilities and Ignite persistence improvements that are worth mentioning.\\n\\n### SQL\\n\\nLet\'s start with SQL first.\\n\\nApache Ignite users have consistently told us that despite all of Ignite\'s SQL capabilities, it\'s been at times challenging trying to figure out how to start using Ignite as an SQL database.\\n\\nThis was mostly caused by scattered documentation pages, lack of \\"getting started\\" guides and tutorials. We\'ve remedied this oversight! All related SQL knowledge has been curated in a [single documentation domain](https://apacheignite-sql.readme.io/docs).\\n\\n\x3c!--truncate--\x3e\\n\\nAre you curious about the SQL scope? Go to the new [SQL Reference Overview](https://apacheignite-sql.readme.io/docs/sql-reference-overview) section!\\n\\nCannot wait to learn how the Ignite SQL engine runs internally? We\'ve prepared an [Architectural Overview](https://apacheignite-sql.readme.io/docs/how-ignite-sql-works) section for you.\\n\\nSimply need to know how to connect to an Ignite cluster from an SQL tool? Here is a [tooling](https://apacheignite-sql.readme.io/docs/how-ignite-sql-works) section for you.\\n\\nLet\'s take a look at some specific SQL features released in Ignite 2.3.\\n\\nFirst, we\'re proud to deliver support of [ALTER TABLE](https://apacheignite-sql.readme.io/docs/alter-table) command. Presently, the command allows adding new columns to an SQL schema in runtime -- avoiding any cluster restarts. Once a new column is added, it can be turned into an index. Again, in runtime. No restarts!\\n\\nAnother significant addition seen in Ignite 2.3 is the integration with [SQLLine tool](https://apacheignite-sql.readme.io/docs/sqlline) that is bundled with every Apache Ignite release and can be used as a default command line tool for SQL based interactions.\\n\\nTo prove that it\'s fairly simple to work with Ignite as with an SQL database using the tool, we recorded a short screencast for you:\\n\\n[![SQLLine Demo](/img/blog/89364a0d-3c7e-427c-8237-868318dcd6f1.png)](https://youtu.be/FKS8A86h-VY)\\n\\n### Ignite Persistence\\n\\nIgnite native persistence keeps getting more attention and installs -- which is why the community released a feature requested by at least a dozen users. The feature allows enabling the persistence for specific data sets. Before Ignite version 2.3, the persistence could be enabled globally only.\\n\\nNow, it\'s up to you to decide which data to persist and which to store in RAM only. The persistence can be configured via [data regions](https://apacheignite.readme.io/docs/memory-configuration#section-data-regions) as shown below:\\n\\n[![Persistence Configuration](/img/blog/0c7ce964-1218-49eb-bc9b-54a4725cabed.png)](https://apacheignite.readme.io/docs/memory-configuration#section-data-regions)\\n\\nThis data region will consume up to 500 MB of RAM and will store a superset of data on disk ensuring that no data loss happens in case of a crash or even if there is no more space left in RAM.\\n\\n### Anything else?\\n\\nFlip through [our release notes](https://ignite.apache.org/releases/ignite2/2.3.0/release_notes.html) to see all the changes and improvements available in Apache Ignite 2.3 -- and, for sure, download and use this version in production.\\n\\nQuestions, comments? Let us know!"},{"id":"/2017/09/15/apache-ignite-community-news-september","metadata":{"permalink":"/suggested-site/blog/2017/09/15/apache-ignite-community-news-september","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2017-09-15-apache-ignite-community-news-september.mdx","source":"@site/blog/2017-09-15-apache-ignite-community-news-september.mdx","title":"Apache Ignite Community News (Issue 3)","description":"by Tom Diederich","date":"2017-09-15T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":3.81,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite Community News (Issue 3)","authors":["denis"],"date":"2017-09-15T00:00:00.000Z","tags":["apache","ignite","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.3 - More SQL and Persistence Capabilities","permalink":"/suggested-site/blog/2017/11/01/apache-ignite-2-3-more"},"nextItem":{"title":"Apache Ignite Community Update (August 2017 Issue)","permalink":"/suggested-site/blog/2017/08/30/apache-ignite-community-update-august"}},"content":"**by Tom Diederich**\\n\\nThis is our third community update. There\'s a lot going on, so let\'s get started.\\n\\nApache Ignite experts have already spoken at two meetups this month, both in Silicon Valley, but there are several more scheduled this month around the world.\\n\\nOn **Sept. 9** Apache Ignite PMC chair Denis Magda was the featured presenter at the **[Big Data and Cloud Meetup](https://www.meetup.com/datariders/events/242523245/)** in Santa Clara, Calif. His talk, titled \\"Apache Spark and Apache Ignite: Where Fast Data Meets the IoT,\\" was highly rated and we\'re planning a hands-on workshop with meetup organizers for November.\\n\\n\x3c!--truncate--\x3e\\n\\nOn **Sept. 13** Denis also spoke at the **[SF Big Analytics Meetup](https://www.meetup.com/SF-Big-Analytics/events/242368299/)** in Mountain View, Calif. Again, to a packed room. The topic of his talk was \\"Better Machine Learning with Apache Ignite.\\"\\n\\nBut Denis isn\'t having all the fun. Next Monday *(Sept. 18)*, technology evangelist Akmal Chaudhri will speak at the Camidge .NET User Group. The title of his talk: \\"**[Scale Out and Conquer: Apache Ignite for .NET Users](https://www.meetup.com/Camidge-NET-User-Group/events/238837204/)**.\\"\\n\\nWe\'re still looking for more meetups to speak at this month, so if you\'re an organizer or would like us to speak at one you\'re a member of, just let me know. In the meantime, here are the meetups planned for the remainder of September:\\n\\n- [Bay Area In-Memory Computing Meetup](https://www.meetup.com/preview/Bay-Area-In-Memory-Computing/events/242961495), Wednesday, Sept. 20 - Denis will present, \\"Apache Spark, Ignite and Flink: Where Fast Data Meets the IoT.\\"\\n- [Internet of Things (IoT) New York Meetup](https://www.meetup.com/preview/IoT-NY), Monday, Sept. 25 - Akmal will present, \\"Apache Spark and Apache Ignite: Where Fast Data Meets the IoT.\\"\\n- [NYC In-Memory Computing Meetup](https://www.meetup.com/preview/NYC-In-Memory-Computing-Meetup/events/243150945), Tuesday, Sept. 26 - Akmal will present, \\"Powering Up Banks and Financial Institutions with Distributed Systems.\\"\\n- [New York Kubernetes Meetup](https://www.meetup.com/preview/New-York-Kubernetes-Meetup/events/242597746), Wednesday, Sept. 27 - Akmal will provide a DevOps perspective on the orchestration of distributed databases and Apache Ignite.\\n\\nSee? I told you Denis wasn\'t having all the fun! Akmal is definitely on the road again.\\n\\n### Webinars\\n\\n- [Implementing In-Memory Computing for Financial Services Use Cases with Apache Ignite](http://bigcommunity.net/big-tv/) Sept 12 *(recording available)*\\n- [Better Machine Learning with Apache Ignite](https://www.gridgain.com/company/news/events/better-machine-learning-with-apacher-ignitetm), Wednesday, Sept. 27\\n\\n### Blog posts\\n\\nOn Sept. 5, Akmal published \\"[Using Java and .NET apps to connect to an Apache Ignite cluster](https://www.gridgain.com/resources/blog/using-java-and-net-apps-connect-apache-ignite-cluster), that details how to create an Apache Ignite cluster that can support the reading and writing of user-defined objects in a common storage format. This is particularly useful in situations where applications need to work with objects but these objects will be accessed by different programming languages and frameworks.\\n\\nOn Sept. 7, Dmitriy Setrakyan published \\"[Apache Ignite - In Memory Performance with Durability of Disk.](http://gridgain.blogspot.com/2017/09/apache-ignite-in-memory-performance.html)\\"\\n\\nNext up, on Sept. 12, was Akmal, who published \\"[Kubernetes and Apache Ignite Deployment on AWS](https://www.gridgain.com/resources/blog/kubernetes-and-apacher-ignitetm-deployment-aws).\\" That post walked through the steps required to get Kubernetes and Apache Ignite deployed on Amazon Web Services (AWS).\\n\\nAnd then on Sept. 13 Dmitriy published \\"[What is Apache Ignite](http://gridgain.blogspot.com/2017/09/what-is-apache-ignite.html).\\" I think the headline of that one is self-explanatory.\\n\\n### In the news\\n\\nNikita Ivanov is also an InfoWorld contributor. Read the first in his series on in-memory computing, \\"[Ensuring big data and fast data performance with in-memory computing](https://www.infoworld.com/article/3224449/big-data/ensuring-big-data-and-fast-data-performance-with-in-memory-computing.html#tk.rss_bigdata).\\"\\n\\n### Useful Resources\\n\\n- Stack Overflow. Stack Overflow is a question and answer site for professional and enthusiast programmers.\\n- Haaha. Haaha (also \\"Ha\\") (Russian: \u0425\u0430\u0431\u0440\u0430\u0445\u0430\u0431\u0440, \u0425\u0430\u0431\u0440) is a Russian collaborative blog with elements of social network about IT, Computer science and anything related to the Internet, owned by Thematic Media.\\n- [In-Memory Computing Planet](https://www.imcplanet.org/) (blogs and events) Add you blog feed!\\n- \\"Meetup in a Box.\\" If you would like to speak at a meetup, start or support a meetup, or have questions about meetups in general, let me know! I can help get you up and running with everything you\'ll need.\\n\\n*Please share any resources I\'ve excluded in the comments section and I\'ll include them in the next edition.*"},{"id":"/2017/08/30/apache-ignite-community-update-august","metadata":{"permalink":"/suggested-site/blog/2017/08/30/apache-ignite-community-update-august","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2017-08-30-apache-ignite-community-update-august.mdx","source":"@site/blog/2017-08-30-apache-ignite-community-update-august.mdx","title":"Apache Ignite Community Update (August 2017 Issue)","description":"by Tom Diederich","date":"2017-08-30T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"bigdata","permalink":"/suggested-site/blog/tags/bigdata"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"meetup","permalink":"/suggested-site/blog/tags/meetup"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":4.54,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite Community Update (August 2017 Issue)","authors":["denis"],"date":"2017-08-30T00:00:00.000Z","tags":["apache","bigdata","ignite","meetup","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite Community News (Issue 3)","permalink":"/suggested-site/blog/2017/09/15/apache-ignite-community-news-september"},"nextItem":{"title":"Apache Ignite 2.1 - A Leap from In-Memory to Memory-Centric Architecture","permalink":"/suggested-site/blog/2017/07/27/apache-ignite-2-1-a"}},"content":"***by Tom Diederich***\\n\\nIgniters, here are some community highlights from the last couple week. If I missed anything, please share it here. Meetups! Did you know that Apache Ignite experts are available to speak at your meetup? And we also have spots open for YOU to speak at the following meetups that some of us co-organize:\\n\\n- [Apache Ignite London](https://www.meetup.com/Apache-Ignite-London/)\\n- [Bay Area In-Memory Computing Meetup](https://www.meetup.com/Bay-Area-In-Memory-Computing/)\\n- [NYC In-Memory Computing Meetup](https://www.meetup.com/NYC-In-Memory-Computing-Meetup/)\\n- [Moscow Apache Ignite Meetup](https://www.meetup.com/Moscow-Apache-Ignite-Meetup/)\\n\\n\x3c!--truncate--\x3e\\n\\nMeanwhile, here\'s where to catch some great talks about Apache Ignite! We have 19 newly scheduled meetup talks on the books since the last update. All upcoming Ignite events can be found [here](https://ignite.apache.org/events.html). Let\'s take a closer look at some of them...\\n\\n### Scheduled speaking engagements\\n\\n**\\\\* Sept. 9: [Big Data and Cloud Meetup](https://www.meetup.com/datariders/events/242523245/) (Santa Clara, Calif.)**\\n\\nApache Ignite PMC chair Denis Magda will be speaking at the Big Data and Cloud Meetup September 9 from 10 a.m. to noon. His talk is titled: \\"Apache Spark and Apache Ignite: Where Fast Data Meets the IoT\\".\\n\\n**\\\\* Sept. 13: [SF Big Analytics Meetup](https://www.meetup.com/SF-Big-Analytics/events/242368299/)**\\n\\nDenis Magda will be the featured speaker at the SF Big Analytics Meetup on Sept. 13. Denis\' talk is titled: \\"Apache Ignite: the in-memory hammer in your data science toolkit.\\"\\n\\n**\\\\* Sept. 18: [Meetup: Camidge .NET User Group](https://www.meetup.com/Camidge-NET-User-Group/events/238837204/)**\\n\\nApache Ignite evangelist Akmal Chaudhri will speak at the Camidge .NET User Group Sept. 17. The title of his talk: \\"Scale Out and Conquer: Apache Ignite for .NET Users.\\"\\n\\n**\\\\* Sept. 21: Joint meetup! [Bay Area In-Memory Computing Meetup](https://www.meetup.com/Bay-Area-In-Memory-Computing/events/242961495/)** & [SF Spark and Friends](https://www.meetup.com/SF-Spark-and-Friends/)\\n\\n**\\\\* Sept. 27: [New York Kubernetes Meetup](https://www.meetup.com/New-York-Kubernetes-Meetup/events/242597746/)**\\n\\nApache Ignite evangelist Akmal Chaudhri will focus on a DevOps perspective on the orchestration of distributed databases such as Apache Ignite. Akmal will speak on node auto-discovery, automated horizontal scalability, availability, and utilization of RAM and disk with Apache Ignite.\\n\\n**\\\\* Oct. 4: Openstack & Ceph User Group Amsterdam**\\n\\nApache Ignite evangelist Akmal Chaudhri will show attendees how to build a Fast Data solution that will receive endless streams from the IoT side and will be capable of processing the streams in real-time using Apache Ignite\'s cluster resources.\\n\\n**\\\\* Oct. 13: [Big Data Week London 2017](http://london.bigdataweek.com/session/powering-banks-financial-institutions-distributed-systems/): A Festival of Data (conference)**\\n\\nAkmal Chaudhri will be speaking at the Big Data Week conference Oct. 13 in London. His talk, titled \\"Powering up banks and financial institutions with distributed systems,\\" will educate attendees about important Apache Ignite features for financial applications such as ACID compliance, SQL compatibility, persistence, replication, security, fault tolerance and more.\\n\\n**\\\\* Oct. 18: [Silicon Valley Java User Group](https://www.meetup.com/sv-jug/)**\\n\\nJoin Apache Ignite PMC Chair Denis Magda will introduce the many components of the open-source Apache Ignite. His talk, titled, \\"Catch an intro to Apache Ignite and skyrocket Java applications,\\" will teach attendees how to solve some of the most demanding scalability and performance challenges. He will also cover a few typical use cases and work through some code examples.\\n\\n**\\\\* Oct. 19: [Eurostaff Big Data London](https://www.meetup.com/Eurostaff-Big-Data/)**\\n\\nApache Ignite evangelist Akmal Chaudhri will show attendees how to build a Fast Data solution that will receive endless streams from the IoT side and will be capable of processing the streams in real-time using Apache Ignite\'s cluster resources.\\n\\n**\\\\* Oct. 24: [Spark Summit Europe 2017](https://spark-summit.org/eu-2017/events/how-to-share-state-across-multiple-apache-spark-jobs-using-apache-ignite/) (conference)**\\n\\nAkmal Chaudhri will be presenting at the Spark Summit Europe conference, Oct. 24-26 at the Convention Centre Dublin in Ireland. His session is titled: \\"How to Share State Across Multiple Spark Jobs using Apache Ignite.\\"\\n\\n**\\\\* Nov. 2: [Byte-Academy-FinTech-Python-Blockchain-Education](https://www.meetup.com/Byte-Academy-Finance-and-Technology-community/) Meetup (London)**\\n\\nIn his talk, titled, \\"Powering up banks and financial institutions with distributed systems,\\" Apache Ignite technical Akmal Chaudhri will explain important Apache Ignite features for financial applications such as ACID compliance, SQL compatibility, persistence, replication, security, fault tolerance and more. A customer case study will also be presented.\\n\\n### Blog posts\\n\\n- [Scale-up vs. scale-out architectures](https://www.gridgain.com/resources/blog/scale-vs-scale-out-architectures-conversation-fujitsus-dr-ferhat-hatay)\\n- [Cloud Wars: Apache Ignite - Getting started with AWS for Beginners](https://www.gridgain.com/resources/blog/cloud-wars-apacher-ignitetm-getting-started-aws-beginners-part-i) (Part I)\\n- [Apache Ignite Tip: Peer Class Loading Deployment Magic](https://www.gridgain.com/resources/blog/apacher-ignitetm-tip-peer-class-loading-deployment-magic)\\n- [The Future of In-Memory Computing](https://www.gridgain.com/resources/blog/future-in-memory-computing)\\n\\n### Webinars\\n\\n***Upcoming***\\n\\n- **Sept. 27**: [Better Machine Learning with Apache Ignite](https://www.gridgain.com/resources/webinars/better-machine-learning-apacher-ignitetm), with technical evangelist Akmal B. Chaudhri.\\n- **Oct. 4**: [Postgres with Apache Ignite: Faster Transactions and Analytics](https://www.gridgain.com/resources/webinars/postgres-apacher-ignitetm-faster-transactions-and-analytics), with GridGain senior solution architect Fotios Filacouris.\\n\\n**Past webinars (recordings available!)**\\n\\n[Deploy like a Boss: Using Kubernetes and Apache Ignite](https://www.gridgain.com/resources/webinars/deploy-boss-using-kubernetesr-and-apacher-ignitetm), with GridGain solution architect Dani Traphagen."},{"id":"/2017/07/27/apache-ignite-2-1-a","metadata":{"permalink":"/suggested-site/blog/2017/07/27/apache-ignite-2-1-a","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2017-07-27-apache-ignite-2-1-a.mdx","source":"@site/blog/2017-07-27-apache-ignite-2-1-a.mdx","title":"Apache Ignite 2.1 - A Leap from In-Memory to Memory-Centric Architecture","description":"The power and beauty of in-memory computing projects are that they truly do what they state -- deliver outstanding performance improvements by moving data closer to the CPU, using RAM as a storage and spreading the data sets out across a cluster of machines relying on horizontal scalability.","date":"2017-07-27T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"biodata","permalink":"/suggested-site/blog/tags/biodata"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"rdbms","permalink":"/suggested-site/blog/tags/rdbms"},{"inline":true,"label":"sql","permalink":"/suggested-site/blog/tags/sql"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":4.11,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite 2.1 - A Leap from In-Memory to Memory-Centric Architecture","authors":["denis"],"date":"2017-07-27T00:00:00.000Z","tags":["apache","biodata","ignite","rdbms","sql","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite Community Update (August 2017 Issue)","permalink":"/suggested-site/blog/2017/08/30/apache-ignite-community-update-august"},"nextItem":{"title":"Apache Ignite 2.0: Redesigned Off-heap Memory, DDL and Machine Learning","permalink":"/suggested-site/blog/2017/05/05/apache-ignite-2-0-redesigned"}},"content":"The power and beauty of in-memory computing projects are that they truly do what they state -- deliver outstanding performance improvements by moving data closer to the CPU, using RAM as a storage and spreading the data sets out across a cluster of machines relying on horizontal scalability.\\n\\nHowever, there is an unspoken side of the story. No matter how fast a platform is, we do not want to lose the data and encounter cluster restarts or other outages. To guarantee this we need to somehow make data persistent on the disk.\\n\\nMost in-memory computing projects address the persistence dilemma by giving a way to sync data back to a relational database (RDBMS). That sounds reasonable and undoubtedly works pretty well in practice, but if we dig deeper, you\'ll likely encounter the following limitations:\\n\\n\x3c!--truncate--\x3e\\n\\n- **RDBMS is a bottleneck.** No matter how fast your in-memory technology project, you can accelerate read operations only because every write has to be persisted to the disk -- which is usually a single machine running an RDBMS instance.\\n\\n- **RDBMS is a single point of failure.** Your distributed in-memory cluster usually consists of dozens and even hundreds of nodes which means you can safely lose this node here or drop that node there without worrying about data consistency and availability. However, if the RDBMS used by the cluster fails, then what? The answer is obvious -- the cluster can no longer be utilized because the RAM and disk parts go out of sync.\\n\\n- **SQL over RAM only**. Apache Ignite provides SQL database capabilities, however, you can only leverage them if all of the data and indexes are located in RAM. If a single piece of data, represented by a disk copy located in the RDBMS, then an Ignite-only SQL query will return an incomplete result set.\\n\\n- **Required RAM warmup**. When your cluster goes down, you have to restart it and preload all of the data from the RDBMS to RAM. That\'s essential if you use SQL or similar advanced querying languages. This dramatically increases the overall time of the downtime and can cost a lot of money.\\n\\nIf you use either Apache Ignite 1.x or 2.0 along with the RDBMS for disk storage, then you will hit these limitations. It\'s just the way in-memory architectures are integrated with the disk.\\n\\nHowever, the limitations are **no longer relevant** for Apache Ignite 2.1! This version made a leap from in-memory to a **[memory-centric](https://apacheignite.readme.io/docs/what-is-ignite)** architecture that:\\n\\n![Cluster and Cylinders](https://files.readme.io/752653f-cluster_and_cylinders.png)\\n\\n- Keeps using RAM as a first memory tier for data and indexes -- giving all of the benefits you had before.\\n- Supports durability criteria by treating disk as a secondary and larger tier that works in a distributed fashion and seamlessly integrates with the whole memory architecture.\\n- Supports the instantaneous cluster restarts -- once your cluster is up and running there is no reason to wait for RAM\'s warmup, go ahead and turn on back your applications that can safely execute all operations including SQL. The data and indexes will be taken from disk.\\n\\nCurious about how Ignite achieved these huge advantages? Lifting the curtain...\\n\\n### Durable Memory Architecture\\n\\nThe Apache Ignite memory-centric platform is based on the [durable memory architecture](https://apacheignite.readme.io/docs/durable-memory) that allows storing and processing data and indexes both in-memory and on disk when the [Ignite Persistent Store](https://apacheignite.readme.io/docs/distributed-persistent-store) is enabled. The memory architecture helps to achieve in-memory performance with the durability of the disk using all of the resources available in the cluster.\\n\\nThe durable memory is built and operates in a way similar to the virtual memory of operating systems such as Linux. However, the one significant difference between these two types of architectures is that the durable memory one always keeps the whole data set and indexes on the disk -- if the Ignite Persistent Store is enabled -- while the virtual memory uses the disk for swapping purposes only.\\n\\n### Ignite Persistent Store\\n\\n[Persistent Store](https://apacheignite.readme.io/docs/distributed-persistent-store) is a distributed ACID and SQL-compliant disk store that transparently integrates with the durable memory as an optional disk layer (SSD, Flash, 3D XPoint). Having the store enabled, you no longer need to keep all of the data in memory or warm-up the RAM after a whole cluster restart. The persistent store will keep the superset of data and all the SQL indexes on the disk making Ignite fully operational from the disk.\\n\\n### The Upshot\\n\\nTired of hooking up Ignite with an RDBMS? Go ahead and download Apache Ignite 2.1, enable Ignite Persistent Store, and launch your first durable Ignite cluster that distributes data sets and workloads relying on the performance of RAM and durability of the disk!\\n\\nFinally, Apache Ignite 2.1 can boast about another achievements in .NET, C++, SQL and Machine Learning. Go ahead and discover them!"},{"id":"/2017/05/05/apache-ignite-2-0-redesigned","metadata":{"permalink":"/suggested-site/blog/2017/05/05/apache-ignite-2-0-redesigned","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2017-05-05-apache-ignite-2-0-redesigned.mdx","source":"@site/blog/2017-05-05-apache-ignite-2-0-redesigned.mdx","title":"Apache Ignite 2.0: Redesigned Off-heap Memory, DDL and Machine Learning","description":"We released the long-awaited Apache Ignite version 2.0 on May 5. The community spent almost a year incorporating tremendous changes to the legacy Apache Ignite 1.x architecture. And all of that effort paid off. Our collective blood, sweat (and perhaps even a few tears) opened up new and exciting opportunities for the Apache Ignite project.","date":"2017-05-05T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"data","permalink":"/suggested-site/blog/tags/data"},{"inline":true,"label":"grid","permalink":"/suggested-site/blog/tags/grid"},{"inline":true,"label":"ignite","permalink":"/suggested-site/blog/tags/ignite"},{"inline":true,"label":"imdb","permalink":"/suggested-site/blog/tags/imdb"},{"inline":true,"label":"learning","permalink":"/suggested-site/blog/tags/learning"},{"inline":true,"label":"machine","permalink":"/suggested-site/blog/tags/machine"},{"inline":true,"label":"rdbms","permalink":"/suggested-site/blog/tags/rdbms"},{"inline":true,"label":"sql","permalink":"/suggested-site/blog/tags/sql"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":3.56,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite 2.0: Redesigned Off-heap Memory, DDL and Machine Learning","authors":["denis"],"date":"2017-05-05T00:00:00.000Z","tags":["apache","data","grid","ignite","imdb","learning","machine","rdbms","sql","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.1 - A Leap from In-Memory to Memory-Centric Architecture","permalink":"/suggested-site/blog/2017/07/27/apache-ignite-2-1-a"},"nextItem":{"title":"Presenting Apache Ignite SQL Grid at Big Data Bootcamp","permalink":"/suggested-site/blog/2017/03/13/presenting-apache-ignite-sql-grid"}},"content":"We released the long-awaited Apache Ignite version 2.0 on May 5. The community spent almost a year incorporating tremendous changes to the legacy Apache Ignite 1.x architecture. And all of that effort paid off. Our collective blood, sweat (and perhaps even a few tears) opened up new and exciting opportunities for the Apache Ignite project.\\n\\nHave I piqued your interest about this new release yet? Let\'s walk through some of the main new features that have appeared under the hood of Apache Ignite 2.0.\\n\\n\x3c!--truncate--\x3e\\n\\n### Reengineered Off-Heap Memory Architecture.\\n\\nThe platform\'s entire memory architecture was reengineered from scratch. In a nutshell, all of the data and indexes are now stored in a completely new manageable off-heap memory that has no issues with memory fragmentation, accelerates SQL Grid significantly and helps your application easily tolerate Java GC pauses.\\n\\nTake a peek at the illustration below and try to guess what\'s changed. Afterward, please read [this documentation](https://apacheignite.readme.io/docs/page-memory) to see if your eye caught everything that\'s new.\\n\\n![Page Memory Architecture](https://files.readme.io/0bf1bbf-Page-Memory-Diagram-v3.png)\\n\\nHere\'s something extremely noteworthy: the architecture now integrates seamlessly with disk drives. Why do we care about this? Stay tuned!\\n\\n### Data Definition Language.\\n\\nThis release introduces support for Data Definition Language (DDL) as a part of its SQL Grid functionality. Now you can define -- and, what\'s more important, alter -- indexes in runtime without the need to restart your cluster. Apache Ignite users have long awaited this feature! Even more exciting news: users can leverage this with standard SQL commands like CREATE or DROP index. This is only the beginning! Go to [this page](https://apacheignite.readme.io/docs/distributed-ddl) to learn more about current DDL support.\\n\\n### Machine Learning Grid Beta - Distributed Algebra.\\n\\nApache Ignite is about more than in-memory storage. And it\'s not just one more product for distributed computations or real-time streaming. It\'s much, much more than that. It\'s a hot blend of well-integrated distributed and highly concurrent modules that turned Apache Ignite into what is today: A robust data-fabric and framework with the goal of making your application thrive and outperform even the best of expectations.\\n\\nBut there was one thing missing until now. Drumroll, please: machine-learning support!\\n\\nWith Apache Ignite 2.0 you can check project\'s own [distributed algebra implementation](https://apacheignite.readme.io/docs/machine-learning). The distributed algebra is the foundation of the entire component. And soon you can expect to get distributed versions of widely used regression algorithms, decision trees and more.\\n\\n### Spring Data Integration.\\n\\n[Spring Data integration](https://apacheignite-mix.readme.io/docs/spring-data) allows the interaction of an Apache Ignite cluster using the well-known and highly adopted Spring Data Framework. You can connect to the cluster by means of Spring Data repositories and start executing distributed SQL queries as well as simple CRUD operations.\\n\\n### Rocket MQ\\n\\nAre you using Rocket MQ in your project and need to push data from the Rocket to Ignite? [Here is](https://apacheignite-mix.readme.io/docs/rocketmq-streamer) an easy solution.\\n\\n### Hibernate 5.\\n\\nHibernate L2 cache users have been anticipating support of Hibernate 5 on Apache Ignite for quite a long time. Apache Ignite 2.0 grants [this desire](https://apacheignite-mix.readme.io/docs/hibernate-l2-cache). The integration now supports Hibernate 5 and contains a number of bug fixes and improvements.\\n\\n### Ignite.NET\\n\\nIgnite.NET has been enhanced with an addition of a [plugin system](https://apacheignite-net.readme.io/docs/plugins) that allows the writing and embedding 3rd party .NET components into Ignite.NET.\\n\\n### Ignite.C++\\n\\nThe Ignite.C++ part of the community finally came up with a way to execute arbitrary C++ code on remote cluster machines.\\n\\nThis approach was initially tested for [continuous queries](https://apacheignite-cpp.readme.io/docs/continuous-queries). You can now register continuous queries\' remote filters on any cluster node you like. Going forward you can expect support for the Ignite.C++ compute grid and more.\\n\\nWant to learn more? Please join me June 7 for a [webinar](https://www.gridgain.com/resources/webinars/apacher-ignitetm-whats-new-version-20) titled, \\"Apache Ignite: What\'s New in Version 2.0.\\" I hope to see you there!\\n\\nP.S. Just in case you can\'t wait until June... [here\'s](https://ignite.apache.org/releases/ignite2/2.0.0/release_notes.html) a full list of the changes inside Apache Ignite 2.0."},{"id":"/2017/03/13/presenting-apache-ignite-sql-grid","metadata":{"permalink":"/suggested-site/blog/2017/03/13/presenting-apache-ignite-sql-grid","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2017-03-13-presenting-apache-ignite-sql-grid.mdx","source":"@site/blog/2017-03-13-presenting-apache-ignite-sql-grid.mdx","title":"Presenting Apache Ignite SQL Grid at Big Data Bootcamp","description":"Apache Ignite community welcomes you to attend Big Data Bootcamp on March 27th, 28th and 29th 2017 in Santa Clara, USA.","date":"2017-03-13T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":1.66,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Presenting Apache Ignite SQL Grid at Big Data Bootcamp","authors":["denis"],"date":"2017-03-13T00:00:00.000Z","tags":["apache","archived"]},"unlisted":false,"prevItem":{"title":"Apache Ignite 2.0: Redesigned Off-heap Memory, DDL and Machine Learning","permalink":"/suggested-site/blog/2017/05/05/apache-ignite-2-0-redesigned"},"nextItem":{"title":"Apache Ignite 1.9 Released","permalink":"/suggested-site/blog/2017/03/07/apache-ignite-1-9-released"}},"content":"Apache Ignite community welcomes you to attend [Big Data Bootcamp](http://globalbigdataconference.com/santa-clara/big-data-bootcamp/event-79.html) on March 27th, 28th and 29th 2017 in Santa Clara, USA.\\n\\nThe conference gathers experts and vendors from Big Data realm in sunny California who will be covering a variety of Big Data products and technologies, including, but not limited to, Hadoop, Spark, NoSQL, Data Science, Machine Learning, Artificial Intelligence & Deep Learning.\\n\\nApache Ignite will be introduced at the conference by its PMC chair and committer - Denis Magda.\\n\\n\x3c!--truncate--\x3e\\n\\nAs all we know, in-memory data grids bring exceptional performance and scalability gains to applications built on top of them. The applications truly achieve 10x more performance improvement and become easily scalable and fault-tolerant thanks to the unique data grids architecture. However, because of this particular architecture, a majority of data grids have to sacrifice traditional SQL support requiring application developers to completely rewrite their SQL-based code to support data grid specific APIs. This, however, is not true for Apache Ignite.\\n\\nIn this presentation, Denis will introduce Apache Ignite SQL Grid component that combines the best of two worlds - performance and scalability of data grids and traditional ANSI-99 SQL support of relational databases. Moreover, Denis will take an existing application that works with a relational database and will show how to run it on top of Apache Ignite with minimum efforts.\\n\\nThe talk is called **[\\"Apache Ignite SQL Grid: Hot Blend of Traditional SQL and Swift Data Grid\\"](http://globalbigdataconference.com/santa-clara/big-data-bootcamp-79/speaker-details/denis-magda-41504.html)** and takes place at 1:00 PM - 1:40PM on March 28. Refer to [Big Data Bootcamp\'s agenda](http://globalbigdataconference.com/santa-clara/big-data-bootcamp/schedule-79.html) for more details.\\n\\nFinally, use promotional code SPEAKER to receive $200 discount on or before March 15th by registering at [the conference site](http://globalbigdataconference.com/santa-clara/big-data-bootcamp/attendee-registration-79.html).\\n\\nSee you at the conference!"},{"id":"/2017/03/07/apache-ignite-1-9-released","metadata":{"permalink":"/suggested-site/blog/2017/03/07/apache-ignite-1-9-released","editUrl":"https://github.com/apache/ignite-website/tree/master/blog/2017-03-07-apache-ignite-1-9-released.mdx","source":"@site/blog/2017-03-07-apache-ignite-1-9-released.mdx","title":"Apache Ignite 1.9 Released","description":"Apache Ignite community is pleased to announce Apache Ignite 1.9 - the next minor release of a well-known in-memory data fabric. The release, as usual, encompasses many bug fixes, performance improvements and fresh features. Below you can see a description of the most significant updates.","date":"2017-03-07T00:00:00.000Z","tags":[{"inline":true,"label":"apache","permalink":"/suggested-site/blog/tags/apache"},{"inline":true,"label":"archived","permalink":"/suggested-site/blog/tags/archived"}],"readingTime":2.62,"hasTruncateMarker":true,"authors":[{"name":"Denis Magda","title":"Apache Ignite PMC Member","url":"https://github.com/dmagda","imageURL":"/suggested-site/img/authors/dmagda.png","key":"denis","page":null}],"frontMatter":{"title":"Apache Ignite 1.9 Released","authors":["denis"],"date":"2017-03-07T00:00:00.000Z","tags":["apache","archived"]},"unlisted":false,"prevItem":{"title":"Presenting Apache Ignite SQL Grid at Big Data Bootcamp","permalink":"/suggested-site/blog/2017/03/13/presenting-apache-ignite-sql-grid"}},"content":"Apache Ignite community is pleased to announce Apache Ignite 1.9 - the next minor release of a well-known in-memory data fabric. The release, as usual, encompasses many bug fixes, performance improvements and fresh features. Below you can see a description of the most significant updates.\\n\\n**Kubernetes Support**\\n\\nApache Ignite was integrated with Kubernetes which is a modern open source container cluster manager. The integration helps to simplify a deployment of an Apache Ignite cluster in environments managed by Kubernetes and let the latter care of resources management, cluster\'s scalability and lifecycle.\\n\\n\x3c!--truncate--\x3e\\n\\nFor instance, you\'re no longer need to monitor a cluster state constantly to be sure that the number of cluster nodes doesn\'t go, let\'s say, below 4. If Kubernetes sees that one cluster node is disconnected and only 3 are left then it will start one more automatically to meet the deployment requirements.\\n\\nRefer to [Kubernetes Deployment Getting Started](https://apacheignite.readme.io/docs/kubernetes-deployment) if this is the feature of interest for you.\\n\\n**Performance Optimizations and Benchmarks Automation**\\n\\nApache Ignite 1.9 can boast of much better performance for core cache operations and SQL queries in compare to the previous Apache Ignite 1.8 release. In general, we observe up to 40% performance increase for particular operations.\\n\\nIt\'s no longer a challenge to reproduce the performance numbers. Starting with Apache Ignite 1.9 release all the benchmarks are delivered in every Apache Ignite distribution and can be easily executed in your own environment.\\n\\n**Data Modification Language and Queries Parallelism**\\n\\nThe community keeps spending significant time improving [SQL Grid](https://apacheignite.readme.io/docs/sql-grid) component that empowers Apache Ignite users with in-memory database capabilities.\\n\\nIn this release, DML (Data Modification Language) support was expanded to the level of [Ignite.NET](https://apacheignite-net.readme.io/docs/distributed-dml) and [Ignite.C++](https://apacheignite-cpp.readme.io/docs/distributed-dml) APIs. Plus, a [streaming mode](https://apacheignite.readme.io/docs/jdbc-driver#streaming-mode) was introduced for DML allowing to execute DML operations even faster for specific scenarios like initial data preloading.\\n\\nOne more SQL Grid related optimization makes it possible to parallelize a query execution on every Ignite node where the query has been mapped. By default, a query is executed in a single thread on every participating node. However, for a variety of OLAP use cases it might be a bottleneck and this is where the [query parallelism](https://apacheignite.readme.io/docs/sql-performance-and-debugging#section-query-parallelism) can help out.\\n\\n**Ignite.NET**\\n\\nApache Ignite implemented .NET TransactionScope API allowing to work with distributed Apache Ignite transactions fully relaying on standard interfaces available in .NET Framework. Refer to [this documentation page](https://apacheignite-net.readme.io/docs/transactionscope-api) for more information.\\n\\n**Ignite.C++**\\n\\nIgnite.C++ introduced support for well-known [continuous queries API](https://apacheignite-cpp.readme.io/docs/continuous-queries). Now, you can listen to data modifications happened on Apache Ignite\'s distributed caches side from your C++ applications.\\n\\n**Spark**\\n\\nIgnite\'s spark integration was upgraded to the latest Spark version. Presently, you can leverage from Ignite Shared RDDs in applications using latest Spark version.\\n\\n**Give a Try**\\n\\nGo and grab the latest 1.9 release from our [main site](https://ignite.apache.org/download). Looking forward to your feedback!"}]}}')}}]);